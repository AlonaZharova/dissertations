{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lifelines\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "from scipy.stats.stats import spearmanr\n",
    "\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "seed = 31415\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### C(t)-INDEX CALCULATION\n",
    "def c_index(Prediction, Time_survival, Death, Time):\n",
    "    '''\n",
    "        Raiber: okay, so it does not make a diffrence if the predictions are scores or risks(softmax probabilities) \n",
    "        This is a cause-specific c(t)-index\n",
    "        - Prediction      : risk at Time (higher --> more risky) <class 'numpy.ndarray'> \n",
    "          shape (7997,)\n",
    "        - Time_survival   : survival/censoring time   <class 'numpy.ndarray'>\n",
    "        shape (7997, 1)\n",
    "        - Death           :   <class 'numpy.ndarray'>\n",
    "            > 1: death\n",
    "            > 0: censored (including death from other cause)\n",
    "        shape (7997,)\n",
    "        - Time            : time of evaluation (time-horizon when evaluating C-index)   <class 'int'>\n",
    "        scalar value \n",
    "    '''\n",
    "    N = len(Prediction)# N = 7997 \n",
    "    #A.shape (7997, 7997) for validation \n",
    "    A = np.zeros((N,N))\n",
    "    Q = np.zeros((N,N))\n",
    "    N_t = np.zeros((N,N))\n",
    "    Num = 0\n",
    "    Den = 0\n",
    "    for i in range(N):\n",
    "        # np.where return the index of the values that fullfil the condition \n",
    "        # let assume i = 0, then np.where(Time_survival[i] < Time_survival) will return the indexes \n",
    "        # from 0 until 7996 (because the length is 7996) that have a bigger time then the index 0\n",
    "        # and the assign to A[0, list of biiger times that the value in 0 ] the value 1 \n",
    "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
    "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
    "  \n",
    "        if (Time_survival[i]<=Time and Death[i]==1):\n",
    "            N_t[i,:] = 1\n",
    "\n",
    "    Num  = np.sum(((A)*N_t)*Q)\n",
    "    Den  = np.sum((A)*N_t)\n",
    "\n",
    "    if Num == 0 and Den == 0:\n",
    "        result = -1 # not able to compute c-index!\n",
    "    else:\n",
    "        result = float(Num/Den)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
    "def CensoringProb(Y, T):\n",
    "\n",
    "    T = T.reshape([-1]) # (N,) - np array\n",
    "    Y = Y.reshape([-1]) # (N,) - np array\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
    "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
    "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
    "    G = CensoringProb(Y_train, T_train)\n",
    "    N = len(Prediction)\n",
    "\n",
    "    W = np.zeros(len(Y_test))\n",
    "    Y_tilde = (T_test > Time).astype(float)\n",
    "\n",
    "    for i in range(N):\n",
    "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
    "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
    "\n",
    "        if len(tmp_idx1) == 0:\n",
    "            G1 = G[1, -1]\n",
    "        else:\n",
    "            G1 = G[1, tmp_idx1[0]]\n",
    "\n",
    "        if len(tmp_idx2) == 0:\n",
    "            G2 = G[1, -1]\n",
    "        else:\n",
    "            G2 = G[1, tmp_idx2[0]]\n",
    "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
    "\n",
    "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
    "\n",
    "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raiber used \n",
    "def f_get_Normalization(X, norm_mode): # raiber added functions \n",
    "    num_Patient, num_Feature = np.shape(X)\n",
    "\n",
    "    if norm_mode == 'standard': #zero mean unit variance\n",
    "        for j in range(num_Feature):\n",
    "            if np.std(X[:,j]) != 0:\n",
    "                X[:,j] = (X[:,j] - np.mean(X[:, j]))/np.std(X[:,j])\n",
    "            else:\n",
    "                X[:,j] = (X[:,j] - np.mean(X[:, j]))\n",
    "    elif norm_mode == 'normal': #min-max normalization\n",
    "        for j in range(num_Feature):\n",
    "            X[:,j] = (X[:,j] - np.min(X[:,j]))/(np.max(X[:,j]) - np.min(X[:,j]))\n",
    "    else:\n",
    "        print(\"INPUT MODE ERROR!\")\n",
    "\n",
    "    return X\n",
    " \n",
    "def formatted_data(x, t, e, idx):\n",
    "    death_time = np.array(t[idx], dtype=float)\n",
    "    censoring = np.array(e[idx], dtype=float)\n",
    "    covariates = np.array(x[idx])\n",
    "\n",
    "    print(\"observed fold:{}\".format(sum(e[idx]) / len(e[idx])))\n",
    "    survival_data = {'x': covariates, 't': death_time, 'e': censoring}\n",
    "    return survival_data\n",
    "\n",
    "\n",
    "def risk_set(data_t):\n",
    "    size = len(data_t)\n",
    "    risk_set = np.zeros(shape=(size, size))\n",
    "    for idx in range(size):\n",
    "        temp = np.zeros(shape=size)\n",
    "        t_i = data_t[idx]\n",
    "        at_risk = data_t > t_i\n",
    "        temp[at_risk] = 1\n",
    "        # temp[idx] = 0\n",
    "        risk_set[idx] = temp\n",
    "    return risk_set\n",
    "\n",
    "def one_hot_encoder(data, encode):\n",
    "    print(\"Encoding data:{}\".format(data.shape))\n",
    "    data_encoded = data.copy()\n",
    "    encoded = pandas.get_dummies(data_encoded, prefix=encode, columns=encode)\n",
    "    print(\"head of data:{}, data shape:{}\".format(data_encoded.head(), data_encoded.shape))\n",
    "    print(\"Encoded:{}, one_hot:{}{}\".format(encode, encoded.shape, encoded[0:5]))\n",
    "    return encoded\n",
    "\n",
    "def get_train_median_mode(x, categorial):\n",
    "    categorical_flat = flatten_nested(categorial)\n",
    "    print(\"categorical_flat:{}\".format(categorical_flat))\n",
    "    imputation_values = []\n",
    "    print(\"len covariates:{}, categorical:{}\".format(x.shape[1], len(categorical_flat)))\n",
    "    median = np.nanmedian(x, axis=0)\n",
    "    mode = []\n",
    "    for idx in np.arange(x.shape[1]):\n",
    "        a = x[:, idx]\n",
    "        (_, idx, counts) = np.unique(a, return_index=True, return_counts=True)\n",
    "        index = idx[np.argmax(counts)]\n",
    "        mode_idx = a[index]\n",
    "        mode.append(mode_idx)\n",
    "    for i in np.arange(x.shape[1]):\n",
    "        if i in categorical_flat:\n",
    "            imputation_values.append(mode[i])\n",
    "        else:\n",
    "            imputation_values.append(median[i])\n",
    "    print(\"imputation_values:{}\".format(imputation_values))\n",
    "    return imputation_values\n",
    "\n",
    "\n",
    "def missing_proportion(dataset):\n",
    "    missing = 0\n",
    "    columns = np.array(dataset.columns.values)\n",
    "    for column in columns:\n",
    "        missing += dataset[column].isnull().sum()\n",
    "    return 100 * (missing / (dataset.shape[0] * dataset.shape[1]))\n",
    "\n",
    "\n",
    "def one_hot_indices(dataset, one_hot_encoder_list):\n",
    "    indices_by_category = []\n",
    "    for colunm in one_hot_encoder_list:\n",
    "        values = dataset.filter(regex=\"{}_.*\".format(colunm)).columns.values\n",
    "        # print(\"values:{}\".format(values, len(values)))\n",
    "        indices_one_hot = []\n",
    "        for value in values:\n",
    "            indice = dataset.columns.get_loc(value)\n",
    "            # print(\"column:{}, indice:{}\".format(colunm, indice))\n",
    "            indices_one_hot.append(indice)\n",
    "        indices_by_category.append(indices_one_hot)\n",
    "    # print(\"one_hot_indices:{}\".format(indices_by_category))\n",
    "    return indices_by_category\n",
    "\n",
    "def flatten_nested(list_of_lists):\n",
    "    flattened = [val for sublist in list_of_lists for val in sublist]\n",
    "    return flattened\n",
    "\n",
    "def get_missing_mask(data, imputation_values=None):\n",
    "    copy = data\n",
    "    for i in np.arange(len(data)):\n",
    "        row = data[i]\n",
    "        indices = np.isnan(row)\n",
    "        # print(\"indices:{}, {}\".format(indices, np.where(indices)))\n",
    "        if imputation_values is None:\n",
    "            copy[i][indices] = 0\n",
    "        else:\n",
    "            for idx in np.arange(len(indices)):\n",
    "                if indices[idx]:\n",
    "                    # print(\"idx:{}, imputation_values:{}\".format(idx, np.array(imputation_values)[idx]))\n",
    "                    copy[i][idx] = imputation_values[idx]\n",
    "    # print(\"copy;{}\".format(copy))\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fontsize = 18\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)  # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "\n",
    "font = {'family': 'normal',\n",
    "        'weight': 'bold',\n",
    "        'size': 24}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          # 'figure.figsize': (15, 5),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize': 'x-large',\n",
    "          'xtick.labelsize': 'x-large',\n",
    "          'ytick.labelsize': 'x-large'}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "# We'll hack a bit with the t-SNE code in sklearn 0.15.2.\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('paper')\n",
    "sns.set()\n",
    "title_fontsize = 18\n",
    "label_fontsize = 18\n",
    "\n",
    "\n",
    "def plot_cost(training, validation, name, model, epochs, best_epoch):\n",
    "    x = np.arange(start=0, stop=len(training), step=1).tolist()\n",
    "    constant = 1e-10\n",
    "    plt.figure()\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.ylim(min(min(training), min(validation), 0) - constant, max(max(training), max(validation)) + constant)\n",
    "    plt.plot(x, training, color='blue', linestyle='-', label='training')\n",
    "    plt.plot(x, validation, color='green', linestyle='-', label='validation')\n",
    "    plt.axvline(x=best_epoch, color='red')\n",
    "    title = 'Training {} {}: epochs={}, best epoch={} '.format(model, name, epochs, best_epoch)\n",
    "    plt.title(title, fontsize=title_fontsize)\n",
    "    plt.ylabel(name)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.savefig('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\plots\\\\ndb_d\\\\d10\\\\{}_{}'.format(model, name))\n",
    "\n",
    "\n",
    "def box_plots(empirical, predicted, name='data', time='days', log_domain=True):\n",
    "    plt.figure()\n",
    "    if log_domain:\n",
    "        plt.yscale('log')\n",
    "    plt.boxplot(x=predicted, sym='o', notch=0, whis='range')\n",
    "    plt.scatter(x=np.arange(start=1, stop=len(predicted) + 1), y=empirical, color='purple', label='empirical')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.xticks(fontsize=5)\n",
    "    plt.ylabel('t ({})'.format(time))\n",
    "    plt.xlabel('Observation index')\n",
    "    plt.savefig('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\plots\\\\ndb_d\\\\d10\\\\{}_box_plot'.format(name))\n",
    "\n",
    "\n",
    "def hist_plots(samples, name, xlabel, empirical=None):\n",
    "    plt.figure()\n",
    "    plt.axvline(x=np.mean(samples), color='grey', label='mean', linestyle='--', )\n",
    "    if empirical:\n",
    "        plt.axvline(x=empirical, color='purple', label='empirical', linestyle='--', )\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.hist(samples, bins=25)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.savefig(\"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\plots\\\\ndb_d\\\\d10\\\\{}_hist\".format(name))\n",
    "    plt.figure()\n",
    "    plt.boxplot(x=samples, sym='o', notch=0, whis='range')\n",
    "    plt.scatter(x=1, y=np.mean(samples), color='purple', label='mean')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.savefig('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\plots\\\\ndb_d\\\\d10\\\\{}_box_plot'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raiber: we draw form the uniform distribution a tensor with shape batch_size (128) and dim(feature dim 17) and values between zero and 1\n",
    "# shape: (128, 17) \n",
    "def uniform(dim, batch_size):\n",
    "    ones = np.ones(shape=dim, dtype=np.float32)\n",
    "    noise = tf.distributions.Uniform(low=0 * ones, high=ones).sample(sample_shape=[batch_size])\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. tf_helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_variables():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "\n",
    "def mlp_neuron(layer_input, weights, biases, activation=True): # if activation is set to false from the called function, we stay false\n",
    "    mlp = tf.add(tf.matmul(layer_input, weights), biases)\n",
    "    if activation:\n",
    "        return tf.nn.relu(mlp)\n",
    "    else:\n",
    "        return mlp\n",
    "\n",
    "\n",
    "def normalized_mlp(layer_input, weights, biases, is_training, batch_norm, layer, activation=tf.nn.relu):\n",
    "    mlp = tf.add(tf.matmul(layer_input, weights), biases)\n",
    "    if batch_norm:\n",
    "        norm = batch_norm_wrapper(mlp, is_training, layer=layer)\n",
    "        # norm = tf_batch_norm(is_training=is_training, inputs=mlp, layer=layer)\n",
    "        return activation(norm)\n",
    "    else:\n",
    "        return activation(mlp)\n",
    "\n",
    "\n",
    "def dropout_normalised_mlp(layer_input, weights, biases, is_training, batch_norm, layer, keep_prob=1,\n",
    "                           activation=tf.nn.relu):\n",
    "    mlp = normalized_mlp(layer_input, weights, biases, is_training, batch_norm,\n",
    "                         layer=layer, activation=activation)  # apply DropOut to hidden layer\n",
    "    drop_out = tf.cond(is_training, lambda: tf.nn.dropout(mlp, keep_prob), lambda: mlp)\n",
    "    return drop_out\n",
    "\n",
    "\n",
    "def create_nn_weights(layer, network, shape): # layer: the name of the layer, exp: h0_z hiddem layer 0 \n",
    "                                              # network: string (in this example would be the \"decoder\")\n",
    "                                              # shape: [input_shape, hidden_dim] in one of the examples ][34,50]\n",
    "    # raiber: create the name of the weights and variables\n",
    "    h_vars = {}\n",
    "    w_h = 'W_' + network + '_' + layer\n",
    "    b_h = 'b_' + network + '_' + layer\n",
    "    # get the values of the weights and variables \n",
    "    h_vars[w_h] = create_weights(shape=shape, name=w_h)\n",
    "    h_vars[b_h] = create_biases([shape[1]], name=b_h)\n",
    "    variable_summaries(h_vars[w_h], w_h)\n",
    "    variable_summaries(h_vars[b_h], b_h)\n",
    "\n",
    "    return h_vars[w_h], h_vars[b_h]\n",
    "\n",
    "\n",
    "def create_biases(shape, name):\n",
    "    print(\"name:{}, shape{}\".format(name, shape))\n",
    "    return tf.Variable(tf.constant(shape=shape, value=0.0), name=name)\n",
    "\n",
    "\n",
    "def create_weights(shape, name):\n",
    "    print(\"name:{}, shape{}\".format(name, shape))\n",
    "    # initialize weights using Glorot and Bengio(2010) scheme\n",
    "    a = tf.sqrt(6.0 / (shape[0] + shape[1]))\n",
    "    # return tf.Variable(tf.random_normal(shape, stddev=tf.square(0.0001)), name=name)\n",
    "    return tf.Variable(tf.random_uniform(shape, minval=-a, maxval=a, dtype=tf.float32), name=name)\n",
    "\n",
    "\n",
    "def variable_summaries(var, summary_name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(summary_name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "\n",
    "\n",
    "def batch_norm_wrapper(inputs, is_training, layer):\n",
    "    # http://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "    # raiber: get_shape()[-1]] gives the dim of the cols \n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False, name='{}_batch_norm_mean'.format(layer))\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False, name='{}_batch_norm_var'.format(layer))\n",
    "    print(\"batch inputs {}, shape for var{}\".format(inputs.get_shape(), inputs.get_shape()[-1]))\n",
    "\n",
    "    offset = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), name='{}_batch_norm_offset'.format(layer))\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]), name='{}_batch_norm_scale'.format(layer))\n",
    "    epsilon = 1e-5\n",
    "    alpha = 0.9  # use numbers closer to 1 if you have more data\n",
    "\n",
    "    def batch_norm():\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs, [0])\n",
    "        print(\"batch mean {}, var {}\".format(batch_mean.shape, batch_var.shape))\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                                pop_mean * alpha + batch_mean * (1 - alpha))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * alpha + batch_var * (1 - alpha))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs, mean=batch_mean, variance=batch_var, offset=offset, scale=scale,\n",
    "                                              variance_epsilon=epsilon)\n",
    "\n",
    "    def pop_norm():\n",
    "        return tf.nn.batch_normalization(inputs, pop_mean, pop_var, offset=offset, scale=scale,\n",
    "                                          variance_epsilon=epsilon)\n",
    "\n",
    "    return tf.cond(is_training, batch_norm, pop_norm)\n",
    "\n",
    "\n",
    "def hidden_mlp_layers(batch_norm, hidden_dim, is_training, keep_prob, layer_input, size):\n",
    "    tmp = layer_input\n",
    "    for i in np.arange(size):\n",
    "        input_shape = tmp.get_shape().as_list()[1]\n",
    "        print(\"layer input shape:{}\".format(input_shape))\n",
    "        w_hi, b_hi = create_nn_weights('h{}_z'.format(i), 'decoder', [input_shape, hidden_dim[i]])\n",
    "        h_i = dropout_normalised_mlp(layer_input=tmp, weights=w_hi, biases=b_hi,\n",
    "                                     is_training=is_training,\n",
    "                                     batch_norm=batch_norm, keep_prob=keep_prob,\n",
    "                                     layer='h{}_z_decoder'.format(i))\n",
    "\n",
    "        tmp = h_i\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def hidden_mlp_layers_noise(batch_norm, hidden_dim, is_training, keep_prob, layer_input, noise_alpha, size,\n",
    "                            batch_size):\n",
    "    # hidden_dim = [50,50]\n",
    "    # layer input shape is (?,34) 34 = 17 + 17 (x + noise)\n",
    "    # size = len(hiiden_dim) = 2\n",
    "    tmp = layer_input\n",
    "    for i in np.arange(size):\n",
    "        input_shape = tmp.get_shape().as_list()[1] # in the second loop input_shape would be 100\n",
    "        print(\"layer input shape:{}\".format(input_shape))\n",
    "        w_hi, b_hi = create_nn_weights('h{}_z'.format(i), 'decoder', [input_shape, hidden_dim[i]])\n",
    "        h_i = dropout_normalised_mlp(layer_input=tmp, weights=w_hi, biases=b_hi,\n",
    "                                     is_training=is_training,\n",
    "                                     batch_norm=batch_norm, keep_prob=keep_prob,\n",
    "                                     layer='h{}_z_decoder'.format(i)) # h_i shape is (?,50)\n",
    "                                                                      # second loop also (?,50)\n",
    "\n",
    "        # noise = standard_gaussian(dim=hidden_dim[i], batch_size=batch_size) * tf.gather(noise_alpha, i + 1)\n",
    "        noise = uniform(dim=hidden_dim[i], batch_size=batch_size) * tf.gather(noise_alpha, i + 1) # noise shape (350 batch_size),50), the same for the second loop\n",
    "        tmp = tf.concat([h_i, noise], axis=1) # the shape become (?, 100) because 50 (h_i) and 50 (noise)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generated_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_distribution(predicted, empirical, data, time='days', cens=False):\n",
    "    predicted_samples = np.transpose(predicted)\n",
    "    print(\"observed_samples:{}, empirical_observed:{}\".format(predicted_samples.shape,\n",
    "                                                              empirical.shape))\n",
    "\n",
    "    best_samples, diff, worst_samples = get_best_worst_indices(cens, empirical, predicted_samples)\n",
    "\n",
    "    predicted_best = predicted_samples[best_samples]\n",
    "    predicted_worst = predicted_samples[worst_samples]\n",
    "    hist_plots(samples=diff, name='{}_absolute_error'.format(data), xlabel=r'|\\tilde{t}-t|')\n",
    "\n",
    "    box_plots(empirical=empirical[best_samples], predicted=list(predicted_best), name=('%s_best' % data),\n",
    "              time=time)\n",
    "    box_plots(empirical=empirical[worst_samples], predicted=list(predicted_worst), name=('%s_worst' % data),\n",
    "              time=time)\n",
    "\n",
    "\n",
    "def get_best_worst_indices(cens, empirical, predicted, size=50):\n",
    "    diff = compute_relative_error(cens=cens, empirical=empirical, predicted=predicted)\n",
    "    indices = sorted(range(len(abs(diff))), key=lambda k: diff[k])\n",
    "    best_samples = indices[0:size]\n",
    "    worst_samples = indices[len(indices) - size - 1: len(indices) - 1]\n",
    "    return best_samples, diff, worst_samples\n",
    "\n",
    "\n",
    "def compute_relative_error(cens, empirical, predicted, relative=False):\n",
    "    predicted_median = np.median(predicted, axis=1)\n",
    "    if cens:\n",
    "        diff = np.minimum(0, predicted_median - empirical)\n",
    "    else:\n",
    "        diff = predicted_median - empirical\n",
    "    if relative:\n",
    "        return diff\n",
    "    else:\n",
    "        return abs(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_metrics(e, risk_set, predicted, batch_size, empirical):\n",
    "    partial_likelihood = tf.constant(0.0, shape=())\n",
    "    rel_abs_err = tf.constant(0.0, shape=())\n",
    "    total_cens_loss = tf.constant(0.0, shape=())\n",
    "    total_obs_loss = tf.constant(0.0, shape=())\n",
    "    predicted = tf.squeeze(predicted)\n",
    "    observed = tf.reduce_sum(e) # compute sum across the tensor\n",
    "    censored = tf.subtract(tf.cast(batch_size, dtype=tf.float32), observed) # what is left from the batch which is not bserved, it is then censored \n",
    "\n",
    "    def condition(i, likelihood, rae, recon_loss, obs_recon_loss):\n",
    "        return i < batch_size\n",
    "\n",
    "    def body(i, likelihood, rae, cens_recon_loss, obs_recon_loss):\n",
    "        # get edges for observation i\n",
    "        pred_t_i = tf.gather(predicted, i)\n",
    "        emp_t_i = tf.gather(empirical, i)\n",
    "        e_i = tf.gather(e, i)\n",
    "        censored = tf.equal(e_i, 0)\n",
    "        obs_at_risk = tf.gather(risk_set, i)\n",
    "        print(\"obs_at_risk:{}, g_theta:{}\".format(obs_at_risk.shape, predicted.shape))\n",
    "        risk_hazard_list = tf.multiply(predicted, obs_at_risk)\n",
    "        num_adjacent = tf.reduce_sum(obs_at_risk)\n",
    "        # calculate partial likelihood\n",
    "        risk = tf.subtract(pred_t_i, risk_hazard_list)\n",
    "        activated_risk = tf.nn.sigmoid(risk)\n",
    "        # logistic = map((lambda ele: log(1 + exp(ele * -1)) * 1 / log(2)), x)\n",
    "        constant = 1e-8\n",
    "        log_activated_risk = tf.div(tf.log(activated_risk + constant), tf.log(2.0))\n",
    "        obs_likelihood = tf.add(log_activated_risk, num_adjacent)\n",
    "        uncensored_likelihood = tf.cond(censored, lambda: tf.constant(0.0), lambda: obs_likelihood)\n",
    "        cumulative_likelihood = tf.reduce_sum(uncensored_likelihood)\n",
    "        updated_likelihood = tf.add(cumulative_likelihood, likelihood)\n",
    "\n",
    "        # RElative absolute error\n",
    "        abs_error_i = tf.abs(tf.subtract(pred_t_i, emp_t_i))\n",
    "        pred_great_empirical = tf.greater(pred_t_i, emp_t_i)\n",
    "        min_rea_i = tf.minimum(tf.div(abs_error_i, pred_t_i), tf.constant(1.0))\n",
    "        rea_i = tf.cond(tf.logical_and(censored, pred_great_empirical), lambda: tf.constant(0.0), lambda: min_rea_i)\n",
    "        cumulative_rae = tf.add(rea_i, rae)\n",
    "\n",
    "        # Censored generated t loss\n",
    "        diff_time = tf.subtract(pred_t_i, emp_t_i)\n",
    "        # logistic = map((lambda ele: log(1 + exp(ele * -1)) * 1 / log(2)), x)\n",
    "        # logistic = tf.div(tf.nn.sigmoid(diff_time) + constant, tf.log(2.0))\n",
    "        # hinge = map(lambda ele: max(0, 1 - ele), x)\n",
    "        hinge = tf.nn.relu(1.0 - diff_time)\n",
    "        censored_loss_i = tf.cond(censored, lambda: hinge, lambda: tf.constant(0.0))\n",
    "        # Sum over all edges and normalize by number of edges\n",
    "        # L1 recon\n",
    "        observed_loss_i = tf.cond(censored, lambda: tf.constant(0.0),\n",
    "                                  lambda: tf.losses.absolute_difference(labels=emp_t_i, predictions=pred_t_i))\n",
    "        # add observation risk to total risk\n",
    "        cum_cens_loss = tf.add(cens_recon_loss, censored_loss_i)\n",
    "        cum_obs_loss = tf.add(obs_recon_loss, observed_loss_i)\n",
    "        return [i + 1, tf.reshape(updated_likelihood, shape=()), tf.reshape(cumulative_rae, shape=()),\n",
    "                tf.reshape(cum_cens_loss, shape=()), tf.reshape(cum_obs_loss, shape=())]\n",
    "\n",
    "    # Relevant Functions\n",
    "    idx = tf.constant(0, shape=())\n",
    "    _, total_likelihood, total_rel_abs_err, batch_cens_loss, batch_obs_loss = \\\n",
    "        tf.while_loop(condition, body,\n",
    "                      loop_vars=[idx,\n",
    "                                 partial_likelihood,\n",
    "                                 rel_abs_err,\n",
    "                                 total_cens_loss,\n",
    "                                 total_obs_loss],\n",
    "                      shape_invariants=[\n",
    "                          idx.get_shape(),\n",
    "                          partial_likelihood.get_shape(),\n",
    "                          rel_abs_err.get_shape(),\n",
    "                          total_cens_loss.get_shape(),\n",
    "                          total_obs_loss.get_shape()])\n",
    "    square_batch_size = tf.pow(batch_size, tf.constant(2))\n",
    "\n",
    "    def normarlize_loss(cost, size):\n",
    "        cast_size = tf.cast(size, dtype=tf.float32)\n",
    "        norm = tf.cond(tf.greater(cast_size, tf.constant(0.0)), lambda: tf.div(cost, cast_size), lambda: 0.0)\n",
    "        return norm\n",
    "\n",
    "    total_recon_loss = tf.add(normarlize_loss(batch_cens_loss, size=censored),\n",
    "                              normarlize_loss(batch_obs_loss, size=observed))\n",
    "    normalized_log_likelihood = normarlize_loss(total_likelihood, size=square_batch_size)\n",
    "    return normalized_log_likelihood, normarlize_loss(total_rel_abs_err, size=batch_size), total_recon_loss\n",
    "\n",
    "\n",
    "def l2_loss(scale):\n",
    "    l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "    return l2 * scale\n",
    "\n",
    "\n",
    "def l1_loss(scale):\n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "        scale=scale, scope=None\n",
    "    )\n",
    "    weights = tf.trainable_variables()  # all vars of your graph\n",
    "    l1 = tf.contrib.layers.apply_regularization(l1_regularizer, weights)\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of data:   time  label  current_year  default  payoff  int.rate  orig.upb  fico.score  \\\n",
      "0    57      1          2006        0       1     6.875    180000         680   \n",
      "1     5      1          2003        0       1     6.500    300000         758   \n",
      "2     5      1          2003        0       1     6.000    269000         769   \n",
      "3     5      1          2001        0       1     8.375    160000         661   \n",
      "4     5      1          2001        0       1     8.500     84000         715   \n",
      "\n",
      "   dti.r  ltv.r  ...  ppi.o.FRMA  equity.est  hpi.st.log12m  hpi.r.st.us  \\\n",
      "0     30     90  ...      -0.145   110482.80       0.049438     1.136017   \n",
      "1     36     66  ...       0.570   172589.30       0.117066     1.048596   \n",
      "2     36     77  ...       0.210    84684.58       0.025783     0.891385   \n",
      "3     48     85  ...       0.495    36270.05       0.132158     1.003634   \n",
      "4     49     90  ...       0.620    13180.12       0.050343     0.995011   \n",
      "\n",
      "   hpi.r.zip.st  st.unemp.r12m  st.unemp.r3m  TB10Y.r12m  T10Y3MM  \\\n",
      "0      0.892428            0.5          -0.1        0.91     0.14   \n",
      "1      1.425959            0.0           0.0       -1.25     2.83   \n",
      "2      1.377818            1.2           0.4       -1.60     2.41   \n",
      "3      1.217811            0.0           0.4       -1.50     0.01   \n",
      "4      1.545843            1.1           0.5       -1.50     0.01   \n",
      "\n",
      "   T10Y3MM.r12m  \n",
      "0         -0.82  \n",
      "1         -0.67  \n",
      "2         -0.82  \n",
      "3         -1.33  \n",
      "4         -1.33  \n",
      "\n",
      "[5 rows x 30 columns], data shape:(10000, 30)\n",
      "missing:0.0\n",
      "Encoding data:(10000, 30)\n",
      "head of data:   time  label  current_year  default  payoff  int.rate  orig.upb  fico.score  \\\n",
      "0    57      1          2006        0       1     6.875    180000         680   \n",
      "1     5      1          2003        0       1     6.500    300000         758   \n",
      "2     5      1          2003        0       1     6.000    269000         769   \n",
      "3     5      1          2001        0       1     8.375    160000         661   \n",
      "4     5      1          2001        0       1     8.500     84000         715   \n",
      "\n",
      "   dti.r  ltv.r  ...  ppi.o.FRMA  equity.est  hpi.st.log12m  hpi.r.st.us  \\\n",
      "0     30     90  ...      -0.145   110482.80       0.049438     1.136017   \n",
      "1     36     66  ...       0.570   172589.30       0.117066     1.048596   \n",
      "2     36     77  ...       0.210    84684.58       0.025783     0.891385   \n",
      "3     48     85  ...       0.495    36270.05       0.132158     1.003634   \n",
      "4     49     90  ...       0.620    13180.12       0.050343     0.995011   \n",
      "\n",
      "   hpi.r.zip.st  st.unemp.r12m  st.unemp.r3m  TB10Y.r12m  T10Y3MM  \\\n",
      "0      0.892428            0.5          -0.1        0.91     0.14   \n",
      "1      1.425959            0.0           0.0       -1.25     2.83   \n",
      "2      1.377818            1.2           0.4       -1.60     2.41   \n",
      "3      1.217811            0.0           0.4       -1.50     0.01   \n",
      "4      1.545843            1.1           0.5       -1.50     0.01   \n",
      "\n",
      "   T10Y3MM.r12m  \n",
      "0         -0.82  \n",
      "1         -0.67  \n",
      "2         -0.82  \n",
      "3         -1.33  \n",
      "4         -1.33  \n",
      "\n",
      "[5 rows x 30 columns], data shape:(10000, 30)\n",
      "Encoded:['t.act.12m', 't.del.30d.12m', 't.del.60d.12m'], one_hot:(10000, 62)   time  label  current_year  default  payoff  int.rate  orig.upb  fico.score  \\\n",
      "0    57      1          2006        0       1     6.875    180000         680   \n",
      "1     5      1          2003        0       1     6.500    300000         758   \n",
      "2     5      1          2003        0       1     6.000    269000         769   \n",
      "3     5      1          2001        0       1     8.375    160000         661   \n",
      "4     5      1          2001        0       1     8.500     84000         715   \n",
      "\n",
      "   dti.r  ltv.r  ...  t.del.30d.12m_12  t.del.60d.12m_0  t.del.60d.12m_1  \\\n",
      "0     30     90  ...                 0                1                0   \n",
      "1     36     66  ...                 0                1                0   \n",
      "2     36     77  ...                 0                1                0   \n",
      "3     48     85  ...                 0                1                0   \n",
      "4     49     90  ...                 0                1                0   \n",
      "\n",
      "   t.del.60d.12m_2  t.del.60d.12m_3  t.del.60d.12m_4  t.del.60d.12m_5  \\\n",
      "0                0                0                0                0   \n",
      "1                0                0                0                0   \n",
      "2                0                0                0                0   \n",
      "3                0                0                0                0   \n",
      "4                0                0                0                0   \n",
      "\n",
      "   t.del.60d.12m_6  t.del.60d.12m_7  t.del.60d.12m_8  \n",
      "0                0                0                0  \n",
      "1                0                0                0  \n",
      "2                0                0                0  \n",
      "3                0                0                0  \n",
      "4                0                0                0  \n",
      "\n",
      "[5 rows x 62 columns]\n",
      "head of dataset data:   int.rate  orig.upb  fico.score     dti.r     ltv.r  bal.repaid  \\\n",
      "0 -0.301876  0.474470   -0.600353 -0.365171  0.923452    0.357399   \n",
      "1 -0.713280  2.241742    0.808184  0.174677 -0.621406   -0.363720   \n",
      "2 -1.261818  1.785197    1.006824  0.174677  0.086654   -0.308027   \n",
      "3  1.343737  0.179925   -0.943458  1.254371  0.601607   -0.324116   \n",
      "4  1.480872 -0.939348    0.031683  1.344345  0.923452   -0.247335   \n",
      "\n",
      "   hpi.st.d.t.o  hpi.zip.o  hpi.zip.d.t.o  ppi.c.FRMA  ...  t.del.30d.12m_12  \\\n",
      "0      3.367343  -0.855293       1.065295   -0.953416  ...                 0   \n",
      "1     -0.740429   1.093700      -0.594514   -0.284572  ...                 0   \n",
      "2     -0.756680   0.230828      -0.813118   -0.224316  ...                 0   \n",
      "3     -0.752580  -0.963833      -0.678805    0.336068  ...                 0   \n",
      "4     -0.918855   0.181822      -0.672896    0.486708  ...                 0   \n",
      "\n",
      "   t.del.60d.12m_0  t.del.60d.12m_1  t.del.60d.12m_2  t.del.60d.12m_3  \\\n",
      "0                1                0                0                0   \n",
      "1                1                0                0                0   \n",
      "2                1                0                0                0   \n",
      "3                1                0                0                0   \n",
      "4                1                0                0                0   \n",
      "\n",
      "   t.del.60d.12m_4  t.del.60d.12m_5  t.del.60d.12m_6  t.del.60d.12m_7  \\\n",
      "0                0                0                0                0   \n",
      "1                0                0                0                0   \n",
      "2                0                0                0                0   \n",
      "3                0                0                0                0   \n",
      "4                0                0                0                0   \n",
      "\n",
      "   t.del.60d.12m_8  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "\n",
      "[5 rows x 57 columns], data shape:(10000, 57)\n",
      "data description:           int.rate      orig.upb    fico.score         dti.r         ltv.r  \\\n",
      "count  1.000000e+04  1.000000e+04  1.000000e+04  1.000000e+04  1.000000e+04   \n",
      "mean   8.743228e-16  8.952838e-17  1.321609e-16  1.399769e-16 -2.373213e-16   \n",
      "std    1.000050e+00  1.000050e+00  1.000050e+00  1.000050e+00  1.000050e+00   \n",
      "min   -2.907431e+00 -1.940802e+00 -7.462458e+00 -2.974431e+00 -4.226076e+00   \n",
      "25%   -7.132798e-01 -7.626208e-01 -6.906438e-01 -6.350940e-01 -4.282992e-01   \n",
      "50%   -2.760747e-02 -1.735300e-01  1.219739e-01 -5.272506e-03  2.797610e-01   \n",
      "75%    7.951993e-01  6.511972e-01  7.901263e-01  7.145235e-01  6.016065e-01   \n",
      "max    3.675023e+00  6.365378e+00  1.945849e+00  2.783937e+00  1.567143e+00   \n",
      "\n",
      "         bal.repaid  hpi.st.d.t.o     hpi.zip.o  hpi.zip.d.t.o    ppi.c.FRMA  \\\n",
      "count  1.000000e+04  1.000000e+04  1.000000e+04   1.000000e+04  1.000000e+04   \n",
      "mean  -1.278977e-17 -8.881784e-18 -1.190159e-16  -8.313350e-17  7.176482e-17   \n",
      "std    1.000050e+00  1.000050e+00  1.000050e+00   1.000050e+00  1.000050e+00   \n",
      "min   -5.091389e-01 -1.405697e+00 -2.093515e+00  -1.200301e+00 -3.429949e+00   \n",
      "25%   -2.801701e-01 -6.339568e-01 -6.469264e-01  -5.968777e-01 -6.054365e-01   \n",
      "50%   -1.753684e-01 -3.304061e-01 -2.501581e-01  -3.380987e-01  3.478643e-02   \n",
      "75%   -3.601785e-03  2.438546e-01  3.662304e-01   1.795578e-01  6.252979e-01   \n",
      "max    1.316752e+01  6.350394e+00  7.716064e+00   7.215728e+00  4.511827e+00   \n",
      "\n",
      "       ...  t.del.30d.12m_12  t.del.60d.12m_0  t.del.60d.12m_1  \\\n",
      "count  ...        10000.0000     10000.000000     10000.000000   \n",
      "mean   ...            0.0001         0.963100         0.024100   \n",
      "std    ...            0.0100         0.188526         0.153367   \n",
      "min    ...            0.0000         0.000000         0.000000   \n",
      "25%    ...            0.0000         1.000000         0.000000   \n",
      "50%    ...            0.0000         1.000000         0.000000   \n",
      "75%    ...            0.0000         1.000000         0.000000   \n",
      "max    ...            1.0000         1.000000         1.000000   \n",
      "\n",
      "       t.del.60d.12m_2  t.del.60d.12m_3  t.del.60d.12m_4  t.del.60d.12m_5  \\\n",
      "count     10000.000000     10000.000000     10000.000000     10000.000000   \n",
      "mean          0.006700         0.003400         0.001500         0.000500   \n",
      "std           0.081583         0.058213         0.038703         0.022356   \n",
      "min           0.000000         0.000000         0.000000         0.000000   \n",
      "25%           0.000000         0.000000         0.000000         0.000000   \n",
      "50%           0.000000         0.000000         0.000000         0.000000   \n",
      "75%           0.000000         0.000000         0.000000         0.000000   \n",
      "max           1.000000         1.000000         1.000000         1.000000   \n",
      "\n",
      "       t.del.60d.12m_6  t.del.60d.12m_7  t.del.60d.12m_8  \n",
      "count     10000.000000     10000.000000       10000.0000  \n",
      "mean          0.000400         0.000200           0.0001  \n",
      "std           0.019997         0.014141           0.0100  \n",
      "min           0.000000         0.000000           0.0000  \n",
      "25%           0.000000         0.000000           0.0000  \n",
      "50%           0.000000         0.000000           0.0000  \n",
      "75%           0.000000         0.000000           0.0000  \n",
      "max           1.000000         1.000000           1.0000  \n",
      "\n",
      "[8 rows x 57 columns]\n",
      "columns:['int.rate' 'orig.upb' 'fico.score' 'dti.r' 'ltv.r' 'bal.repaid'\n",
      " 'hpi.st.d.t.o' 'hpi.zip.o' 'hpi.zip.d.t.o' 'ppi.c.FRMA' 'TB10Y.d.t.o'\n",
      " 'FRMA30Y.d.t.o' 'ppi.o.FRMA' 'equity.est' 'hpi.st.log12m' 'hpi.r.st.us'\n",
      " 'hpi.r.zip.st' 'st.unemp.r12m' 'st.unemp.r3m' 'TB10Y.r12m' 'T10Y3MM'\n",
      " 'T10Y3MM.r12m' 't.act.12m_0' 't.act.12m_1' 't.act.12m_2' 't.act.12m_3'\n",
      " 't.act.12m_4' 't.act.12m_5' 't.act.12m_6' 't.act.12m_7' 't.act.12m_8'\n",
      " 't.act.12m_9' 't.act.12m_10' 't.act.12m_11' 't.act.12m_12'\n",
      " 't.del.30d.12m_0' 't.del.30d.12m_1' 't.del.30d.12m_2' 't.del.30d.12m_3'\n",
      " 't.del.30d.12m_4' 't.del.30d.12m_5' 't.del.30d.12m_6' 't.del.30d.12m_7'\n",
      " 't.del.30d.12m_8' 't.del.30d.12m_9' 't.del.30d.12m_10' 't.del.30d.12m_11'\n",
      " 't.del.30d.12m_12' 't.del.60d.12m_0' 't.del.60d.12m_1' 't.del.60d.12m_2'\n",
      " 't.del.60d.12m_3' 't.del.60d.12m_4' 't.del.60d.12m_5' 't.del.60d.12m_6'\n",
      " 't.del.60d.12m_7' 't.del.60d.12m_8']\n",
      "x:[-0.30187639  0.47446994 -0.60035295 -0.36517052  0.92345207  0.35739886\n",
      "  3.36734339 -0.8552933   1.06529496 -0.95341645  1.42735758  0.61766385\n",
      " -0.6033995   0.16616233 -0.58683744  1.14799302 -2.59755919  0.31462759\n",
      " -0.50174834  1.86634098 -1.79831605 -0.60246714  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ], t:57, e:0, len:10000\n",
      "x_shape:(10000, 57)\n",
      "end_time:72\n",
      "observed percent:0.0318\n",
      "shuffled x:[ 1.20660266 -0.71843895  1.27769692  1.9741666   0.92345207 -0.04562641\n",
      "  0.92002962 -0.67560408  0.81793847  1.67978268 -2.11137347 -1.95068017\n",
      " -0.27733652 -0.40633445  0.71460745  0.59795992 -0.04198785 -1.07106406\n",
      " -0.50174834 -1.40626485  0.46292613 -0.59555742  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ], t:37, e:0, len:10000\n",
      "num_examples:8000\n",
      "test:2000, valid:1600, train:8000, all: 11600\n",
      "categorical_flat:[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]\n",
      "len covariates:57, categorical:35\n",
      "imputation_values:[-0.027607470355740633, -0.17352995336914429, 0.10391575026652726, -0.0052725059705027374, 0.27976100827992123, -0.17477381021602584, -0.3303739811127087, -0.24217188945866244, -0.3428252254283816, 0.0347864283255566, -0.054256461163903966, -0.0689231647934416, -0.040199810624787106, -0.2890943006333361, -0.24413001232188639, -0.05274888970827871, -0.0629754898546519, -0.06328831675981932, -0.19505620133005064, -0.209976164935182, 0.3026806190090808, -0.3053493403307765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "imputation_values:[-0.027607470355740633, -0.17352995336914429, 0.10391575026652726, -0.0052725059705027374, 0.27976100827992123, -0.17477381021602584, -0.3303739811127087, -0.24217188945866244, -0.3428252254283816, 0.0347864283255566, -0.054256461163903966, -0.0689231647934416, -0.040199810624787106, -0.2890943006333361, -0.24413001232188639, -0.05274888970827871, -0.0629754898546519, -0.06328831675981932, -0.19505620133005064, -0.209976164935182, 0.3026806190090808, -0.3053493403307765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "observed fold:0.028125\n",
      "observed fold:0.0415\n",
      "observed fold:0.034375\n"
     ]
    }
   ],
   "source": [
    "def generate_data():\n",
    "    np.random.seed(31415)\n",
    "    data_frame = pandas.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\datasets\\\\data batches\\\\ndb10.csv', sep=',')\n",
    "    print(\"head of data:{}, data shape:{}\".format(data_frame.head(), data_frame.shape))\n",
    "    # x_data = data_frame[['age', 'sex', 'kappa', 'lambda', 'flc.grp', 'creatinine', 'mgus']]\n",
    "    # Preprocess\n",
    "    to_drop = ['time', 'label',\t'default', 'payoff', 'current_year']\n",
    "    print(\"missing:{}\".format(missing_proportion(data_frame.drop(labels=to_drop, axis=1))))\n",
    "    one_hot_encoder_list = ['t.act.12m', 't.del.30d.12m', 't.del.60d.12m']\n",
    "    data_frame = one_hot_encoder(data_frame, encode=one_hot_encoder_list)\n",
    "    t_data = data_frame[['time']]\n",
    "    e_data = data_frame[['default']]\n",
    "    dataset1 = data_frame.drop(labels=to_drop, axis=1)\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    ll_n = f_get_Normalization(np.asarray(dataset1.iloc[:,:22]), 'standard')\n",
    "    ll_p = pandas.DataFrame(ll_n, columns=dataset1.iloc[:,:22].columns)\n",
    "    ll = pandas.concat([ll_p, dataset1.iloc[:,22:].reindex(ll_p.index)], axis=1)\n",
    "    dataset = ll\n",
    "\n",
    "\n",
    "\n",
    "    print(\"head of dataset data:{}, data shape:{}\".format(dataset.head(), dataset.shape))\n",
    "    encoded_indices = one_hot_indices(dataset, one_hot_encoder_list)\n",
    "    print(\"data description:{}\".format(dataset.describe()))\n",
    "    covariates = np.array(dataset.columns.values)\n",
    "    print(\"columns:{}\".format(covariates))\n",
    "    x = np.array(dataset).reshape(dataset.shape)\n",
    "    t = np.array(t_data).reshape(len(t_data))\n",
    "    e = np.array(e_data).reshape(len(e_data))\n",
    "\n",
    "    print(\"x:{}, t:{}, e:{}, len:{}\".format(x[0], t[0], e[0], len(t)))\n",
    "    idx = np.arange(0, x.shape[0])\n",
    "    print(\"x_shape:{}\".format(x.shape))\n",
    "\n",
    "    np.random.shuffle(idx)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "    e = e[idx]\n",
    "    end_time = max(t)\n",
    "    print(\"end_time:{}\".format(end_time))\n",
    "    print(\"observed percent:{}\".format(sum(e) / len(e)))\n",
    "    print(\"shuffled x:{}, t:{}, e:{}, len:{}\".format(x[0], t[0], e[0], len(t)))\n",
    "\n",
    "    num_examples = int(0.80 * len(e))\n",
    "    print(\"num_examples:{}\".format(num_examples))\n",
    "    vali_example = int(0.20 * num_examples)\n",
    "    train_idx = idx[0: num_examples - vali_example]\n",
    "    valid_idx = idx[num_examples - vali_example: num_examples]\n",
    "    split = int(len(t) - num_examples)\n",
    "    test_idx = idx[num_examples: num_examples + split]\n",
    "\n",
    "    print(\"test:{}, valid:{}, train:{}, all: {}\".format(len(test_idx), len(valid_idx), num_examples,\n",
    "                                                        len(test_idx) + len(valid_idx) + num_examples))\n",
    "    # print(\"test_idx:{}, valid_idx:{},train_idx:{} \".format(test_idx, valid_idx, train_idx))\n",
    "\n",
    "    imputation_values = get_train_median_mode(x=np.array(x[train_idx]), categorial=encoded_indices)\n",
    "    print(\"imputation_values:{}\".format(imputation_values))\n",
    "    preprocessed = {\n",
    "        'train': formatted_data(x=x, t=t, e=e, idx=train_idx),\n",
    "        'test': formatted_data(x=x, t=t, e=e, idx=test_idx),\n",
    "        'valid': formatted_data(x=x, t=t, e=e, idx=valid_idx),\n",
    "        'end_t': end_time,\n",
    "        'covariates': covariates,\n",
    "        'one_hot_indices': encoded_indices,\n",
    "        'imputation_values': imputation_values\n",
    "    }\n",
    "    return preprocessed\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. risk_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raiber: it return the network output after applying the relu fuction to it\n",
    "def pt_given_x(x, hidden_dim, is_training, batch_norm, batch_size, input_dim, noise_alpha, keep_prob=0.9, reuse=False):\n",
    "    size = len(hidden_dim)\n",
    "    with tf.variable_scope('generate_t_given_x', reuse=reuse):\n",
    "        # Variables\n",
    "        # first we add the noise to the input, then using the function hidden_mlp_layers_noise we add the noise to each hidden layer \n",
    "        noise = uniform(dim=input_dim, batch_size=batch_size) * tf.gather(noise_alpha, 0) # tf.gather give us the value of noise_alpha inside the index 0\n",
    "        x_plus_noise = tf.concat([x, noise], axis=1) # the layer input shape would be 17 + 17 = 34 (17 is the number of features)\n",
    "        hidden_x = hidden_mlp_layers_noise(batch_norm=batch_norm, hidden_dim=hidden_dim,\n",
    "                                           is_training=is_training, keep_prob=keep_prob,\n",
    "                                           layer_input=x_plus_noise, size=size, batch_size=batch_size,\n",
    "                                           noise_alpha=noise_alpha) # hidden_x shape (?,100)\n",
    "\n",
    "        w_t, b_t = create_nn_weights('t', 'encoder', [hidden_x.get_shape().as_list()[1], 1])\n",
    "        # name:W_encoder_t, shape[100, 1]\n",
    "        #name:b_encoder_t, shape[1]\n",
    "        t_mu = mlp_neuron(hidden_x, w_t, b_t, activation=False) # mlp = tf.add(tf.matmul(layer_input, weights), biases)\n",
    "        # no activation is applied \n",
    "        logit = tf.nn.sigmoid(t_mu) \n",
    "        return tf.exp(t_mu), logit\n",
    "\n",
    "def discriminator(pair_one, pair_two, hidden_dim, is_training, batch_norm, scope, keep_prob=1, reuse=False):\n",
    "    size = len(hidden_dim)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Variables\n",
    "        print(\"scope:{}, pair_one:{}, pair_two:{}\".format(scope, pair_one.shape, pair_two.shape))\n",
    "        # create one structure for the input feature \n",
    "        hidden_pair_one = hidden_mlp_layers(batch_norm=batch_norm, hidden_dim=hidden_dim,\n",
    "                                            is_training=is_training, keep_prob=keep_prob,\n",
    "                                            layer_input=pair_one, size=size) # shape=(?, 50)\n",
    "\n",
    "        # this structure is for the time \n",
    "        hidden_pair_two = hidden_mlp_layers(batch_norm=batch_norm, hidden_dim=hidden_dim,\n",
    "                                            is_training=is_training, keep_prob=keep_prob,\n",
    "                                            layer_input=pair_two, size=size) #shape=(?, 50)\n",
    "        hidden_pairs = tf.concat([hidden_pair_one, hidden_pair_two], axis=1) #shape=(?, 100)\n",
    "        print(\"hidden_pairs:{}\".format(hidden_pairs.get_shape()))\n",
    "        #name:W_discriminator_logits, shape[100, 1]\n",
    "        #name:b_discriminator_logits, shape[1]\n",
    "        w_logit, b_logit = create_nn_weights('logits', 'discriminator', [hidden_dim[size - 1] * 2, 1])\n",
    "        f = mlp_neuron(layer_input=hidden_pairs, weights=w_logit, biases=b_logit, activation=False) \n",
    "        logit = tf.nn.sigmoid(f)\n",
    "\n",
    "    return tf.squeeze(logit), tf.squeeze(f)\n",
    "\n",
    "def discriminator_one(pair_one, pair_two, hidden_dim, is_training, batch_norm, keep_prob=1, reuse=False):\n",
    "    score, f = discriminator(pair_one=pair_one, pair_two=pair_two, scope='Discriminator_one', batch_norm=batch_norm,\n",
    "                             is_training=is_training,\n",
    "                             keep_prob=keep_prob, reuse=reuse, hidden_dim=hidden_dim)\n",
    "    return score, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. DATE-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATE_AE(object):\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 learning_rate,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 require_improvement,\n",
    "                 seed,\n",
    "                 num_iterations,\n",
    "                 hidden_dim,\n",
    "                 latent_dim,\n",
    "                 input_dim,\n",
    "                 num_examples,\n",
    "                 keep_prob,\n",
    "                 train_data,\n",
    "                 valid_data,\n",
    "                 test_data,\n",
    "                 end_t,\n",
    "                 gen_updates,\n",
    "                 covariates,\n",
    "                 imputation_values,\n",
    "                 sample_size,\n",
    "                 disc_updates,\n",
    "                 categorical_indices,\n",
    "                 l2_reg,\n",
    "                 max_epochs,\n",
    "                 path_large_data=\"\"\n",
    "                 ):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.disc_updates = disc_updates\n",
    "        self.gen_updates = gen_updates\n",
    "        self.latent_dim = latent_dim\n",
    "        self.path_large_data = path_large_data\n",
    "        self.seed = seed\n",
    "        self.require_improvement = require_improvement\n",
    "        self.num_iterations = num_iterations\n",
    "        self.learning_rate, self.beta1, self.beta2 = learning_rate, beta1, beta2\n",
    "        self.l2_reg = l2_reg\n",
    "        self.log_file = 'model.log'\n",
    "        logging.basicConfig(filename=self.log_file, filemode='w', level=logging.DEBUG)\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        self.batch_norm = True\n",
    "        self.covariates = covariates\n",
    "        self.sample_size = sample_size\n",
    "        self.z_sample_size = 10  # num of z_samples\n",
    "\n",
    "        self.config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        # self.config.gpu_options.per_process_gpu_memory_fraction = gpu_memory_fraction\n",
    "        # Load Data\n",
    "        self.train_x, self.train_t, self.train_e = train_data['x'], train_data['t'], train_data['e']\n",
    "        self.valid_x, self.valid_t, self.valid_e = valid_data['x'], valid_data['t'], valid_data['e']\n",
    "\n",
    "        self.test_x, self.test_t, self.test_e = test_data['x'], test_data['t'], test_data['e']\n",
    "        self.end_t = end_t\n",
    "        self.keep_prob = keep_prob\n",
    "        self.input_dim = input_dim\n",
    "        # self.imputation_values = imputation_values\n",
    "        self.imputation_values = np.zeros(shape=self.input_dim)\n",
    "        self.num_examples = num_examples\n",
    "        self.categorical_indices = categorical_indices\n",
    "        self.continuous_indices = np.setdiff1d(np.arange(input_dim), flatten_nested(categorical_indices))\n",
    "        print_features = \"input_dim:{}, continuous:{}, size:{}, categorical:{}, \" \\\n",
    "                         \"size{}\".format(self.input_dim,\n",
    "                                         self.continuous_indices,\n",
    "                                         len(\n",
    "                                             self.continuous_indices),\n",
    "                                         self.categorical_indices,\n",
    "                                         len(\n",
    "                                             self.categorical_indices))\n",
    "        print(print_features)\n",
    "        logging.debug(print_features)\n",
    "        print_model = \"model is DATE_AE\"\n",
    "        print(print_model)\n",
    "        logging.debug(\"Imputation values:{}\".format(imputation_values))\n",
    "        logging.debug(print_model)\n",
    "        self.model = 'DATE_AE'\n",
    "\n",
    "        self._build_graph()\n",
    "        self.train_cost, self.train_ci, self.train_t_rae, self.train_gen, self.train_disc, self.train_ranking, \\\n",
    "        self.train_layer_one_recon = [], [], [], [], [], [], []\n",
    "        self.valid_cost, self.valid_ci, self.valid_t_rae, self.valid_gen, self.valid_disc, self.valid_ranking, \\\n",
    "        self.valid_layer_one_recon = [], [], [], [], [], [], []\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self.G = tf.Graph()\n",
    "        with self.G.as_default():\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, self.input_dim], name='x')\n",
    "            self.e = tf.placeholder(tf.float32, shape=[None], name='e')\n",
    "            self.t = tf.placeholder(tf.float32, shape=[None], name='t')\n",
    "            self.t_lab = tf.placeholder(tf.float32, shape=[None], name='t_lab')\n",
    "            # are used to feed data into our queue\n",
    "            self.batch_size_tensor = tf.placeholder(tf.int32, shape=[], name='batch_size')\n",
    "            self.risk_set = tf.placeholder(tf.float32, shape=[None, None])\n",
    "            self.impute_mask = tf.placeholder(tf.float32, shape=[None, self.input_dim], name='impute_mask')\n",
    "            self.is_training = tf.placeholder(tf.bool)\n",
    "            self.noise_dim = len(self.hidden_dim) + 1\n",
    "            self.noise_alpha = tf.placeholder(tf.float32, shape=[self.noise_dim])\n",
    "\n",
    "            self._objective()\n",
    "            self.session = tf.Session(config=self.config)\n",
    "\n",
    "            self.capacity = 1400\n",
    "            self.coord = tf.train.Coordinator()\n",
    "            enqueue_thread = threading.Thread(target=self.enqueue)\n",
    "            self.queue = tf.RandomShuffleQueue(capacity=self.capacity, dtypes=[tf.float32, tf.float32, tf.float32],\n",
    "                                               shapes=[[self.input_dim], [], []], min_after_dequeue=self.batch_size)\n",
    "            # self.queue = tf.FIFOQueue(capacity=self.capacity, dtypes=[tf.float32, tf.float32, tf.float32],\n",
    "            #                           shapes=[[self.input_dim], [], []])\n",
    "            self.enqueue_op = self.queue.enqueue_many([self.x, self.t, self.e])\n",
    "            # enqueue_thread.isDaemon()\n",
    "            enqueue_thread.start()\n",
    "            dequeue_op = self.queue.dequeue()\n",
    "            self.x_batch, self.t_batch, self.e_batch = tf.train.batch(dequeue_op, batch_size=self.batch_size,\n",
    "                                                                      capacity=self.capacity)\n",
    "            self.threads = tf.train.start_queue_runners(coord=self.coord, sess=self.session)\n",
    "\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.current_dir = os.getcwd()\n",
    "            self.save_path = \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\summaries\\\\ndb_d\\\\d10\\\\{0}_model\".format(self.model)\n",
    "            self.train_writer = tf.summary.FileWriter(self.save_path, self.session.graph)\n",
    "\n",
    "    def _objective(self):\n",
    "        self.num_batches = self.num_examples / self.batch_size\n",
    "        logging.debug(\"num batches:{}, batch_size:{} epochs:{}\".format(self.num_batches, self.batch_size,\n",
    "                                                                       int(self.num_iterations / self.num_batches)))\n",
    "        self._build_model()\n",
    "        self.reg_loss = l2_loss(self.l2_reg) + l1_loss(self.l2_reg)\n",
    "        self.cost = self.t_regularization_loss + self.disc_one_loss + self.disc_two_loss + self.gen_one_loss + \\\n",
    "                    self.gen_two_loss + self.layer_one_recon\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1,\n",
    "                                           beta2=self.beta2)\n",
    "\n",
    "        dvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Discriminator_one\")\n",
    "        dvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Discriminator_two\")\n",
    "        genvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generate_t_given_x\")\n",
    "        genvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generate_t_given_z\")\n",
    "\n",
    "        self.disc_solver = optimizer.minimize(self.disc_one_loss + self.disc_two_loss, var_list=dvars1 + dvars2)\n",
    "        self.gen_solver = optimizer.minimize(\n",
    "            self.gen_one_loss + self.gen_two_loss + self.t_regularization_loss + self.layer_one_recon,\n",
    "            var_list=genvars1 + genvars2)\n",
    "\n",
    "    def _build_model(self):\n",
    "        self._denoising_date()\n",
    "        self._risk_date()\n",
    "\n",
    "    @staticmethod\n",
    "    def log(x):\n",
    "        return tf.log(x + 1e-8)\n",
    "\n",
    "    def _risk_date(self):\n",
    "        def expand_t_dim(t):\n",
    "            return tf.expand_dims(t, axis=1)\n",
    "\n",
    "        indices_lab = tf.where(tf.equal(tf.constant(1.0, dtype=tf.float32), self.e))\n",
    "        z_lab = tf.squeeze(tf.gather(self.z_real, indices_lab), axis=[1])\n",
    "        t_lab_exp = expand_t_dim(self.t_lab)\n",
    "\n",
    "        t_gen = pt_given_z(z=self.z_real, hidden_dim=self.hidden_dim, is_training=self.is_training,\n",
    "                           batch_norm=self.batch_norm, keep_prob=self.keep_prob, batch_size=self.batch_size_tensor,\n",
    "                           latent_dim=self.latent_dim, noise_alpha=self.noise_alpha)\n",
    "\n",
    "        # Discriminator B\n",
    "        d_two_real, f_two_real = discriminator_two(pair_one=z_lab, pair_two=t_lab_exp, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   keep_prob=self.keep_prob)  # (z_nc, t_nc)\n",
    "        d_two_fake, f_two_fake = discriminator_two(pair_one=self.z_real, pair_two=t_gen, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   reuse=True, keep_prob=self.keep_prob)  # (z, t_gen)\n",
    "\n",
    "        # Discriminator loss\n",
    "        self.disc_two_loss = -tf.reduce_mean(self.log(d_two_real)) - tf.reduce_mean(self.log(1 - d_two_fake))\n",
    "\n",
    "        # Generator loss\n",
    "        self.gen_two_loss = tf.reduce_mean(f_two_real) - tf.reduce_mean(f_two_fake)\n",
    "        self.disc_logit = d_two_fake # added b raiber \n",
    "        self.disc_f = f_two_fake # added by raiber \n",
    "        self.predicted_time = tf.squeeze(t_gen)\n",
    "        self.ranking_partial_lik, self.total_rae, self.total_t_recon_loss = \\\n",
    "            batch_metrics(e=self.e,\n",
    "                          risk_set=self.risk_set,\n",
    "                          predicted=self.predicted_time,\n",
    "                          batch_size=self.batch_size_tensor,\n",
    "                          empirical=self.t)\n",
    "\n",
    "        # self.t_regularization_loss = tf.add(self.ranking_partial_lik, self.total_t_recon_loss)\n",
    "        self.t_regularization_loss = self.total_t_recon_loss\n",
    "        self.t_mse = tf.losses.mean_squared_error(labels=self.t_lab,\n",
    "                                                  predictions=tf.gather(self.predicted_time, indices_lab))\n",
    "\n",
    "    def _denoising_date(self):\n",
    "        self.z_real = generate_z_given_x(latent_dim=self.latent_dim,\n",
    "                                         is_training=self.is_training,\n",
    "                                         batch_norm=self.batch_norm,\n",
    "                                         input_dim=self.input_dim, batch_size=self.batch_size_tensor,\n",
    "                                         hidden_dim=self.hidden_dim, x=self.impute_mask, keep_prob=self.keep_prob,\n",
    "                                         reuse=True, sample_size=self.z_sample_size)\n",
    "\n",
    "        z_ones = np.ones(shape=self.latent_dim, dtype=np.float32)\n",
    "        print(\"z_ones:{}\".format(z_ones.shape))\n",
    "\n",
    "        z_fake = tf.distributions.Uniform(low=-z_ones, high=z_ones).sample(sample_shape=[self.batch_size_tensor])\n",
    "        x_fake = generate_x_given_z(z=z_fake, latent_dim=self.latent_dim,\n",
    "                                    is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                    hidden_dim=self.hidden_dim, keep_prob=self.keep_prob,\n",
    "                                    batch_size=self.batch_size_tensor, input_dim=self.input_dim)\n",
    "\n",
    "        self.x_recon = generate_x_given_z(z=self.z_real, latent_dim=self.latent_dim,\n",
    "                                          is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                          hidden_dim=self.hidden_dim, reuse=True, keep_prob=self.keep_prob,\n",
    "                                          batch_size=self.batch_size_tensor, input_dim=self.input_dim)\n",
    "\n",
    "        z_rec = generate_z_given_x(x=x_fake, latent_dim=self.latent_dim,\n",
    "                                   is_training=self.is_training,\n",
    "                                   batch_norm=self.batch_norm,\n",
    "                                   input_dim=self.input_dim, batch_size=self.batch_size_tensor,\n",
    "                                   hidden_dim=self.hidden_dim, reuse=True, keep_prob=self.keep_prob,\n",
    "                                   sample_size=self.z_sample_size)\n",
    "        # Reconstruction Loss\n",
    "\n",
    "        self.x_recon_loss = x_reconstruction(x_recon=self.x_recon, x=self.x,\n",
    "                                             categorical_indices=self.categorical_indices,\n",
    "                                             continuous_indices=self.continuous_indices,\n",
    "                                             batch_size=self.batch_size_tensor)\n",
    "\n",
    "        self.z_recon_loss = tf.losses.mean_squared_error(z_fake, z_rec)\n",
    "        self.layer_one_recon = tf.add(self.x_recon_loss, self.z_recon_loss)\n",
    "\n",
    "        d_one_real, f_one_real = discriminator_one(pair_one=self.impute_mask, pair_two=self.z_real,\n",
    "                                                   hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   keep_prob=self.keep_prob)  # real\n",
    "        d_one_fake, f_one_fake = discriminator_one(pair_one=x_fake, pair_two=z_fake, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   reuse=True, keep_prob=self.keep_prob)  # fake\n",
    "\n",
    "        self.disc_one_loss = -tf.reduce_mean(self.log(d_one_real)) - tf.reduce_mean(self.log(1 - d_one_fake))\n",
    "\n",
    "        # Generator loss\n",
    "        self.gen_one_loss = tf.reduce_mean(f_one_real) - tf.reduce_mean(f_one_fake)\n",
    "\n",
    "    def predict_concordance_index(self, x, t, e, outcomes=None):\n",
    "        input_size = x.shape[0]\n",
    "        i = 0\n",
    "        num_batches = input_size / self.batch_size\n",
    "        predicted_time = np.zeros(shape=input_size, dtype=np.int)\n",
    "        total_ranking = 0.0\n",
    "        total_rae = 0.0\n",
    "        total_cost = 0.0\n",
    "        total_gen_loss = 0.0\n",
    "        total_disc_loss = 0.0\n",
    "        total_layer_one_recon = 0.0\n",
    "        total_t_reg_loss = 0.0\n",
    "        total_reg = 0.0\n",
    "        total_mse = 0.0\n",
    "        while i < input_size:\n",
    "            # The ending index for the next batch is denoted j.\n",
    "            j = min(i + self.batch_size, input_size)\n",
    "            feed_dict = self.batch_feed_dict(e=e, i=i, j=j, t=t, x=x, outcomes=outcomes)\n",
    "            cost, ranking, gen_loss, rae, reg, disc_loss, layer_one_recon, t_reg_loss, t_mse = self.session.run(\n",
    "                [self.cost, self.ranking_partial_lik, self.gen_one_loss, self.total_rae,\n",
    "                 self.reg_loss,\n",
    "                 self.disc_one_loss, self.layer_one_recon, self.t_regularization_loss, self.t_mse],\n",
    "                feed_dict=feed_dict)\n",
    "\n",
    "            temp_pred_time = []\n",
    "            for p in range(self.sample_size):\n",
    "                gen_time = self.session.run(self.predicted_time, feed_dict=feed_dict)\n",
    "                temp_pred_time.append(gen_time)\n",
    "\n",
    "            temp_pred_time = np.array(temp_pred_time)\n",
    "            # print(\"temp_pred_time:{}\".format(temp_pred_time.shape))\n",
    "            predicted_time[i:j] = np.median(temp_pred_time, axis=0)\n",
    "\n",
    "            total_ranking += ranking\n",
    "            total_cost += cost\n",
    "            total_rae += rae\n",
    "            total_gen_loss += gen_loss\n",
    "            total_reg += reg\n",
    "            total_layer_one_recon += layer_one_recon\n",
    "            total_disc_loss += disc_loss\n",
    "            total_t_reg_loss += t_reg_loss\n",
    "            total_mse += t_mse\n",
    "            i = j\n",
    "\n",
    "        predicted_event_times = predicted_time.reshape(input_size)\n",
    "        #RAIBER NEW CHANGE\n",
    "        #ci_index = concordance_index(event_times=t, predicted_scores=predicted_event_times.tolist(),\n",
    "        #                            event_observed=e)\n",
    "        \n",
    "        ci_index = 0\n",
    "        def batch_average(total):\n",
    "            return total / num_batches\n",
    "\n",
    "        return ci_index, batch_average(total_cost), batch_average(total_rae), batch_average(\n",
    "            total_ranking), batch_average(\n",
    "            total_gen_loss), batch_average(total_reg), batch_average(total_disc_loss), batch_average(\n",
    "            total_layer_one_recon), batch_average(total_t_reg_loss), batch_average(total_mse)\n",
    "\n",
    "    def batch_feed_dict(self, e, i, j, t, x, outcomes):\n",
    "        batch_x = x[i:j, :]\n",
    "        batch_t = t[i:j]\n",
    "        batch_risk = risk_set(batch_t)\n",
    "        batch_impute_mask = get_missing_mask(batch_x, self.imputation_values)\n",
    "        batch_e = e[i:j]\n",
    "        idx_observed = batch_e == 1\n",
    "        feed_dict = {self.x: batch_x,\n",
    "                     self.impute_mask: batch_impute_mask,\n",
    "                     self.t: batch_t,\n",
    "                     self.t_lab: batch_t[idx_observed],\n",
    "                     self.e: batch_e,\n",
    "                     self.risk_set: batch_risk,\n",
    "                     self.batch_size_tensor: len(batch_t),\n",
    "                     self.is_training: False,\n",
    "                     self.noise_alpha: np.ones(shape=self.noise_dim)}\n",
    "        # TODO replace with abstract methods\n",
    "\n",
    "        updated_feed_dic = self.outcomes_function(idx=i, j=j, feed_dict=feed_dict, outcomes=outcomes)\n",
    "        return updated_feed_dic\n",
    "\n",
    "    def outcomes_function(self, idx, j, feed_dict, outcomes):\n",
    "        return feed_dict\n",
    "\n",
    "    def train_neural_network(self):\n",
    "        train_print = \"Training {0} Model:\".format(self.model)\n",
    "        params_print = \"Parameters:, l2_reg:{}, learning_rate:{},\" \\\n",
    "                       \" momentum: beta1={} beta2={}, batch_size:{}, batch_norm:{},\" \\\n",
    "                       \" hidden_dim:{}, latent_dim:{}, num_of_batches:{}, keep_prob:{}, disc_update:{}\" \\\n",
    "            .format(self.l2_reg, self.learning_rate, self.beta1, self.beta2, self.batch_size,\n",
    "                    self.batch_norm, self.hidden_dim, self.latent_dim, self.num_batches, self.keep_prob,\n",
    "                    self.disc_updates)\n",
    "        print(train_print)\n",
    "        print(params_print)\n",
    "        logging.debug(train_print)\n",
    "        logging.debug(params_print)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        best_ci = 0\n",
    "        best_t_reg = np.inf\n",
    "        best_validation_epoch = 0\n",
    "        last_improvement = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        epochs = 0\n",
    "        show_all_variables()\n",
    "        j = 0\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            # Batch Training\n",
    "            run_options = tf.RunOptions(timeout_in_ms=4000)\n",
    "            x_batch, t_batch, e_batch = self.session.run([self.x_batch, self.t_batch, self.e_batch],\n",
    "                                                         options=run_options)\n",
    "            risk_batch = risk_set(data_t=t_batch)\n",
    "            batch_impute_mask = get_missing_mask(x_batch, self.imputation_values)\n",
    "            batch_size = len(t_batch)\n",
    "            idx_observed = e_batch == 1\n",
    "            # TODO simplify batch processing\n",
    "            feed_dict_train = {self.x: x_batch,\n",
    "                               self.impute_mask: batch_impute_mask,\n",
    "                               self.t: t_batch,\n",
    "                               self.t_lab: t_batch[idx_observed],\n",
    "                               self.e: e_batch,\n",
    "                               self.risk_set: risk_batch, self.batch_size_tensor: batch_size, self.is_training: True,\n",
    "                               self.noise_alpha: np.ones(shape=self.noise_dim)}\n",
    "            for k in range(self.disc_updates):\n",
    "                _ = self.session.run([self.disc_solver], feed_dict=feed_dict_train)\n",
    "\n",
    "            for m in range(self.gen_updates):\n",
    "                _ = self.session.run([self.gen_solver], feed_dict=feed_dict_train)\n",
    "\n",
    "            summary, train_time, train_cost, train_ranking, train_rae, train_reg, train_gen, train_layer_one_recon, \\\n",
    "            train_t_reg, train_t_mse, train_disc = self.session.run(\n",
    "                [self.merged, self.predicted_time, self.cost, self.ranking_partial_lik, self.total_rae,\n",
    "                 self.reg_loss, self.gen_one_loss, self.layer_one_recon, self.t_regularization_loss, self.t_mse,\n",
    "                 self.disc_one_loss],\n",
    "                feed_dict=feed_dict_train)\n",
    "            try:\n",
    "                #RAIBER NEW CHANGE\n",
    "                train_ci = 0.0\n",
    "                #train_ci = concordance_index(event_times=t_batch,\n",
    "                #                             predicted_scores=train_time.reshape(t_batch.shape),\n",
    "                #                             event_observed=e_batch)\n",
    "            except IndexError:\n",
    "                train_ci = 0.0\n",
    "                print(\"C-Index IndexError\")\n",
    "\n",
    "            tf.verify_tensor_all_finite(train_cost, \"Training Cost has Nan or Infinite\")\n",
    "            if j >= self.num_examples:\n",
    "                epochs += 1\n",
    "                is_epoch = True\n",
    "                # idx = 0\n",
    "                j = 0\n",
    "            else:\n",
    "                # idx = j\n",
    "                j += self.batch_size\n",
    "                is_epoch = False\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                train_print = \"it:{}, trainCI:{}, train_ranking:{}, train_RAE:{},  train_Gen:{}, train_Disc:{}, \" \\\n",
    "                              \"train_reg:{}, train_t_reg:{}, train_t_mse:{}, train_layer_one_recon:{}\".format(\n",
    "                    i, train_ci, train_ranking, train_rae, train_gen, train_disc, train_reg, train_t_reg, train_t_mse,\n",
    "                    train_layer_one_recon)\n",
    "                print(train_print)\n",
    "                logging.debug(train_print)\n",
    "\n",
    "            if is_epoch or (i == (self.num_iterations - 1)):\n",
    "                improved_str = ''\n",
    "                # Calculate  Vaid CI the CI\n",
    "                self.train_ci.append(train_ci)\n",
    "                self.train_cost.append(train_cost)\n",
    "                self.train_t_rae.append(train_rae)\n",
    "                self.train_gen.append(train_gen)\n",
    "                self.train_disc.append(train_disc)\n",
    "                self.train_ranking.append(train_ranking)\n",
    "                self.train_layer_one_recon.append(train_layer_one_recon)\n",
    "\n",
    "                self.train_writer.add_summary(summary, i)\n",
    "                valid_ci, valid_cost, valid_rae, valid_ranking, valid_gen, valid_reg, valid_disc, \\\n",
    "                valid_layer_one_recon, valid_t_reg, valid_t_mse = \\\n",
    "                    self.predict_concordance_index(\n",
    "                        x=self.valid_x,\n",
    "                        e=self.valid_e,\n",
    "                        t=self.valid_t)\n",
    "                self.valid_cost.append(valid_cost)\n",
    "                self.valid_ci.append(valid_ci)\n",
    "                self.valid_t_rae.append(valid_rae)\n",
    "                self.valid_gen.append(valid_gen)\n",
    "                self.valid_disc.append(valid_disc)\n",
    "                self.valid_ranking.append(valid_ranking)\n",
    "                self.valid_layer_one_recon.append(valid_layer_one_recon)\n",
    "                tf.verify_tensor_all_finite(valid_cost, \"Validation Cost has Nan or Infinite\")\n",
    "\n",
    "                if valid_t_reg < best_t_reg:\n",
    "                    self.saver.save(sess=self.session, save_path=self.save_path)\n",
    "                    best_validation_epoch = epochs\n",
    "                    best_t_reg = valid_t_reg\n",
    "                    last_improvement = i\n",
    "                    improved_str = '*'\n",
    "                    # Save  Best Perfoming all variables of the TensorFlow graph to file.\n",
    "                # update best validation accuracy\n",
    "                optimization_print = \"Iteration: {} epochs:{}, Training: RAE:{}, Loss: {},\" \\\n",
    "                                     \" Ranking:{}, Reg:{}, Gen:{}, Disc:{}, Recon_One:{}, T_Reg:{},T_MSE:{},  CI:{}\" \\\n",
    "                                     \" Validation RAE:{} Loss:{}, Ranking:{}, Reg:{}, Gen:{}, Disc:{}, \" \\\n",
    "                                     \"Recon_One:{}, T_Reg:{}, T_MSE:{}, CI:{}, {}\" \\\n",
    "                    .format(i + 1, epochs, train_rae, train_cost, train_ranking, train_reg, train_gen,\n",
    "                            train_disc, train_layer_one_recon, train_t_reg, train_t_mse,\n",
    "                            train_ci, valid_rae, valid_cost, valid_ranking, valid_reg, valid_gen, valid_disc,\n",
    "                            valid_layer_one_recon, valid_t_reg, valid_t_mse, valid_ci, improved_str)\n",
    "\n",
    "                print(optimization_print)\n",
    "                logging.debug(optimization_print)\n",
    "                if i - last_improvement > self.require_improvement or math.isnan(\n",
    "                        train_cost) or epochs >= self.max_epochs:\n",
    "                    # if i - last_improvement > self.require_improvement:\n",
    "                    print(\"No improvement found in a while, stopping optimization.\")\n",
    "                    # Break out from the for-loop.\n",
    "                    break\n",
    "        # Ending time.\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_dif = end_time - start_time\n",
    "        time_dif_print = \"Time usage: \" + str(timedelta(seconds=int(round(time_dif))))\n",
    "        print(time_dif_print)\n",
    "        logging.debug(time_dif_print)\n",
    "        # shutdown everything to avoid zombies\n",
    "        self.session.run(self.queue.close(cancel_pending_enqueues=True))\n",
    "        self.coord.request_stop()\n",
    "        self.coord.join(self.threads)\n",
    "        return best_validation_epoch, epochs\n",
    "\n",
    "    def train_test(self, train=True):\n",
    "\n",
    "        def get_dict(x, t, e):\n",
    "            observed_idx = e == 1\n",
    "            feed_dict = {self.x: x,\n",
    "                         self.impute_mask: get_missing_mask(x, self.imputation_values),\n",
    "                         self.t: t,\n",
    "                         self.t_lab: t[observed_idx],\n",
    "                         self.e: e,\n",
    "                         self.batch_size_tensor: len(t),\n",
    "                         self.is_training: False, self.noise_alpha: np.ones(shape=self.noise_dim)}\n",
    "            return {'feed_dict': feed_dict, 'outcomes': {}}\n",
    "\n",
    "        session_dict = {'Test': get_dict(x=self.test_x, t=self.test_t, e=self.test_e),\n",
    "                        'Train': get_dict(x=self.train_x, t=self.train_t, e=self.train_e),\n",
    "                        'Valid': get_dict(x=self.valid_x, t=self.valid_t, e=self.valid_e)}\n",
    "\n",
    "        if train:\n",
    "            best_epoch, epochs = self.train_neural_network()\n",
    "            self.time_related_metrics(best_epoch, epochs, session_dict=session_dict)\n",
    "        else:\n",
    "            self.generate_statistics(data_x=self.test_x, data_e=self.test_e, data_t=self.test_t, name='Test',\n",
    "                                     session_dict=session_dict['Test'])\n",
    "\n",
    "        self.session.close()\n",
    "\n",
    "    def time_related_metrics(self, best_epoch, epochs, session_dict):\n",
    "        #plot_cost(training=self.train_cost, validation=self.valid_cost, model=self.model, name=\"Cost\",\n",
    "        #          epochs=epochs,\n",
    "        #          best_epoch=best_epoch)\n",
    "        #plot_cost(training=self.train_ci, validation=self.valid_ci, model=self.model, name=\"CI\",\n",
    "        #          epochs=epochs,\n",
    "        #          best_epoch=best_epoch)\n",
    "        #plot_cost(training=self.train_t_rae, validation=self.valid_t_rae, model=self.model, name=\"RAE\",\n",
    "        #          epochs=epochs,\n",
    "        #          best_epoch=best_epoch)\n",
    "        #plot_cost(training=self.train_ranking, validation=self.valid_ranking, model=self.model, name=\"Rank\",\n",
    "        #          epochs=epochs,\n",
    "        #          best_epoch=best_epoch)\n",
    "        #plot_cost(training=self.train_gen, validation=self.valid_gen, model=self.model, name=\"Gen_Loss\",\n",
    "        #          epochs=epochs, best_epoch=best_epoch)\n",
    "\n",
    "        #plot_cost(training=self.train_disc, validation=self.valid_disc, model=self.model, name=\"Disc_Loss\",\n",
    "        #          epochs=epochs, best_epoch=best_epoch)\n",
    "\n",
    "        #plot_cost(training=self.train_layer_one_recon, validation=self.valid_layer_one_recon, model=self.model,\n",
    "        #          name=\"Recon\",\n",
    "        #         epochs=epochs, best_epoch=best_epoch)\n",
    "         # TEST\n",
    "        self.generate_statistics(data_x=self.test_x, data_e=self.test_e, data_t=self.test_t, name='Test',\n",
    "                                 session_dict=session_dict['Test'], t_train_R=self.train_t, y_train_R=self.train_e)\n",
    "\n",
    "        # VALID\n",
    "        self.generate_statistics(data_x=self.valid_x, data_e=self.valid_e, data_t=self.valid_t, name='Valid',\n",
    "                                 session_dict=session_dict['Valid'], t_train_R=self.train_t, y_train_R=self.train_e)\n",
    "        # TRAIN\n",
    "        self.generate_statistics(data_x=self.train_x, data_e=self.train_e, data_t=self.train_t, name='Train',\n",
    "                                 session_dict=session_dict['Train'], t_train_R=self.train_t, y_train_R=self.train_e)\n",
    "      \n",
    "\n",
    "    def generate_statistics(self, data_x, data_e, data_t, name, session_dict, t_train_R, y_train_R, save=True):\n",
    "        self.saver.restore(sess=self.session, save_path=self.save_path)\n",
    "        ci, cost, rae, ranking, gen, reg, disc, layer_one_recon, t_reg, t_mse = \\\n",
    "            self.predict_concordance_index(x=data_x,\n",
    "                                           e=data_e,\n",
    "                                           t=data_t,\n",
    "                                           outcomes=\n",
    "                                           session_dict[\n",
    "                                               'outcomes'])\n",
    "\n",
    "        observed_idx = self.extract_observed_death(name=name, observed_e=data_e, observed_t=data_t, save=save)\n",
    "\n",
    "        median_predicted_time, median_disc_prob, median_disc_score, median_prob_t_gen = self.median_predict_time(session_dict)\n",
    "\n",
    "        if name == 'Test':\n",
    "            self.save_time_samples(x=data_x[observed_idx], e=data_e[observed_idx],\n",
    "                                   t=data_t[observed_idx], name='obs_samples_predicted', cens=False)\n",
    "\n",
    "            self.save_time_samples(x=data_x[np.logical_not(observed_idx)], e=data_e[np.logical_not(observed_idx)],\n",
    "                                   t=data_t[np.logical_not(observed_idx)], name='cen_samples_predicted', cens=True)\n",
    "            \n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\{}_predicted_time'.format(name), median_predicted_time)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\{}_disc_prob'.format(name), median_disc_prob)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\{}_disc_score'.format(name), median_disc_score)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\{}_prob_t_gen'.format(name), median_prob_t_gen)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\{}_empirical_time'.format(name), data_t)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\{}_data_e'.format(name), data_e)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\{}_t_train'.format(name), t_train_R)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\{}_y_train'.format(name), y_train_R)\n",
    "        \n",
    "        observed_empirical = data_t[observed_idx]\n",
    "        observed_predicted = median_predicted_time[observed_idx]\n",
    "        #RAIBER NEW CHANGE\n",
    "        #observed_ci = concordance_index(event_times=observed_empirical, predicted_scores=observed_predicted,\n",
    "        #                                event_observed=data_e[observed_idx])\n",
    "        observed_ci = 0\n",
    "\n",
    "        corr = spearmanr(observed_empirical, observed_predicted)\n",
    "        results = \":{} RAE:{}, Loss:{}, Gen:{}, Disc:{}, Reg:{}, Ranking{}, Recon:{}, T_Reg:{},T_MSE:{},\" \\\n",
    "                  \" CI:{}, Observed: CI:{}, \" \\\n",
    "                  \"Correlation:{}\".format(name, rae, cost, gen, disc, reg, ranking, layer_one_recon, t_reg, t_mse, ci,\n",
    "                                          observed_ci, corr)\n",
    "        logging.debug(results)\n",
    "        print(results)\n",
    "\n",
    "    def median_predict_time(self, session_dict):\n",
    "        #pdb.set_trace()\n",
    "        predicted_time = []\n",
    "        disc_prob = []\n",
    "        disc_score = []\n",
    "        prob_t_gen = []\n",
    "        for p in range(self.sample_size):\n",
    "            gen_time, disc_prob1, disc_score1, prob_t_gen1 = self.session.run([self.predicted_time, self.disc_logit, self.disc_f, self.probability_t_gen], feed_dict=session_dict['feed_dict'])\n",
    "            predicted_time.append(gen_time)\n",
    "            disc_prob.append(disc_prob1)\n",
    "            disc_score.append(disc_score1)\n",
    "            prob_t_gen.append(prob_t_gen1)\n",
    "        predicted_time = np.array(predicted_time)\n",
    "        disc_prob = np.array(disc_prob)\n",
    "        disc_score = np.array(disc_score)\n",
    "        prob_t_gen = np.array(prob_t_gen)\n",
    "        # print(\"predicted_time_shape:{}\".format(predicted_time.shape))\n",
    "        return np.median(predicted_time, axis=0), np.median(disc_prob, axis=0), np.median(disc_score, axis=0), np.median(prob_t_gen, axis=0)\n",
    "\n",
    "    def save_time_samples(self, x, t, e, name, cens=False):\n",
    "        predicted_time = self.generate_time_samples(e, x)\n",
    "        plot_predicted_distribution(predicted=predicted_time, empirical=t, data='Test_' + name, cens=cens)\n",
    "        return\n",
    "\n",
    "    def generate_time_samples(self, e, x):\n",
    "        # observed = e == 1\n",
    "        feed_dict = {self.x: x,\n",
    "                     self.impute_mask: get_missing_mask(x, self.imputation_values),\n",
    "                     # self.t: t,\n",
    "                     # self.t_lab: t[observed],\n",
    "                     self.e: e,\n",
    "                     # self.risk_set: risk_set(t),\n",
    "                     self.batch_size_tensor: len(x),\n",
    "                     self.is_training: False, self.noise_alpha: np.ones(shape=self.noise_dim)}\n",
    "        predicted_time = []\n",
    "        for p in range(self.sample_size):\n",
    "            gen_time = self.session.run(self.predicted_time, feed_dict=feed_dict)\n",
    "            predicted_time.append(gen_time)\n",
    "        predicted_time = np.array(predicted_time)\n",
    "        return predicted_time\n",
    "\n",
    "    def enqueue(self):\n",
    "        \"\"\" Iterates over our data puts small junks into our queue.\"\"\"\n",
    "        # TensorFlow Input Pipelines for Large Data Sets\n",
    "        # ischlag.github.io\n",
    "        # http://ischlag.github.io/2016/11/07/tensorflow-input-pipeline-for-large-datasets/\n",
    "        # http://web.stanford.edu/class/cs20si/lectures/slides_09.pdf\n",
    "        under = 0\n",
    "        max = len(self.train_x)\n",
    "        try:\n",
    "            while not self.coord.should_stop():\n",
    "                # print(\"starting to write into queue\")\n",
    "                upper = under + self.capacity\n",
    "                # print(\"try to enqueue \", under, \" to \", upper)\n",
    "                if upper <= max:\n",
    "                    curr_x = self.train_x[under:upper]\n",
    "                    curr_t = self.train_t[under:upper]\n",
    "                    curr_e = self.train_e[under:upper]\n",
    "                    under = upper\n",
    "                else:\n",
    "                    rest = upper - max\n",
    "                    curr_x = np.concatenate((self.train_x[under:max], self.train_x[0:rest]))\n",
    "                    curr_t = np.concatenate((self.train_t[under:max], self.train_t[0:rest]))\n",
    "                    curr_e = np.concatenate((self.train_e[under:max], self.train_e[0:rest]))\n",
    "                    under = rest\n",
    "\n",
    "                self.session.run(self.enqueue_op,\n",
    "                                 feed_dict={self.x: curr_x, self.t: curr_t, self.e: curr_e})\n",
    "        except tf.errors.CancelledError:\n",
    "            print(\"finished enqueueing\")\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_observed_death(name, observed_e, observed_t, save=False):\n",
    "        idx_observed = observed_e == 1\n",
    "        observed_death = observed_t[idx_observed]\n",
    "        if save:\n",
    "            death_observed_print = \"{} observed_death:{}, percentage:{}\".format(name, observed_death.shape, float(\n",
    "                len(observed_death) / len(observed_t)))\n",
    "            logging.debug(death_observed_print)\n",
    "            print(death_observed_print)\n",
    "        return idx_observed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATE(DATE_AE):\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 learning_rate,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 require_improvement,\n",
    "                 seed,\n",
    "                 num_iterations,\n",
    "                 hidden_dim,\n",
    "                 latent_dim,\n",
    "                 input_dim,\n",
    "                 num_examples,\n",
    "                 keep_prob,\n",
    "                 train_data,\n",
    "                 valid_data,\n",
    "                 test_data,\n",
    "                 end_t,\n",
    "                 covariates,\n",
    "                 imputation_values,\n",
    "                 sample_size,\n",
    "                 disc_updates,\n",
    "                 categorical_indices,\n",
    "                 l2_reg,\n",
    "                 gen_updates,\n",
    "                 max_epochs,\n",
    "                 path_large_data=\"\"\n",
    "                 ):\n",
    "        DATE_AE.__init__(self, batch_size=batch_size,\n",
    "                         learning_rate=learning_rate,\n",
    "                         beta1=beta1,\n",
    "                         beta2=beta2,\n",
    "                         require_improvement=require_improvement,\n",
    "                         num_iterations=num_iterations, seed=seed,\n",
    "                         l2_reg=l2_reg,\n",
    "                         hidden_dim=hidden_dim,\n",
    "                         train_data=train_data, test_data=test_data, valid_data=valid_data,\n",
    "                         input_dim=input_dim,\n",
    "                         num_examples=num_examples, keep_prob=keep_prob,\n",
    "                         latent_dim=latent_dim, end_t=end_t,\n",
    "                         path_large_data=path_large_data,\n",
    "                         covariates=covariates,\n",
    "                         categorical_indices=categorical_indices,\n",
    "                         disc_updates=disc_updates,\n",
    "                         sample_size=sample_size, imputation_values=imputation_values,\n",
    "                         max_epochs=max_epochs, gen_updates=gen_updates)\n",
    "\n",
    "        print_model = \"model is DATE\"\n",
    "        print(print_model)\n",
    "        logging.debug(print_model)\n",
    "        self.model = 'DATE'\n",
    "        self.imputation_values = imputation_values\n",
    "\n",
    "    def _objective(self):\n",
    "        self.num_batches = self.num_examples / self.batch_size\n",
    "        logging.debug(\"num batches:{}, batch_size:{} epochs:{}\".format(self.num_batches, self.batch_size,\n",
    "                                                                       int(self.num_iterations / self.num_batches)))\n",
    "        self._build_model()\n",
    "        self.reg_loss = l2_loss(self.l2_reg) + l1_loss(self.l2_reg)\n",
    "        self.layer_one_recon = tf.constant(0.0)\n",
    "        self.cost = self.t_regularization_loss + self.disc_one_loss + self.gen_one_loss\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1,\n",
    "                                           beta2=self.beta2)\n",
    "\n",
    "        dvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Discriminator_one\")\n",
    "        genvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generate_t_given_x\")\n",
    "        self.disc_solver = optimizer.minimize(self.disc_one_loss, var_list=dvars1)\n",
    "        self.gen_solver = optimizer.minimize(self.gen_one_loss + self.t_regularization_loss, var_list=genvars1)\n",
    "\n",
    "    def _build_model(self):\n",
    "        self._risk_date()\n",
    "\n",
    "    @staticmethod\n",
    "    def log(x):\n",
    "        return tf.log(x + 1e-8)\n",
    "\n",
    "    def _risk_date(self):\n",
    "        def expand_t_dim(t):\n",
    "            return tf.expand_dims(t, axis=1)\n",
    "\n",
    "        indices_lab = tf.where(tf.equal(tf.constant(1.0, dtype=tf.float32), self.e))\n",
    "        x_lab = tf.squeeze(tf.gather(self.x, indices_lab), axis=[1])\n",
    "        t_lab_exp = expand_t_dim(self.t_lab)\n",
    "\n",
    "        t_gen, prob_t_gen = pt_given_x(x=self.x, hidden_dim=self.hidden_dim, is_training=self.is_training,\n",
    "                           batch_norm=self.batch_norm, keep_prob=self.keep_prob, batch_size=self.batch_size_tensor,\n",
    "                           input_dim=self.input_dim, noise_alpha=self.noise_alpha)\n",
    "\n",
    "        # Discriminator B\n",
    "        d_one_real, f_one_real = discriminator_one(pair_one=x_lab, pair_two=t_lab_exp, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   keep_prob=self.keep_prob)  # (x_nc, t_nc)\n",
    "        d_one_fake, f_one_fake = discriminator_one(pair_one=self.x, pair_two=t_gen, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   reuse=True, keep_prob=self.keep_prob)  # (x, t_gen)\n",
    "\n",
    "        # Discriminator loss\n",
    "        self.disc_one_loss = -tf.reduce_mean(self.log(d_one_real)) - tf.reduce_mean(self.log(1 - d_one_fake))\n",
    "\n",
    "        # Generator loss\n",
    "        self.gen_one_loss = tf.reduce_mean(f_one_real) - tf.reduce_mean(f_one_fake)\n",
    "        self.disc_logit = d_one_fake # added b raiber \n",
    "        self.disc_f = f_one_fake # added by raiber\n",
    "        self.probability_t_gen = tf.squeeze(prob_t_gen) # added by raiber\n",
    "        self.predicted_time = tf.squeeze(t_gen)\n",
    "        self.ranking_partial_lik, self.total_rae, self.total_t_recon_loss = \\\n",
    "            batch_metrics(e=self.e,\n",
    "                          risk_set=self.risk_set,\n",
    "                          predicted=self.predicted_time,\n",
    "                          batch_size=self.batch_size_tensor,\n",
    "                          empirical=self.t)\n",
    "\n",
    "        self.t_regularization_loss = self.total_t_recon_loss\n",
    "        self.t_mse = tf.losses.mean_squared_error(labels=self.t_lab,\n",
    "                                                  predictions=tf.gather(self.predicted_time, indices_lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of data:   time  label  current_year  default  payoff  int.rate  orig.upb  fico.score  \\\n",
      "0    57      1          2006        0       1     6.875    180000         680   \n",
      "1     5      1          2003        0       1     6.500    300000         758   \n",
      "2     5      1          2003        0       1     6.000    269000         769   \n",
      "3     5      1          2001        0       1     8.375    160000         661   \n",
      "4     5      1          2001        0       1     8.500     84000         715   \n",
      "\n",
      "   dti.r  ltv.r  ...  ppi.o.FRMA  equity.est  hpi.st.log12m  hpi.r.st.us  \\\n",
      "0     30     90  ...      -0.145   110482.80       0.049438     1.136017   \n",
      "1     36     66  ...       0.570   172589.30       0.117066     1.048596   \n",
      "2     36     77  ...       0.210    84684.58       0.025783     0.891385   \n",
      "3     48     85  ...       0.495    36270.05       0.132158     1.003634   \n",
      "4     49     90  ...       0.620    13180.12       0.050343     0.995011   \n",
      "\n",
      "   hpi.r.zip.st  st.unemp.r12m  st.unemp.r3m  TB10Y.r12m  T10Y3MM  \\\n",
      "0      0.892428            0.5          -0.1        0.91     0.14   \n",
      "1      1.425959            0.0           0.0       -1.25     2.83   \n",
      "2      1.377818            1.2           0.4       -1.60     2.41   \n",
      "3      1.217811            0.0           0.4       -1.50     0.01   \n",
      "4      1.545843            1.1           0.5       -1.50     0.01   \n",
      "\n",
      "   T10Y3MM.r12m  \n",
      "0         -0.82  \n",
      "1         -0.67  \n",
      "2         -0.82  \n",
      "3         -1.33  \n",
      "4         -1.33  \n",
      "\n",
      "[5 rows x 30 columns], data shape:(10000, 30)\n",
      "missing:0.0\n",
      "Encoding data:(10000, 30)\n",
      "head of data:   time  label  current_year  default  payoff  int.rate  orig.upb  fico.score  \\\n",
      "0    57      1          2006        0       1     6.875    180000         680   \n",
      "1     5      1          2003        0       1     6.500    300000         758   \n",
      "2     5      1          2003        0       1     6.000    269000         769   \n",
      "3     5      1          2001        0       1     8.375    160000         661   \n",
      "4     5      1          2001        0       1     8.500     84000         715   \n",
      "\n",
      "   dti.r  ltv.r  ...  ppi.o.FRMA  equity.est  hpi.st.log12m  hpi.r.st.us  \\\n",
      "0     30     90  ...      -0.145   110482.80       0.049438     1.136017   \n",
      "1     36     66  ...       0.570   172589.30       0.117066     1.048596   \n",
      "2     36     77  ...       0.210    84684.58       0.025783     0.891385   \n",
      "3     48     85  ...       0.495    36270.05       0.132158     1.003634   \n",
      "4     49     90  ...       0.620    13180.12       0.050343     0.995011   \n",
      "\n",
      "   hpi.r.zip.st  st.unemp.r12m  st.unemp.r3m  TB10Y.r12m  T10Y3MM  \\\n",
      "0      0.892428            0.5          -0.1        0.91     0.14   \n",
      "1      1.425959            0.0           0.0       -1.25     2.83   \n",
      "2      1.377818            1.2           0.4       -1.60     2.41   \n",
      "3      1.217811            0.0           0.4       -1.50     0.01   \n",
      "4      1.545843            1.1           0.5       -1.50     0.01   \n",
      "\n",
      "   T10Y3MM.r12m  \n",
      "0         -0.82  \n",
      "1         -0.67  \n",
      "2         -0.82  \n",
      "3         -1.33  \n",
      "4         -1.33  \n",
      "\n",
      "[5 rows x 30 columns], data shape:(10000, 30)\n",
      "Encoded:['t.act.12m', 't.del.30d.12m', 't.del.60d.12m'], one_hot:(10000, 62)   time  label  current_year  default  payoff  int.rate  orig.upb  fico.score  \\\n",
      "0    57      1          2006        0       1     6.875    180000         680   \n",
      "1     5      1          2003        0       1     6.500    300000         758   \n",
      "2     5      1          2003        0       1     6.000    269000         769   \n",
      "3     5      1          2001        0       1     8.375    160000         661   \n",
      "4     5      1          2001        0       1     8.500     84000         715   \n",
      "\n",
      "   dti.r  ltv.r  ...  t.del.30d.12m_12  t.del.60d.12m_0  t.del.60d.12m_1  \\\n",
      "0     30     90  ...                 0                1                0   \n",
      "1     36     66  ...                 0                1                0   \n",
      "2     36     77  ...                 0                1                0   \n",
      "3     48     85  ...                 0                1                0   \n",
      "4     49     90  ...                 0                1                0   \n",
      "\n",
      "   t.del.60d.12m_2  t.del.60d.12m_3  t.del.60d.12m_4  t.del.60d.12m_5  \\\n",
      "0                0                0                0                0   \n",
      "1                0                0                0                0   \n",
      "2                0                0                0                0   \n",
      "3                0                0                0                0   \n",
      "4                0                0                0                0   \n",
      "\n",
      "   t.del.60d.12m_6  t.del.60d.12m_7  t.del.60d.12m_8  \n",
      "0                0                0                0  \n",
      "1                0                0                0  \n",
      "2                0                0                0  \n",
      "3                0                0                0  \n",
      "4                0                0                0  \n",
      "\n",
      "[5 rows x 62 columns]\n",
      "head of dataset data:   int.rate  orig.upb  fico.score     dti.r     ltv.r  bal.repaid  \\\n",
      "0 -0.301876  0.474470   -0.600353 -0.365171  0.923452    0.357399   \n",
      "1 -0.713280  2.241742    0.808184  0.174677 -0.621406   -0.363720   \n",
      "2 -1.261818  1.785197    1.006824  0.174677  0.086654   -0.308027   \n",
      "3  1.343737  0.179925   -0.943458  1.254371  0.601607   -0.324116   \n",
      "4  1.480872 -0.939348    0.031683  1.344345  0.923452   -0.247335   \n",
      "\n",
      "   hpi.st.d.t.o  hpi.zip.o  hpi.zip.d.t.o  ppi.c.FRMA  ...  t.del.30d.12m_12  \\\n",
      "0      3.367343  -0.855293       1.065295   -0.953416  ...                 0   \n",
      "1     -0.740429   1.093700      -0.594514   -0.284572  ...                 0   \n",
      "2     -0.756680   0.230828      -0.813118   -0.224316  ...                 0   \n",
      "3     -0.752580  -0.963833      -0.678805    0.336068  ...                 0   \n",
      "4     -0.918855   0.181822      -0.672896    0.486708  ...                 0   \n",
      "\n",
      "   t.del.60d.12m_0  t.del.60d.12m_1  t.del.60d.12m_2  t.del.60d.12m_3  \\\n",
      "0                1                0                0                0   \n",
      "1                1                0                0                0   \n",
      "2                1                0                0                0   \n",
      "3                1                0                0                0   \n",
      "4                1                0                0                0   \n",
      "\n",
      "   t.del.60d.12m_4  t.del.60d.12m_5  t.del.60d.12m_6  t.del.60d.12m_7  \\\n",
      "0                0                0                0                0   \n",
      "1                0                0                0                0   \n",
      "2                0                0                0                0   \n",
      "3                0                0                0                0   \n",
      "4                0                0                0                0   \n",
      "\n",
      "   t.del.60d.12m_8  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "\n",
      "[5 rows x 57 columns], data shape:(10000, 57)\n",
      "data description:           int.rate      orig.upb    fico.score         dti.r         ltv.r  \\\n",
      "count  1.000000e+04  1.000000e+04  1.000000e+04  1.000000e+04  1.000000e+04   \n",
      "mean   8.743228e-16  8.952838e-17  1.321609e-16  1.399769e-16 -2.373213e-16   \n",
      "std    1.000050e+00  1.000050e+00  1.000050e+00  1.000050e+00  1.000050e+00   \n",
      "min   -2.907431e+00 -1.940802e+00 -7.462458e+00 -2.974431e+00 -4.226076e+00   \n",
      "25%   -7.132798e-01 -7.626208e-01 -6.906438e-01 -6.350940e-01 -4.282992e-01   \n",
      "50%   -2.760747e-02 -1.735300e-01  1.219739e-01 -5.272506e-03  2.797610e-01   \n",
      "75%    7.951993e-01  6.511972e-01  7.901263e-01  7.145235e-01  6.016065e-01   \n",
      "max    3.675023e+00  6.365378e+00  1.945849e+00  2.783937e+00  1.567143e+00   \n",
      "\n",
      "         bal.repaid  hpi.st.d.t.o     hpi.zip.o  hpi.zip.d.t.o    ppi.c.FRMA  \\\n",
      "count  1.000000e+04  1.000000e+04  1.000000e+04   1.000000e+04  1.000000e+04   \n",
      "mean  -1.278977e-17 -8.881784e-18 -1.190159e-16  -8.313350e-17  7.176482e-17   \n",
      "std    1.000050e+00  1.000050e+00  1.000050e+00   1.000050e+00  1.000050e+00   \n",
      "min   -5.091389e-01 -1.405697e+00 -2.093515e+00  -1.200301e+00 -3.429949e+00   \n",
      "25%   -2.801701e-01 -6.339568e-01 -6.469264e-01  -5.968777e-01 -6.054365e-01   \n",
      "50%   -1.753684e-01 -3.304061e-01 -2.501581e-01  -3.380987e-01  3.478643e-02   \n",
      "75%   -3.601785e-03  2.438546e-01  3.662304e-01   1.795578e-01  6.252979e-01   \n",
      "max    1.316752e+01  6.350394e+00  7.716064e+00   7.215728e+00  4.511827e+00   \n",
      "\n",
      "       ...  t.del.30d.12m_12  t.del.60d.12m_0  t.del.60d.12m_1  \\\n",
      "count  ...        10000.0000     10000.000000     10000.000000   \n",
      "mean   ...            0.0001         0.963100         0.024100   \n",
      "std    ...            0.0100         0.188526         0.153367   \n",
      "min    ...            0.0000         0.000000         0.000000   \n",
      "25%    ...            0.0000         1.000000         0.000000   \n",
      "50%    ...            0.0000         1.000000         0.000000   \n",
      "75%    ...            0.0000         1.000000         0.000000   \n",
      "max    ...            1.0000         1.000000         1.000000   \n",
      "\n",
      "       t.del.60d.12m_2  t.del.60d.12m_3  t.del.60d.12m_4  t.del.60d.12m_5  \\\n",
      "count     10000.000000     10000.000000     10000.000000     10000.000000   \n",
      "mean          0.006700         0.003400         0.001500         0.000500   \n",
      "std           0.081583         0.058213         0.038703         0.022356   \n",
      "min           0.000000         0.000000         0.000000         0.000000   \n",
      "25%           0.000000         0.000000         0.000000         0.000000   \n",
      "50%           0.000000         0.000000         0.000000         0.000000   \n",
      "75%           0.000000         0.000000         0.000000         0.000000   \n",
      "max           1.000000         1.000000         1.000000         1.000000   \n",
      "\n",
      "       t.del.60d.12m_6  t.del.60d.12m_7  t.del.60d.12m_8  \n",
      "count     10000.000000     10000.000000       10000.0000  \n",
      "mean          0.000400         0.000200           0.0001  \n",
      "std           0.019997         0.014141           0.0100  \n",
      "min           0.000000         0.000000           0.0000  \n",
      "25%           0.000000         0.000000           0.0000  \n",
      "50%           0.000000         0.000000           0.0000  \n",
      "75%           0.000000         0.000000           0.0000  \n",
      "max           1.000000         1.000000           1.0000  \n",
      "\n",
      "[8 rows x 57 columns]\n",
      "columns:['int.rate' 'orig.upb' 'fico.score' 'dti.r' 'ltv.r' 'bal.repaid'\n",
      " 'hpi.st.d.t.o' 'hpi.zip.o' 'hpi.zip.d.t.o' 'ppi.c.FRMA' 'TB10Y.d.t.o'\n",
      " 'FRMA30Y.d.t.o' 'ppi.o.FRMA' 'equity.est' 'hpi.st.log12m' 'hpi.r.st.us'\n",
      " 'hpi.r.zip.st' 'st.unemp.r12m' 'st.unemp.r3m' 'TB10Y.r12m' 'T10Y3MM'\n",
      " 'T10Y3MM.r12m' 't.act.12m_0' 't.act.12m_1' 't.act.12m_2' 't.act.12m_3'\n",
      " 't.act.12m_4' 't.act.12m_5' 't.act.12m_6' 't.act.12m_7' 't.act.12m_8'\n",
      " 't.act.12m_9' 't.act.12m_10' 't.act.12m_11' 't.act.12m_12'\n",
      " 't.del.30d.12m_0' 't.del.30d.12m_1' 't.del.30d.12m_2' 't.del.30d.12m_3'\n",
      " 't.del.30d.12m_4' 't.del.30d.12m_5' 't.del.30d.12m_6' 't.del.30d.12m_7'\n",
      " 't.del.30d.12m_8' 't.del.30d.12m_9' 't.del.30d.12m_10' 't.del.30d.12m_11'\n",
      " 't.del.30d.12m_12' 't.del.60d.12m_0' 't.del.60d.12m_1' 't.del.60d.12m_2'\n",
      " 't.del.60d.12m_3' 't.del.60d.12m_4' 't.del.60d.12m_5' 't.del.60d.12m_6'\n",
      " 't.del.60d.12m_7' 't.del.60d.12m_8']\n",
      "x:[-0.30187639  0.47446994 -0.60035295 -0.36517052  0.92345207  0.35739886\n",
      "  3.36734339 -0.8552933   1.06529496 -0.95341645  1.42735758  0.61766385\n",
      " -0.6033995   0.16616233 -0.58683744  1.14799302 -2.59755919  0.31462759\n",
      " -0.50174834  1.86634098 -1.79831605 -0.60246714  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ], t:57, e:0, len:10000\n",
      "x_shape:(10000, 57)\n",
      "end_time:72\n",
      "observed percent:0.0318\n",
      "shuffled x:[ 1.20660266 -0.71843895  1.27769692  1.9741666   0.92345207 -0.04562641\n",
      "  0.92002962 -0.67560408  0.81793847  1.67978268 -2.11137347 -1.95068017\n",
      " -0.27733652 -0.40633445  0.71460745  0.59795992 -0.04198785 -1.07106406\n",
      " -0.50174834 -1.40626485  0.46292613 -0.59555742  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ], t:37, e:0, len:10000\n",
      "num_examples:8000\n",
      "test:2000, valid:1600, train:8000, all: 11600\n",
      "categorical_flat:[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]\n",
      "len covariates:57, categorical:35\n",
      "imputation_values:[-0.027607470355740633, -0.17352995336914429, 0.10391575026652726, -0.0052725059705027374, 0.27976100827992123, -0.17477381021602584, -0.3303739811127087, -0.24217188945866244, -0.3428252254283816, 0.0347864283255566, -0.054256461163903966, -0.0689231647934416, -0.040199810624787106, -0.2890943006333361, -0.24413001232188639, -0.05274888970827871, -0.0629754898546519, -0.06328831675981932, -0.19505620133005064, -0.209976164935182, 0.3026806190090808, -0.3053493403307765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "imputation_values:[-0.027607470355740633, -0.17352995336914429, 0.10391575026652726, -0.0052725059705027374, 0.27976100827992123, -0.17477381021602584, -0.3303739811127087, -0.24217188945866244, -0.3428252254283816, 0.0347864283255566, -0.054256461163903966, -0.0689231647934416, -0.040199810624787106, -0.2890943006333361, -0.24413001232188639, -0.05274888970827871, -0.0629754898546519, -0.06328831675981932, -0.19505620133005064, -0.209976164935182, 0.3026806190090808, -0.3053493403307765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "observed fold:0.028125\n",
      "observed fold:0.0415\n",
      "observed fold:0.034375\n",
      "imputation_values:[-0.027607470355740633, -0.17352995336914429, 0.10391575026652726, -0.0052725059705027374, 0.27976100827992123, -0.17477381021602584, -0.3303739811127087, -0.24217188945866244, -0.3428252254283816, 0.0347864283255566, -0.054256461163903966, -0.0689231647934416, -0.040199810624787106, -0.2890943006333361, -0.24413001232188639, -0.05274888970827871, -0.0629754898546519, -0.06328831675981932, -0.19505620133005064, -0.209976164935182, 0.3026806190090808, -0.3053493403307765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], one_hot_indices:[[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34], [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55, 56]]\n",
      "end_t:72\n",
      "input_dim:57, continuous:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21], size:22, categorical:[[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34], [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55, 56]], size3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is DATE_AE\n",
      "WARNING:tensorflow:From <ipython-input-14-09dbafe12efb>:81: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From <ipython-input-7-001b2cc10fce>:5: Uniform.__init__ (from tensorflow.python.ops.distributions.uniform) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\tensorflow_core\\python\\ops\\distributions\\uniform.py:131: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "layer input shape:114\n",
      "name:W_decoder_h0_z, shape[114, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "WARNING:tensorflow:From <ipython-input-8-c815479674f8>:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "layer input shape:100\n",
      "name:W_decoder_h1_z, shape[100, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "name:W_encoder_t, shape[100, 1]\n",
      "name:b_encoder_t, shape[1]\n",
      "scope:Discriminator_one, pair_one:(?, 57), pair_two:(?, 1)\n",
      "layer input shape:57\n",
      "name:W_decoder_h0_z, shape[57, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:50\n",
      "name:W_decoder_h1_z, shape[50, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:1\n",
      "name:W_decoder_h0_z, shape[1, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:50\n",
      "name:W_decoder_h1_z, shape[50, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "hidden_pairs:(?, 100)\n",
      "name:W_discriminator_logits, shape[100, 1]\n",
      "name:b_discriminator_logits, shape[1]\n",
      "scope:Discriminator_one, pair_one:(?, 57), pair_two:(?, 1)\n",
      "layer input shape:57\n",
      "name:W_decoder_h0_z, shape[57, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:50\n",
      "name:W_decoder_h1_z, shape[50, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:1\n",
      "name:W_decoder_h0_z, shape[1, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:50\n",
      "name:W_decoder_h1_z, shape[50, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "hidden_pairs:(?, 100)\n",
      "name:W_discriminator_logits, shape[100, 1]\n",
      "name:b_discriminator_logits, shape[1]\n",
      "obs_at_risk:(?,), g_theta:<unknown>\n",
      "WARNING:tensorflow:From <ipython-input-10-011d4144dfda>:28: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From <ipython-input-13-ceb9a93d0907>:118: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-13-ceb9a93d0907>:119: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "model is DATE\n",
      "Training DATE Model:\n",
      "Parameters:, l2_reg:0.001, learning_rate:0.0003, momentum: beta1=0.9 beta2=0.999, batch_size:350, batch_norm:True, hidden_dim:[50, 50], latent_dim:50, num_of_batches:18.285714285714285, keep_prob:0.8, disc_update:1\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generate_t_given_x/W_decoder_h0_z:0 (float32_ref 114x50) [5700, bytes: 22800]\n",
      "generate_t_given_x/b_decoder_h0_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/h0_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/h0_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/W_decoder_h1_z:0 (float32_ref 100x50) [5000, bytes: 20000]\n",
      "generate_t_given_x/b_decoder_h1_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/h1_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/h1_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/W_encoder_t:0 (float32_ref 100x1) [100, bytes: 400]\n",
      "generate_t_given_x/b_encoder_t:0 (float32_ref 1) [1, bytes: 4]\n",
      "Discriminator_one/W_decoder_h0_z:0 (float32_ref 57x50) [2850, bytes: 11400]\n",
      "Discriminator_one/b_decoder_h0_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h0_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h0_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/W_decoder_h1_z:0 (float32_ref 50x50) [2500, bytes: 10000]\n",
      "Discriminator_one/b_decoder_h1_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h1_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h1_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/W_decoder_h0_z_2:0 (float32_ref 1x50) [50, bytes: 200]\n",
      "Discriminator_one/b_decoder_h0_z_2:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h0_z_decoder_batch_norm_offset_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h0_z_decoder_batch_norm_scale_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/W_decoder_h1_z_2:0 (float32_ref 50x50) [2500, bytes: 10000]\n",
      "Discriminator_one/b_decoder_h1_z_2:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h1_z_decoder_batch_norm_offset_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h1_z_decoder_batch_norm_scale_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/W_discriminator_logits:0 (float32_ref 100x1) [100, bytes: 400]\n",
      "Discriminator_one/b_discriminator_logits:0 (float32_ref 1) [1, bytes: 4]\n",
      "Discriminator_one_1/W_decoder_h0_z:0 (float32_ref 57x50) [2850, bytes: 11400]\n",
      "Discriminator_one_1/b_decoder_h0_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h0_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h0_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/W_decoder_h1_z:0 (float32_ref 50x50) [2500, bytes: 10000]\n",
      "Discriminator_one_1/b_decoder_h1_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h1_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h1_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/W_decoder_h0_z_2:0 (float32_ref 1x50) [50, bytes: 200]\n",
      "Discriminator_one_1/b_decoder_h0_z_2:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h0_z_decoder_batch_norm_offset_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h0_z_decoder_batch_norm_scale_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/W_decoder_h1_z_2:0 (float32_ref 50x50) [2500, bytes: 10000]\n",
      "Discriminator_one_1/b_decoder_h1_z_2:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h1_z_decoder_batch_norm_offset_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h1_z_decoder_batch_norm_scale_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/W_discriminator_logits:0 (float32_ref 100x1) [100, bytes: 400]\n",
      "Discriminator_one_1/b_discriminator_logits:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 28303\n",
      "Total bytes of variables: 113212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:0, trainCI:0.0, train_ranking:2.252565860748291, train_RAE:0.9539338946342468,  train_Gen:-0.6556562185287476, train_Disc:inf, train_reg:3.959383487701416, train_t_reg:42.61172103881836, train_t_mse:784.3614501953125, train_layer_one_recon:0.0\n",
      "Iteration: 20 epochs:1, Training: RAE:0.9000533819198608, Loss: 30.2692813873291, Ranking:2.626041889190674, Reg:3.96140193939209, Gen:-0.2041487693786621, Disc:1.588399052619934, Recon_One:0.0, T_Reg:28.88503074645996,T_MSE:168.2265167236328,  CI:0.0 Validation RAE:1.001488782465458 Loss:inf, Ranking:5.163677617907524, Reg:4.332783371210098, Gen:-0.2064886528532952, Disc:inf, Recon_One:0.0, T_Reg:44.3428156375885, T_MSE:998.7283592224121, CI:0, *\n",
      "Iteration: 40 epochs:2, Training: RAE:0.8614776730537415, Loss: 34.49325180053711, Ranking:4.516865253448486, Reg:3.96358585357666, Gen:0.829878568649292, Disc:1.1786831617355347, Recon_One:0.0, T_Reg:32.48468780517578,T_MSE:431.66253662109375,  CI:0.0 Validation RAE:0.9871980994939804 Loss:41.38370877504349, Ranking:5.161663897335529, Reg:4.335172027349472, Gen:0.7390689980238676, Disc:1.230793409049511, Recon_One:0.0, T_Reg:39.41384768486023, T_MSE:758.3285179138184, CI:0, *\n",
      "Iteration: 60 epochs:3, Training: RAE:0.7728691101074219, Loss: 45.602455139160156, Ranking:7.208436965942383, Reg:3.966310739517212, Gen:1.6733640432357788, Disc:0.8099084496498108, Recon_One:0.0, T_Reg:43.11918258666992,T_MSE:3212.57763671875,  CI:0.0 Validation RAE:0.8867204263806343 Loss:37.0009069442749, Ranking:5.144350312650204, Reg:4.3381523713469505, Gen:1.266526009887457, Disc:1.0180595628917217, Recon_One:0.0, T_Reg:34.71632069349289, T_MSE:1027.3018236160278, CI:0, *\n",
      "Iteration: 80 epochs:4, Training: RAE:0.7177667021751404, Loss: 93.76892852783203, Ranking:1.8856722116470337, Reg:3.969486951828003, Gen:2.3398051261901855, Disc:0.6484853029251099, Recon_One:0.0, T_Reg:90.7806396484375,T_MSE:35326.109375,  CI:0.0 Validation RAE:0.8277601525187492 Loss:32.57112139463425, Ranking:5.144316986203194, Reg:4.341626353561878, Gen:2.2222529388964176, Disc:0.7293123286217451, Recon_One:0.0, T_Reg:29.61955600976944, T_MSE:558.392993927002, CI:0, *\n",
      "Iteration: 100 epochs:5, Training: RAE:0.605936586856842, Loss: 28.93801498413086, Ranking:4.460507869720459, Reg:3.9720287322998047, Gen:3.3309340476989746, Disc:0.4662456512451172, Recon_One:0.0, T_Reg:25.14083480834961,T_MSE:531.7235107421875,  CI:0.0 Validation RAE:0.7264307253062725 Loss:29.971720278263092, Ranking:5.118205793201923, Reg:4.344406425952911, Gen:2.790292449295521, Disc:0.5878908205777407, Recon_One:0.0, T_Reg:26.59353756904602, T_MSE:572.767107963562, CI:0, *\n",
      "it:100, trainCI:0.0, train_ranking:4.297766208648682, train_RAE:0.6001737713813782,  train_Gen:2.91288423538208, train_Disc:0.522513747215271, train_reg:3.9721570014953613, train_t_reg:25.632362365722656, train_t_mse:359.337158203125, train_layer_one_recon:0.0\n",
      "Iteration: 120 epochs:6, Training: RAE:0.5469367504119873, Loss: 25.17733383178711, Ranking:7.635525226593018, Reg:3.974416494369507, Gen:3.5710318088531494, Disc:0.3985620141029358, Recon_One:0.0, T_Reg:21.207740783691406,T_MSE:520.9642944335938,  CI:0.0 Validation RAE:0.5941249132156372 Loss:30.162714302539825, Ranking:5.099728390574455, Reg:4.347018040716648, Gen:3.2659788727760315, Disc:0.49220008216798306, Recon_One:0.0, T_Reg:26.404534578323364, T_MSE:1232.4501571655273, CI:0, *\n",
      "Iteration: 140 epochs:7, Training: RAE:0.4464872479438782, Loss: 23.33951187133789, Ranking:4.0594353675842285, Reg:3.9766364097595215, Gen:4.076709270477295, Disc:0.3491523265838623, Recon_One:0.0, T_Reg:18.913650512695312,T_MSE:172.7025909423828,  CI:0.0 Validation RAE:0.5392039259895682 Loss:26.777402758598328, Ranking:5.065122343599796, Reg:4.349446073174477, Gen:3.5790935158729553, Disc:0.4328221846371889, Recon_One:0.0, T_Reg:22.76548683643341, T_MSE:455.88077449798584, CI:0, *\n",
      "Iteration: 160 epochs:8, Training: RAE:0.38450464606285095, Loss: 20.551998138427734, Ranking:5.899011135101318, Reg:3.9788317680358887, Gen:4.857823848724365, Disc:0.22754868865013123, Recon_One:0.0, T_Reg:15.466625213623047,T_MSE:222.6404571533203,  CI:0.0 Validation RAE:0.4375225631520152 Loss:26.84259283542633, Ranking:5.062066845595837, Reg:4.351847246289253, Gen:3.9806414246559143, Disc:0.36635842733085155, Recon_One:0.0, T_Reg:22.49559247493744, T_MSE:648.6746864318848, CI:0, *\n",
      "Iteration: 180 epochs:9, Training: RAE:0.30064159631729126, Loss: 29.816669464111328, Ranking:3.1446855068206787, Reg:3.980536460876465, Gen:4.623839378356934, Disc:0.2485467493534088, Recon_One:0.0, T_Reg:24.944284439086914,T_MSE:613.1058349609375,  CI:0.0 Validation RAE:0.32647520396858454 Loss:24.719220638275146, Ranking:5.064868927001953, Reg:4.353711754083633, Gen:4.296228431165218, Disc:0.32204508129507303, Recon_One:0.0, T_Reg:20.100947380065918, T_MSE:475.17494106292725, CI:0, *\n",
      "Iteration: 200 epochs:10, Training: RAE:0.2291535884141922, Loss: 25.285991668701172, Ranking:4.596681118011475, Reg:3.982379198074341, Gen:5.1203484535217285, Disc:0.21923738718032837, Recon_One:0.0, T_Reg:19.9464054107666,T_MSE:631.78466796875,  CI:0.0 Validation RAE:0.3224300825968385 Loss:23.912012994289398, Ranking:5.018783092498779, Reg:4.35572724789381, Gen:4.562650844454765, Disc:0.2851455919444561, Recon_One:0.0, T_Reg:19.064216375350952, T_MSE:389.5392360687256, CI:0, *\n",
      "it:200, trainCI:0.0, train_ranking:1.951995611190796, train_RAE:0.2521294057369232,  train_Gen:4.978363990783691, train_Disc:0.20705769956111908, train_reg:3.982489585876465, train_t_reg:18.67571258544922, train_t_mse:477.06640625, train_layer_one_recon:0.0\n",
      "Iteration: 220 epochs:11, Training: RAE:0.226201131939888, Loss: 19.586681365966797, Ranking:2.848813772201538, Reg:3.9839038848876953, Gen:6.152925491333008, Disc:0.12714490294456482, Recon_One:0.0, T_Reg:13.306610107421875,T_MSE:141.25064086914062,  CI:0.0 Validation RAE:0.23534032190218568 Loss:22.52402102947235, Ranking:4.975810073316097, Reg:4.357394874095917, Gen:5.283936336636543, Disc:0.21932751825079322, Recon_One:0.0, T_Reg:17.020757764577866, T_MSE:587.1050329208374, CI:0, *\n",
      "Iteration: 240 epochs:12, Training: RAE:0.22308622300624847, Loss: 20.306121826171875, Ranking:2.8236029148101807, Reg:3.985001564025879, Gen:5.727829933166504, Disc:0.15672096610069275, Recon_One:0.0, T_Reg:14.421571731567383,T_MSE:155.02947998046875,  CI:0.0 Validation RAE:0.23496552975848317 Loss:25.394711315631866, Ranking:4.97971224039793, Reg:4.358595460653305, Gen:5.62236687541008, Disc:0.18740415899083018, Recon_One:0.0, T_Reg:19.58494007587433, T_MSE:553.7553310394287, CI:0, \n",
      "Iteration: 260 epochs:13, Training: RAE:0.16656853258609772, Loss: 20.42525291442871, Ranking:1.4746191501617432, Reg:3.9862351417541504, Gen:6.218759059906006, Disc:0.12252990156412125, Recon_One:0.0, T_Reg:14.083964347839355,T_MSE:139.65975952148438,  CI:0.0 Validation RAE:0.16897537698969245 Loss:20.770909309387207, Ranking:4.9437620267271996, Reg:4.359944686293602, Gen:5.834935680031776, Disc:0.17968330066651106, Recon_One:0.0, T_Reg:14.756290882825851, T_MSE:628.6895794868469, CI:0, *\n",
      "Iteration: 280 epochs:14, Training: RAE:0.12794840335845947, Loss: 24.05995750427246, Ranking:4.473867893218994, Reg:3.987489700317383, Gen:6.012382507324219, Disc:0.1340024620294571, Recon_One:0.0, T_Reg:17.913572311401367,T_MSE:1240.3125,  CI:0.0 Validation RAE:0.1616536455694586 Loss:20.76076117157936, Ranking:4.938032902777195, Reg:4.3613168597221375, Gen:5.8562880754470825, Disc:0.17240365268662572, Recon_One:0.0, T_Reg:14.732069700956345, T_MSE:479.1956539154053, CI:0, *\n",
      "Iteration: 300 epochs:15, Training: RAE:0.13042880594730377, Loss: 27.28240394592285, Ranking:5.013762950897217, Reg:3.9888880252838135, Gen:6.459756374359131, Disc:0.11839644610881805, Recon_One:0.0, T_Reg:20.70425033569336,T_MSE:2634.622802734375,  CI:0.0 Validation RAE:0.13182087056338787 Loss:21.517467617988586, Ranking:4.931586243212223, Reg:4.362846277654171, Gen:6.044177055358887, Disc:0.15706473169848323, Recon_One:0.0, T_Reg:15.31622588634491, T_MSE:552.4578647613525, CI:0, \n",
      "it:300, trainCI:0.0, train_ranking:2.407011032104492, train_RAE:0.11371803283691406,  train_Gen:6.789703369140625, train_Disc:0.09126465767621994, train_reg:3.9889426231384277, train_t_reg:18.592206954956055, train_t_mse:1027.9952392578125, train_layer_one_recon:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 320 epochs:16, Training: RAE:0.19575810432434082, Loss: 26.608318328857422, Ranking:3.153075933456421, Reg:3.9898812770843506, Gen:6.744994640350342, Disc:0.08517929166555405, Recon_One:0.0, T_Reg:19.77814483642578,T_MSE:690.2162475585938,  CI:0.0 Validation RAE:0.14031672524288297 Loss:19.933411419391632, Ranking:4.930122122168541, Reg:4.3639326468110085, Gen:6.178850635886192, Disc:0.14431129628792405, Recon_One:0.0, T_Reg:13.61024934053421, T_MSE:501.2757787704468, CI:0, *\n",
      "Iteration: 340 epochs:17, Training: RAE:0.13695912063121796, Loss: 16.800451278686523, Ranking:5.190652370452881, Reg:3.9910738468170166, Gen:7.007131576538086, Disc:0.09621076285839081, Recon_One:0.0, T_Reg:9.69710922241211,T_MSE:227.1564483642578,  CI:0.0 Validation RAE:0.10271923034451902 Loss:20.687265872955322, Ranking:4.91485782712698, Reg:4.365237019956112, Gen:6.544113293290138, Disc:0.13143231137655675, Recon_One:0.0, T_Reg:14.011720061302185, T_MSE:749.795298576355, CI:0, \n",
      "Iteration: 360 epochs:18, Training: RAE:0.09368434548377991, Loss: 23.858524322509766, Ranking:2.974843978881836, Reg:3.9923179149627686, Gen:7.137465953826904, Disc:0.08149749040603638, Recon_One:0.0, T_Reg:16.63956069946289,T_MSE:326.2829895019531,  CI:0.0 Validation RAE:0.09879573388025165 Loss:18.465136617422104, Ranking:4.893651984632015, Reg:4.366597719490528, Gen:7.0032480508089066, Disc:0.11092662741430104, Recon_One:0.0, T_Reg:11.35096125304699, T_MSE:544.4380350112915, CI:0, *\n",
      "Iteration: 380 epochs:19, Training: RAE:0.09787341207265854, Loss: 22.593473434448242, Ranking:2.747014045715332, Reg:3.993408679962158, Gen:6.613027572631836, Disc:0.09863337874412537, Recon_One:0.0, T_Reg:15.88181209564209,T_MSE:467.017822265625,  CI:0.0 Validation RAE:0.11714183050207794 Loss:21.263057380914688, Ranking:4.903692737221718, Reg:4.3677907437086105, Gen:6.732586815953255, Disc:0.1128238586243242, Recon_One:0.0, T_Reg:14.417646825313568, T_MSE:838.0118837356567, CI:0, \n",
      "Iteration: 400 epochs:20, Training: RAE:0.0847652330994606, Loss: 18.531265258789062, Ranking:5.632175922393799, Reg:3.994509696960449, Gen:6.709478378295898, Disc:0.09001118689775467, Recon_One:0.0, T_Reg:11.731776237487793,T_MSE:311.74078369140625,  CI:0.0 Validation RAE:0.10199393925722688 Loss:18.130506545305252, Ranking:4.887590169906616, Reg:4.368994981050491, Gen:6.761235147714615, Disc:0.11356391827575862, Recon_One:0.0, T_Reg:11.255707383155823, T_MSE:650.0339632034302, CI:0, *\n",
      "it:400, trainCI:0.0, train_ranking:4.397614479064941, train_RAE:0.09416902810335159,  train_Gen:6.952938556671143, train_Disc:0.08230416476726532, train_reg:3.9945545196533203, train_t_reg:11.378929138183594, train_t_mse:280.6392822265625, train_layer_one_recon:0.0\n",
      "Iteration: 420 epochs:21, Training: RAE:0.0930175855755806, Loss: 17.696102142333984, Ranking:2.6774051189422607, Reg:3.995758056640625, Gen:7.585716247558594, Disc:0.0632917732000351, Recon_One:0.0, T_Reg:10.04709529876709,T_MSE:307.65704345703125,  CI:0.0 Validation RAE:0.08214246772695333 Loss:21.80215099453926, Ranking:4.890831232070923, Reg:4.370360374450684, Gen:7.262650579214096, Disc:0.09112143935635686, Recon_One:0.0, T_Reg:14.448378920555115, T_MSE:586.4519939422607, CI:0, \n",
      "Iteration: 440 epochs:22, Training: RAE:0.08596832305192947, Loss: 16.37890625, Ranking:2.7726073265075684, Reg:3.9971446990966797, Gen:7.244654655456543, Disc:0.07139014452695847, Recon_One:0.0, T_Reg:9.062862396240234,T_MSE:129.9103546142578,  CI:0.0 Validation RAE:0.07957747136242688 Loss:19.584717482328415, Ranking:4.886818341910839, Reg:4.371877014636993, Gen:7.230782568454742, Disc:0.0907346778549254, Recon_One:0.0, T_Reg:12.263200089335442, T_MSE:570.1960945129395, CI:0, \n",
      "Iteration: 460 epochs:23, Training: RAE:0.051249101758003235, Loss: 21.289705276489258, Ranking:2.0784454345703125, Reg:3.998141288757324, Gen:7.199161052703857, Disc:0.06786588579416275, Recon_One:0.0, T_Reg:14.022677421569824,T_MSE:425.5239562988281,  CI:0.0 Validation RAE:0.07885845832061023 Loss:20.53049325942993, Ranking:4.87260265648365, Reg:4.372967034578323, Gen:7.144507646560669, Disc:0.09585949266329408, Recon_One:0.0, T_Reg:13.290126234292984, T_MSE:689.9183874130249, CI:0, \n",
      "Iteration: 480 epochs:24, Training: RAE:0.07384137809276581, Loss: 15.691908836364746, Ranking:1.887869119644165, Reg:3.9989418983459473, Gen:6.9552202224731445, Disc:0.07631272822618484, Recon_One:0.0, T_Reg:8.660375595092773,T_MSE:401.419921875,  CI:0.0 Validation RAE:0.07230999239254743 Loss:20.299887388944626, Ranking:4.879848159849644, Reg:4.37384270131588, Gen:7.431916385889053, Disc:0.09130977420136333, Recon_One:0.0, T_Reg:12.776661545038223, T_MSE:720.9429092407227, CI:0, \n",
      "Iteration: 500 epochs:25, Training: RAE:0.060741011053323746, Loss: 16.75383949279785, Ranking:4.708712577819824, Reg:4.000081539154053, Gen:7.952638626098633, Disc:0.04934366047382355, Recon_One:0.0, T_Reg:8.751856803894043,T_MSE:690.0216064453125,  CI:0.0 Validation RAE:0.07593508041463792 Loss:20.556580722332, Ranking:4.878506392240524, Reg:4.375089183449745, Gen:7.496515765786171, Disc:0.082786092069, Recon_One:0.0, T_Reg:12.97727882862091, T_MSE:776.0227947235107, CI:0, \n",
      "it:500, trainCI:0.0, train_ranking:5.134800434112549, train_RAE:0.07910837233066559,  train_Gen:7.487236976623535, train_Disc:0.07796254754066467, train_reg:4.000147819519043, train_t_reg:9.921597480773926, train_t_mse:488.5331115722656, train_layer_one_recon:0.0\n",
      "Iteration: 520 epochs:26, Training: RAE:0.08714171499013901, Loss: 28.024089813232422, Ranking:5.508398056030273, Reg:4.000967502593994, Gen:8.472729682922363, Disc:0.048066794872283936, Recon_One:0.0, T_Reg:19.503293991088867,T_MSE:1906.3385009765625,  CI:0.0 Validation RAE:0.07309687009546906 Loss:22.27512389421463, Ranking:4.86801278591156, Reg:4.376058205962181, Gen:8.224587440490723, Disc:0.06264074461068958, Recon_One:0.0, T_Reg:13.987896084785461, T_MSE:800.7119178771973, CI:0, \n",
      "Iteration: 540 epochs:27, Training: RAE:0.08443363010883331, Loss: 20.93939781188965, Ranking:3.3745391368865967, Reg:4.00185489654541, Gen:8.305545806884766, Disc:0.045953430235385895, Recon_One:0.0, T_Reg:12.587898254394531,T_MSE:226.26699829101562,  CI:0.0 Validation RAE:0.06424360524397343 Loss:20.230046838521957, Ranking:4.871216401457787, Reg:4.377028793096542, Gen:8.1744334846735, Disc:0.060553008457645774, Recon_One:0.0, T_Reg:11.995060503482819, T_MSE:576.2459592819214, CI:0, \n",
      "Iteration: 560 epochs:28, Training: RAE:0.06329528987407684, Loss: 20.304866790771484, Ranking:4.169102191925049, Reg:4.002712726593018, Gen:8.292546272277832, Disc:0.05211799964308739, Recon_One:0.0, T_Reg:11.960201263427734,T_MSE:590.3497924804688,  CI:0.0 Validation RAE:0.07070207525976002 Loss:19.63292306661606, Ranking:4.878217354416847, Reg:4.377967044711113, Gen:8.19948986172676, Disc:0.05930193676613271, Recon_One:0.0, T_Reg:11.3741315305233, T_MSE:523.4180946350098, CI:0, \n",
      "Iteration: 580 epochs:29, Training: RAE:0.05235360190272331, Loss: 15.850072860717773, Ranking:3.6340057849884033, Reg:4.003702640533447, Gen:8.287031173706055, Disc:0.061768654733896255, Recon_One:0.0, T_Reg:7.5012736320495605,T_MSE:417.67608642578125,  CI:0.0 Validation RAE:0.06519482366275042 Loss:20.47827160358429, Ranking:4.856092348694801, Reg:4.379049763083458, Gen:7.549282014369965, Disc:0.07662377646192908, Recon_One:0.0, T_Reg:12.852365463972092, T_MSE:635.7198171615601, CI:0, \n",
      "Iteration: 600 epochs:30, Training: RAE:0.07005951553583145, Loss: 19.79762077331543, Ranking:2.409576892852783, Reg:4.004482746124268, Gen:7.955362319946289, Disc:0.049374327063560486, Recon_One:0.0, T_Reg:11.79288387298584,T_MSE:389.6619873046875,  CI:0.0 Validation RAE:0.05474492406938225 Loss:20.30691796541214, Ranking:4.862614370882511, Reg:4.379903003573418, Gen:8.326903909444809, Disc:0.05547421472147107, Recon_One:0.0, T_Reg:11.924540281295776, T_MSE:568.6223735809326, CI:0, \n",
      "it:600, trainCI:0.0, train_ranking:4.7011542320251465, train_RAE:0.06122232601046562,  train_Gen:8.072617530822754, train_Disc:0.054665643721818924, train_reg:4.004531383514404, train_t_reg:14.477705955505371, train_t_mse:550.128662109375, train_layer_one_recon:0.0\n",
      "Iteration: 620 epochs:31, Training: RAE:0.07166357338428497, Loss: 19.34085464477539, Ranking:4.133113861083984, Reg:4.0053324699401855, Gen:8.359342575073242, Disc:0.040384724736213684, Recon_One:0.0, T_Reg:10.941128730773926,T_MSE:603.3434448242188,  CI:0.0 Validation RAE:0.06028049858286977 Loss:21.227604508399963, Ranking:4.872406452894211, Reg:4.380832388997078, Gen:7.963704153895378, Disc:0.06526927114464343, Recon_One:0.0, T_Reg:13.198630899190903, T_MSE:738.9502439498901, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 640 epochs:32, Training: RAE:0.05591227486729622, Loss: 15.755905151367188, Ranking:4.292815685272217, Reg:4.00622034072876, Gen:8.329179763793945, Disc:0.038452453911304474, Recon_One:0.0, T_Reg:7.388273239135742,T_MSE:173.0963134765625,  CI:0.0 Validation RAE:0.058516254532150924 Loss:21.846798837184906, Ranking:4.855586089193821, Reg:4.381803497672081, Gen:8.74712224304676, Disc:0.04614648886490613, Recon_One:0.0, T_Reg:13.053529739379883, T_MSE:618.8161859512329, CI:0, \n",
      "Iteration: 660 epochs:33, Training: RAE:0.05064992606639862, Loss: 22.871883392333984, Ranking:4.172789573669434, Reg:4.007120132446289, Gen:8.733094215393066, Disc:0.035622116178274155, Recon_One:0.0, T_Reg:14.103166580200195,T_MSE:462.60992431640625,  CI:0.0 Validation RAE:0.05198856140486896 Loss:22.104905545711517, Ranking:4.8668220564723015, Reg:4.382787644863129, Gen:8.97494861483574, Disc:0.042375519406050444, Recon_One:0.0, T_Reg:13.087581127882004, T_MSE:714.6137924194336, CI:0, \n",
      "Iteration: 680 epochs:34, Training: RAE:0.05847667157649994, Loss: 17.717288970947266, Ranking:3.5695571899414062, Reg:4.00788688659668, Gen:8.638456344604492, Disc:0.04285481199622154, Recon_One:0.0, T_Reg:9.035978317260742,T_MSE:354.3089599609375,  CI:0.0 Validation RAE:0.05799506022594869 Loss:22.72784036397934, Ranking:4.88089332729578, Reg:4.383626282215118, Gen:8.882123351097107, Disc:0.044229958206415176, Recon_One:0.0, T_Reg:13.801486611366272, T_MSE:679.5248785018921, CI:0, \n",
      "Iteration: 700 epochs:35, Training: RAE:0.04790572077035904, Loss: 17.756229400634766, Ranking:4.7783613204956055, Reg:4.008620262145996, Gen:9.038702964782715, Disc:0.030970187857747078, Recon_One:0.0, T_Reg:8.686554908752441,T_MSE:382.65325927734375,  CI:0.0 Validation RAE:0.0491987313143909 Loss:25.111716270446777, Ranking:4.848907709121704, Reg:4.384428411722183, Gen:9.16849359869957, Disc:0.03918522025924176, Recon_One:0.0, T_Reg:15.904038280248642, T_MSE:1028.5875873565674, CI:0, \n",
      "it:700, trainCI:0.0, train_ranking:3.468647003173828, train_RAE:0.05365881696343422,  train_Gen:8.723966598510742, train_Disc:0.033939823508262634, train_reg:4.008647441864014, train_t_reg:7.847159385681152, train_t_mse:116.2569580078125, train_layer_one_recon:0.0\n",
      "Iteration: 720 epochs:36, Training: RAE:0.061501212418079376, Loss: 16.691740036010742, Ranking:4.579885005950928, Reg:4.009186744689941, Gen:9.298155784606934, Disc:0.02840898185968399, Recon_One:0.0, T_Reg:7.365175724029541,T_MSE:252.61666870117188,  CI:0.0 Validation RAE:0.045062559947837144 Loss:22.429211407899857, Ranking:4.8528322502970695, Reg:4.385048002004623, Gen:9.473921984434128, Disc:0.035813822236377746, Recon_One:0.0, T_Reg:12.919475123286247, T_MSE:1068.4720582962036, CI:0, \n",
      "Iteration: 740 epochs:37, Training: RAE:0.04286355525255203, Loss: 21.743247985839844, Ranking:5.443024158477783, Reg:4.0097479820251465, Gen:9.274258613586426, Disc:0.03535892441868782, Recon_One:0.0, T_Reg:12.43363094329834,T_MSE:1255.03466796875,  CI:0.0 Validation RAE:0.048528110317420214 Loss:20.259227454662323, Ranking:4.842543713748455, Reg:4.385661855340004, Gen:9.162580788135529, Disc:0.04010601370828226, Recon_One:0.0, T_Reg:11.056540608406067, T_MSE:602.2026882171631, CI:0, *\n",
      "Iteration: 760 epochs:38, Training: RAE:0.055179204791784286, Loss: 19.37530517578125, Ranking:5.580765724182129, Reg:4.010491371154785, Gen:9.40005111694336, Disc:0.028855636715888977, Recon_One:0.0, T_Reg:9.94639778137207,T_MSE:556.3417358398438,  CI:0.0 Validation RAE:0.0478630987345241 Loss:20.317094057798386, Ranking:4.854951426386833, Reg:4.386474937200546, Gen:9.057878896594048, Disc:0.04288745962549001, Recon_One:0.0, T_Reg:11.216328039765358, T_MSE:609.3841009140015, CI:0, \n",
      "Iteration: 780 epochs:39, Training: RAE:0.048187512904405594, Loss: 15.609682083129883, Ranking:2.2728023529052734, Reg:4.010908603668213, Gen:9.064525604248047, Disc:0.03683413565158844, Recon_One:0.0, T_Reg:6.508321762084961,T_MSE:523.972412109375,  CI:0.0 Validation RAE:0.0497790650697425 Loss:19.526845455169678, Ranking:4.8532752469182014, Reg:4.386931285262108, Gen:9.114284247159958, Disc:0.04057438450399786, Recon_One:0.0, T_Reg:10.371986702084541, T_MSE:515.4017963409424, CI:0, *\n",
      "Iteration: 800 epochs:40, Training: RAE:0.08401413261890411, Loss: 23.809043884277344, Ranking:3.573244571685791, Reg:4.0113959312438965, Gen:9.256442070007324, Disc:0.02743673138320446, Recon_One:0.0, T_Reg:14.525165557861328,T_MSE:444.1087646484375,  CI:0.0 Validation RAE:0.058138399152085185 Loss:19.667190372943878, Ranking:4.859682582318783, Reg:4.387464299798012, Gen:9.125424772500992, Disc:0.03892081789672375, Recon_One:0.0, T_Reg:10.502844482660294, T_MSE:511.21989822387695, CI:0, \n",
      "it:800, trainCI:0.0, train_ranking:4.727064609527588, train_RAE:0.08703691512346268,  train_Gen:9.354761123657227, train_Disc:0.02783147804439068, train_reg:4.011419296264648, train_t_reg:12.38364315032959, train_t_mse:215.95326232910156, train_layer_one_recon:0.0\n",
      "Iteration: 820 epochs:41, Training: RAE:0.072548508644104, Loss: 24.14611053466797, Ranking:3.178896903991699, Reg:4.012118816375732, Gen:9.648717880249023, Disc:0.02427389845252037, Recon_One:0.0, T_Reg:14.473119735717773,T_MSE:328.0714111328125,  CI:0.0 Validation RAE:0.0536839539417997 Loss:21.205715864896774, Ranking:4.858230926096439, Reg:4.388254955410957, Gen:9.50397253036499, Disc:0.03368012164719403, Recon_One:0.0, T_Reg:11.668062970042229, T_MSE:561.1884183883667, CI:0, \n",
      "Iteration: 840 epochs:42, Training: RAE:0.03929634392261505, Loss: 19.887969970703125, Ranking:2.7373788356781006, Reg:4.012687683105469, Gen:9.474595069885254, Disc:0.028522592037916183, Recon_One:0.0, T_Reg:10.384852409362793,T_MSE:564.2959594726562,  CI:0.0 Validation RAE:0.047720137517899275 Loss:20.79365473985672, Ranking:4.840506888926029, Reg:4.3888771533966064, Gen:9.229686170816422, Disc:0.0381446240353398, Recon_One:0.0, T_Reg:11.525824025273323, T_MSE:939.4620628356934, CI:0, \n",
      "Iteration: 860 epochs:43, Training: RAE:0.03906650096178055, Loss: 17.960081100463867, Ranking:3.9865126609802246, Reg:4.013490200042725, Gen:9.289328575134277, Disc:0.02843249961733818, Recon_One:0.0, T_Reg:8.642319679260254,T_MSE:139.88665771484375,  CI:0.0 Validation RAE:0.045077365182805806 Loss:24.85179376602173, Ranking:4.847956366837025, Reg:4.38975490629673, Gen:10.158144921064377, Disc:0.026416375767439604, Recon_One:0.0, T_Reg:14.667232394218445, T_MSE:889.3259153366089, CI:0, \n",
      "Iteration: 880 epochs:44, Training: RAE:0.047161608934402466, Loss: 15.864322662353516, Ranking:3.1526472568511963, Reg:4.01400899887085, Gen:9.79240608215332, Disc:0.020918775349855423, Recon_One:0.0, T_Reg:6.050997257232666,T_MSE:281.34173583984375,  CI:0.0 Validation RAE:0.04559328133473173 Loss:19.758919775485992, Ranking:4.844691731035709, Reg:4.390322342514992, Gen:9.846650391817093, Disc:0.02990119339665398, Recon_One:0.0, T_Reg:9.882368206977844, T_MSE:553.0888338088989, CI:0, *\n",
      "Iteration: 900 epochs:45, Training: RAE:0.050908997654914856, Loss: 17.64995765686035, Ranking:5.511307239532471, Reg:4.014603614807129, Gen:10.053176879882812, Disc:0.019943375140428543, Recon_One:0.0, T_Reg:7.576837062835693,T_MSE:299.46685791015625,  CI:0.0 Validation RAE:0.04516456392593682 Loss:19.891111135482788, Ranking:4.849132910370827, Reg:4.390972703695297, Gen:9.781789928674698, Disc:0.029497498529963195, Recon_One:0.0, T_Reg:10.079823911190033, T_MSE:580.1332511901855, CI:0, \n",
      "it:900, trainCI:0.0, train_ranking:3.054600715637207, train_RAE:0.029314637184143066,  train_Gen:10.562578201293945, train_Disc:0.015173304826021194, train_reg:4.014591217041016, train_t_reg:7.339417457580566, train_t_mse:547.2882690429688, train_layer_one_recon:0.0\n",
      "Iteration: 920 epochs:46, Training: RAE:0.061002813279628754, Loss: 20.59949493408203, Ranking:3.215930700302124, Reg:4.014890670776367, Gen:9.803289413452148, Disc:0.01979837939143181, Recon_One:0.0, T_Reg:10.776408195495605,T_MSE:240.9933624267578,  CI:0.0 Validation RAE:0.04459293745458126 Loss:20.636890053749084, Ranking:4.841731049120426, Reg:4.391286671161652, Gen:10.037566810846329, Disc:0.027761687815655023, Recon_One:0.0, T_Reg:10.571561947464943, T_MSE:798.314172744751, CI:0, \n",
      "Iteration: 940 epochs:47, Training: RAE:0.03076607920229435, Loss: 22.47699737548828, Ranking:6.424116134643555, Reg:4.015345573425293, Gen:9.961220741271973, Disc:0.01960470899939537, Recon_One:0.0, T_Reg:12.496171951293945,T_MSE:588.369140625,  CI:0.0 Validation RAE:0.04511292732786387 Loss:21.51447793841362, Ranking:4.84556682407856, Reg:4.391784220933914, Gen:9.966677755117416, Disc:0.027022897789720446, Recon_One:0.0, T_Reg:11.5207779109478, T_MSE:665.625955581665, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 960 epochs:48, Training: RAE:0.043749839067459106, Loss: 23.45842742919922, Ranking:4.677998065948486, Reg:4.015815734863281, Gen:10.845311164855957, Disc:0.013539107516407967, Recon_One:0.0, T_Reg:12.599576950073242,T_MSE:380.38623046875,  CI:0.0 Validation RAE:0.037803707586135715 Loss:21.929604470729828, Ranking:4.851594425737858, Reg:4.392298460006714, Gen:9.931880354881287, Disc:0.03292889491422102, Recon_One:0.0, T_Reg:11.964795708656311, T_MSE:835.9264621734619, CI:0, \n",
      "Iteration: 980 epochs:49, Training: RAE:0.03114614263176918, Loss: 17.314855575561523, Ranking:4.786899089813232, Reg:4.0162529945373535, Gen:10.286992073059082, Disc:0.021444987505674362, Recon_One:0.0, T_Reg:7.0064191818237305,T_MSE:278.2757873535156,  CI:0.0 Validation RAE:0.040067733032628894 Loss:21.005151569843292, Ranking:4.844078294932842, Reg:4.39277671277523, Gen:10.11879113316536, Disc:0.025736797717399895, Recon_One:0.0, T_Reg:10.860623344779015, T_MSE:624.5234127044678, CI:0, \n",
      "Iteration: 1000 epochs:50, Training: RAE:0.04719100520014763, Loss: 22.51857566833496, Ranking:3.2078161239624023, Reg:4.016743183135986, Gen:10.331445693969727, Disc:0.017793208360671997, Recon_One:0.0, T_Reg:12.169336318969727,T_MSE:448.0831298828125,  CI:0.0 Validation RAE:0.04448195645818487 Loss:20.466256350278854, Ranking:4.83403816819191, Reg:4.393312856554985, Gen:10.268131166696548, Disc:0.024478043895214796, Recon_One:0.0, T_Reg:10.1736468821764, T_MSE:720.8774137496948, CI:0, \n",
      "it:1000, trainCI:0.0, train_ranking:3.417421579360962, train_RAE:0.04184511676430702,  train_Gen:9.768085479736328, train_Disc:0.03449337184429169, train_reg:4.016770839691162, train_t_reg:13.767342567443848, train_t_mse:619.2431030273438, train_layer_one_recon:0.0\n",
      "Iteration: 1020 epochs:51, Training: RAE:0.05941404029726982, Loss: 21.260820388793945, Ranking:3.329869031906128, Reg:4.0171308517456055, Gen:10.869781494140625, Disc:0.013665607199072838, Recon_One:0.0, T_Reg:10.377373695373535,T_MSE:339.7470397949219,  CI:0.0 Validation RAE:0.03709660522872582 Loss:20.815312027931213, Ranking:4.835779018700123, Reg:4.393736869096756, Gen:10.040186405181885, Disc:0.025479079224169254, Recon_One:0.0, T_Reg:10.74964627623558, T_MSE:492.8405990600586, CI:0, \n",
      "Iteration: 1040 epochs:52, Training: RAE:0.03413078188896179, Loss: 18.853551864624023, Ranking:4.527668476104736, Reg:4.017476558685303, Gen:10.127763748168945, Disc:0.01852986216545105, Recon_One:0.0, T_Reg:8.707258224487305,T_MSE:732.5875244140625,  CI:0.0 Validation RAE:0.0345644936314784 Loss:20.84871357679367, Ranking:4.8348870277404785, Reg:4.39411498606205, Gen:10.17000100016594, Disc:0.025040940672624856, Recon_One:0.0, T_Reg:10.653671950101852, T_MSE:727.285117149353, CI:0, \n",
      "Iteration: 1060 epochs:53, Training: RAE:0.04143005609512329, Loss: 20.469863891601562, Ranking:4.937380790710449, Reg:4.0180134773254395, Gen:9.690057754516602, Disc:0.02821589820086956, Recon_One:0.0, T_Reg:10.751590728759766,T_MSE:446.36273193359375,  CI:0.0 Validation RAE:0.03394058800768107 Loss:20.518554151058197, Ranking:4.832709230482578, Reg:4.394702240824699, Gen:10.174390494823456, Disc:0.024105673655867577, Recon_One:0.0, T_Reg:10.320058465003967, T_MSE:685.6265735626221, CI:0, \n",
      "Iteration: 1080 epochs:54, Training: RAE:0.039089616388082504, Loss: 24.232746124267578, Ranking:3.512242317199707, Reg:4.018532752990723, Gen:10.246439933776855, Disc:0.0177033431828022, Recon_One:0.0, T_Reg:13.968603134155273,T_MSE:592.9959106445312,  CI:0.0 Validation RAE:0.036431703658308834 Loss:21.03290233016014, Ranking:4.8377770408988, Reg:4.395270198583603, Gen:10.503532081842422, Disc:0.021093896648380905, Recon_One:0.0, T_Reg:10.508276328444481, T_MSE:669.703125, CI:0, \n",
      "Iteration: 1100 epochs:55, Training: RAE:0.04140346869826317, Loss: 21.389467239379883, Ranking:2.075427770614624, Reg:4.018944263458252, Gen:10.922189712524414, Disc:0.011210039258003235, Recon_One:0.0, T_Reg:10.456067085266113,T_MSE:107.57034301757812,  CI:0.0 Validation RAE:0.031305091571994126 Loss:22.118050038814545, Ranking:4.832870490849018, Reg:4.395720288157463, Gen:10.867003351449966, Disc:0.018501589685911313, Recon_One:0.0, T_Reg:11.232545241713524, T_MSE:578.4596548080444, CI:0, \n",
      "it:1100, trainCI:0.0, train_ranking:3.32797908782959, train_RAE:0.02867596223950386,  train_Gen:10.72398567199707, train_Disc:0.017998501658439636, train_reg:4.018967151641846, train_t_reg:8.826666831970215, train_t_mse:214.7943115234375, train_layer_one_recon:0.0\n",
      "Iteration: 1120 epochs:56, Training: RAE:0.036871537566185, Loss: 22.413597106933594, Ranking:1.8436729907989502, Reg:4.01926326751709, Gen:10.799276351928711, Disc:0.011336938478052616, Recon_One:0.0, T_Reg:11.602983474731445,T_MSE:369.94476318359375,  CI:0.0 Validation RAE:0.038833514030557126 Loss:21.327341228723526, Ranking:4.855147264897823, Reg:4.396069198846817, Gen:10.397863984107971, Disc:0.0225643870071508, Recon_One:0.0, T_Reg:10.906912788748741, T_MSE:485.50116539001465, CI:0, \n",
      "Iteration: 1140 epochs:57, Training: RAE:0.023599864915013313, Loss: 18.66029167175293, Ranking:3.1612119674682617, Reg:4.0198235511779785, Gen:10.379854202270508, Disc:0.016572419553995132, Recon_One:0.0, T_Reg:8.26386547088623,T_MSE:356.0199890136719,  CI:0.0 Validation RAE:0.03200946684228256 Loss:22.95619985461235, Ranking:4.835831642150879, Reg:4.396682009100914, Gen:10.85644382238388, Disc:0.018751822237391025, Recon_One:0.0, T_Reg:12.08100414276123, T_MSE:881.8488883972168, CI:0, \n",
      "Iteration: 1160 epochs:58, Training: RAE:0.02488592639565468, Loss: 19.676891326904297, Ranking:5.7253899574279785, Reg:4.020150661468506, Gen:10.763349533081055, Disc:0.013463178649544716, Recon_One:0.0, T_Reg:8.900079727172852,T_MSE:629.2947998046875,  CI:0.0 Validation RAE:0.03733556374209002 Loss:23.664860725402832, Ranking:4.834318704903126, Reg:4.397039785981178, Gen:10.741321861743927, Disc:0.019316774618346244, Recon_One:0.0, T_Reg:12.904222562909126, T_MSE:934.2260284423828, CI:0, \n",
      "Iteration: 1180 epochs:59, Training: RAE:0.03490062803030014, Loss: 22.91790008544922, Ranking:4.6587233543396, Reg:4.020639419555664, Gen:10.42680835723877, Disc:0.017614584416151047, Recon_One:0.0, T_Reg:12.47347640991211,T_MSE:305.5927429199219,  CI:0.0 Validation RAE:0.028160691261291504 Loss:18.783932715654373, Ranking:4.828829646110535, Reg:4.397574365139008, Gen:10.725243598222733, Disc:0.019523931376170367, Recon_One:0.0, T_Reg:8.039165191352367, T_MSE:581.6327714920044, CI:0, *\n",
      "Iteration: 1200 epochs:60, Training: RAE:0.024346381425857544, Loss: 20.06502914428711, Ranking:4.044560432434082, Reg:4.021064758300781, Gen:10.969184875488281, Disc:0.011903270147740841, Recon_One:0.0, T_Reg:9.083941459655762,T_MSE:321.9982604980469,  CI:0.0 Validation RAE:0.024056489317445084 Loss:20.912294387817383, Ranking:4.8343919813632965, Reg:4.3980395793914795, Gen:10.619029343128204, Disc:0.01981155655812472, Recon_One:0.0, T_Reg:10.273453071713448, T_MSE:568.2470178604126, CI:0, \n",
      "it:1200, trainCI:0.0, train_ranking:3.5882444381713867, train_RAE:0.03810911625623703,  train_Gen:11.620864868164062, train_Disc:0.01203697919845581, train_reg:4.021073818206787, train_t_reg:7.652927398681641, train_t_mse:281.7912902832031, train_layer_one_recon:0.0\n",
      "Iteration: 1220 epochs:61, Training: RAE:0.03231700509786606, Loss: 20.151596069335938, Ranking:2.4899935722351074, Reg:4.0214056968688965, Gen:10.906756401062012, Disc:0.01645425334572792, Recon_One:0.0, T_Reg:9.228385925292969,T_MSE:349.0184631347656,  CI:0.0 Validation RAE:0.02989858528599143 Loss:20.607335180044174, Ranking:4.830940268933773, Reg:4.3984124809503555, Gen:10.75834932923317, Disc:0.018459690210875124, Recon_One:0.0, T_Reg:9.83052633702755, T_MSE:614.2985992431641, CI:0, \n",
      "Iteration: 1240 epochs:62, Training: RAE:0.023229427635669708, Loss: 17.62480354309082, Ranking:5.237611293792725, Reg:4.022090911865234, Gen:11.510905265808105, Disc:0.013311319053173065, Recon_One:0.0, T_Reg:6.100587368011475,T_MSE:220.96331787109375,  CI:0.0 Validation RAE:0.03203533933265135 Loss:23.121676564216614, Ranking:4.836378373205662, Reg:4.3991619348526, Gen:11.05852684378624, Disc:0.01653734824503772, Recon_One:0.0, T_Reg:12.046612188220024, T_MSE:797.6084222793579, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1260 epochs:63, Training: RAE:0.03762071952223778, Loss: 21.7630558013916, Ranking:5.8096442222595215, Reg:4.022365093231201, Gen:11.401899337768555, Disc:0.016200898215174675, Recon_One:0.0, T_Reg:10.344955444335938,T_MSE:310.8014221191406,  CI:0.0 Validation RAE:0.030400778399780393 Loss:21.747379422187805, Ranking:4.8325593918561935, Reg:4.399461820721626, Gen:11.55329617857933, Disc:0.013794771977700293, Recon_One:0.0, T_Reg:10.180287972092628, T_MSE:724.8207015991211, CI:0, \n",
      "Iteration: 1280 epochs:64, Training: RAE:0.025151530280709267, Loss: 23.620376586914062, Ranking:1.0717437267303467, Reg:4.022635459899902, Gen:11.963937759399414, Disc:0.008598094806075096, Recon_One:0.0, T_Reg:11.647839546203613,T_MSE:225.45904541015625,  CI:0.0 Validation RAE:0.032877663732506335 Loss:20.45261514186859, Ranking:4.831320263445377, Reg:4.399757534265518, Gen:11.05481430888176, Disc:0.016862945252796635, Recon_One:0.0, T_Reg:9.380937963724136, T_MSE:591.2479047775269, CI:0, \n",
      "Iteration: 1300 epochs:65, Training: RAE:0.02005201019346714, Loss: 16.614341735839844, Ranking:3.4662368297576904, Reg:4.023118019104004, Gen:11.297480583190918, Disc:0.010188354179263115, Recon_One:0.0, T_Reg:5.3066725730896,T_MSE:496.82305908203125,  CI:0.0 Validation RAE:0.03017043141881004 Loss:23.925810039043427, Ranking:4.838542506098747, Reg:4.400285333395004, Gen:11.168721705675125, Disc:0.0160660965484567, Recon_One:0.0, T_Reg:12.741022273898125, T_MSE:775.0950164794922, CI:0, \n",
      "it:1300, trainCI:0.0, train_ranking:4.561653137207031, train_RAE:0.036364320665597916,  train_Gen:11.35016918182373, train_Disc:0.01055271364748478, train_reg:4.023153305053711, train_t_reg:10.885614395141602, train_t_mse:494.9893798828125, train_layer_one_recon:0.0\n",
      "Iteration: 1320 epochs:66, Training: RAE:0.01139193493872881, Loss: 14.401955604553223, Ranking:1.0977076292037964, Reg:4.023387432098389, Gen:11.853445053100586, Disc:0.008052724413573742, Recon_One:0.0, T_Reg:2.5404577255249023,T_MSE:396.166015625,  CI:0.0 Validation RAE:0.025506084435619414 Loss:22.259345829486847, Ranking:4.82775741070509, Reg:4.400580003857613, Gen:11.439201444387436, Disc:0.01418353570625186, Recon_One:0.0, T_Reg:10.80596087872982, T_MSE:776.5019111633301, CI:0, \n",
      "Iteration: 1340 epochs:67, Training: RAE:0.03895100951194763, Loss: 19.454818725585938, Ranking:2.112921714782715, Reg:4.023775577545166, Gen:11.791023254394531, Disc:0.008394322358071804, Recon_One:0.0, T_Reg:7.655402183532715,T_MSE:401.3746643066406,  CI:0.0 Validation RAE:0.038186087331268936 Loss:19.429706424474716, Ranking:4.831403397023678, Reg:4.401004537940025, Gen:11.152622997760773, Disc:0.016196303855394945, Recon_One:0.0, T_Reg:8.260886929929256, T_MSE:585.2572937011719, CI:0, \n",
      "Iteration: 1360 epochs:68, Training: RAE:0.028495576232671738, Loss: 22.7412109375, Ranking:3.997422218322754, Reg:4.024336338043213, Gen:11.901183128356934, Disc:0.008359401486814022, Recon_One:0.0, T_Reg:10.83166790008545,T_MSE:481.330322265625,  CI:0.0 Validation RAE:0.022269696171861142 Loss:21.054426312446594, Ranking:4.8234168365597725, Reg:4.401617869734764, Gen:11.660225987434387, Disc:0.017310642870143056, Recon_One:0.0, T_Reg:9.376890130341053, T_MSE:764.3336420059204, CI:0, \n",
      "Iteration: 1380 epochs:69, Training: RAE:0.03238002955913544, Loss: 18.153682708740234, Ranking:3.5294806957244873, Reg:4.024601459503174, Gen:11.675591468811035, Disc:0.010778290219604969, Recon_One:0.0, T_Reg:6.467313766479492,T_MSE:349.54644775390625,  CI:0.0 Validation RAE:0.03801650594687089 Loss:22.316358149051666, Ranking:4.8299825116992, Reg:4.401907846331596, Gen:11.234848260879517, Disc:0.015148791717365384, Recon_One:0.0, T_Reg:11.066361531615257, T_MSE:703.5628786087036, CI:0, \n",
      "Iteration: 1400 epochs:70, Training: RAE:0.036986127495765686, Loss: 22.24608612060547, Ranking:7.1403961181640625, Reg:4.025162696838379, Gen:11.556270599365234, Disc:0.010320580564439297, Recon_One:0.0, T_Reg:10.679494857788086,T_MSE:540.3432006835938,  CI:0.0 Validation RAE:0.028901981422677636 Loss:21.121719658374786, Ranking:4.827164053916931, Reg:4.402521699666977, Gen:11.421102106571198, Disc:0.014277464360930026, Recon_One:0.0, T_Reg:9.68634058535099, T_MSE:577.1669015884399, CI:0, \n",
      "it:1400, trainCI:0.0, train_ranking:3.857255220413208, train_RAE:0.021474627777934074,  train_Gen:11.670415878295898, train_Disc:0.008831026032567024, train_reg:4.0251946449279785, train_t_reg:8.849531173706055, train_t_mse:297.2215881347656, train_layer_one_recon:0.0\n",
      "Iteration: 1420 epochs:71, Training: RAE:0.022824369370937347, Loss: 24.413211822509766, Ranking:2.3674824237823486, Reg:4.025715351104736, Gen:12.366634368896484, Disc:0.00685805594548583, Recon_One:0.0, T_Reg:12.039719581604004,T_MSE:352.42205810546875,  CI:0.0 Validation RAE:0.027868271979968995 Loss:21.98539137840271, Ranking:4.830202601850033, Reg:4.403126165270805, Gen:11.362269401550293, Disc:0.01407977071357891, Recon_One:0.0, T_Reg:10.609041944146156, T_MSE:880.5004463195801, CI:0, \n",
      "Iteration: 1440 epochs:72, Training: RAE:0.030792051926255226, Loss: 21.69015884399414, Ranking:4.054455280303955, Reg:4.026021957397461, Gen:11.457416534423828, Disc:0.010772963985800743, Recon_One:0.0, T_Reg:10.221969604492188,T_MSE:824.1372680664062,  CI:0.0 Validation RAE:0.027608019532635808 Loss:21.905319452285767, Ranking:4.83562109619379, Reg:4.403461515903473, Gen:11.370380610227585, Disc:0.01455627428367734, Recon_One:0.0, T_Reg:10.520382642745972, T_MSE:660.219783782959, CI:0, \n",
      "Iteration: 1460 epochs:73, Training: RAE:0.029904065653681755, Loss: 24.340652465820312, Ranking:2.2466578483581543, Reg:4.026137351989746, Gen:12.8931884765625, Disc:0.00562441349029541, Recon_One:0.0, T_Reg:11.441840171813965,T_MSE:259.052978515625,  CI:0.0 Validation RAE:0.030944636499043554 Loss:20.37099799513817, Ranking:4.8344758450984955, Reg:4.403587728738785, Gen:11.4499731361866, Disc:0.013966560742119327, Recon_One:0.0, T_Reg:8.907057896256447, T_MSE:541.4484272003174, CI:0, \n",
      "Iteration: 1480 epochs:74, Training: RAE:0.02281312458217144, Loss: 19.128406524658203, Ranking:1.491153597831726, Reg:4.026594638824463, Gen:12.231500625610352, Disc:0.007012543734163046, Recon_One:0.0, T_Reg:6.889894485473633,T_MSE:320.5589904785156,  CI:0.0 Validation RAE:0.030915293318685144 Loss:23.182061791419983, Ranking:4.834568105638027, Reg:4.404087886214256, Gen:12.212781608104706, Disc:0.009993501822464168, Recon_One:0.0, T_Reg:10.959286525845528, T_MSE:562.3841199874878, CI:0, \n",
      "Iteration: 1500 epochs:75, Training: RAE:0.016715094447135925, Loss: 18.10938262939453, Ranking:1.8555582761764526, Reg:4.0272674560546875, Gen:11.908320426940918, Disc:0.008159616962075233, Recon_One:0.0, T_Reg:6.192902565002441,T_MSE:365.51373291015625,  CI:0.0 Validation RAE:0.0253402084344998 Loss:24.41818404197693, Ranking:4.824660763144493, Reg:4.4048237800598145, Gen:11.893860131502151, Disc:0.012110240728361532, Recon_One:0.0, T_Reg:12.51221352815628, T_MSE:1003.4687747955322, CI:0, \n",
      "it:1500, trainCI:0.0, train_ranking:2.10079026222229, train_RAE:0.022540997713804245,  train_Gen:11.956160545349121, train_Disc:0.00998767651617527, train_reg:4.0272932052612305, train_t_reg:8.152195930480957, train_t_mse:231.2942352294922, train_layer_one_recon:0.0\n",
      "Iteration: 1520 epochs:76, Training: RAE:0.027095893397927284, Loss: 19.155548095703125, Ranking:5.397170543670654, Reg:4.02734375, Gen:12.091144561767578, Disc:0.009531006217002869, Recon_One:0.0, T_Reg:7.054873466491699,T_MSE:422.9152526855469,  CI:0.0 Validation RAE:0.027336263854522258 Loss:19.823570787906647, Ranking:4.828235402703285, Reg:4.4049072265625, Gen:11.81505200266838, Disc:0.012016127089736983, Recon_One:0.0, T_Reg:7.996503010392189, T_MSE:538.6359329223633, CI:0, *\n",
      "Iteration: 1540 epochs:77, Training: RAE:0.023182792589068413, Loss: 23.765155792236328, Ranking:2.6956539154052734, Reg:4.0276713371276855, Gen:12.037454605102539, Disc:0.008888568729162216, Recon_One:0.0, T_Reg:11.7188138961792,T_MSE:308.8822326660156,  CI:0.0 Validation RAE:0.0278934316302184 Loss:24.205418199300766, Ranking:4.829090155661106, Reg:4.405265524983406, Gen:12.261350184679031, Disc:0.01479853194905445, Recon_One:0.0, T_Reg:11.929270133376122, T_MSE:1187.8096227645874, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1560 epochs:78, Training: RAE:0.030947135761380196, Loss: 26.50009536743164, Ranking:2.957991600036621, Reg:4.028023719787598, Gen:11.792787551879883, Disc:0.010333092883229256, Recon_One:0.0, T_Reg:14.69697380065918,T_MSE:542.2256469726562,  CI:0.0 Validation RAE:0.026899793825577945 Loss:23.819842994213104, Ranking:4.832130998373032, Reg:4.405650943517685, Gen:11.651091307401657, Disc:0.012707862333627418, Recon_One:0.0, T_Reg:12.156044140458107, T_MSE:1515.370244026184, CI:0, \n",
      "Iteration: 1580 epochs:79, Training: RAE:0.018712051212787628, Loss: 17.827993392944336, Ranking:4.0583930015563965, Reg:4.028279781341553, Gen:12.394935607910156, Disc:0.006864847149699926, Recon_One:0.0, T_Reg:5.4261932373046875,T_MSE:741.28173828125,  CI:0.0 Validation RAE:0.023667317524086684 Loss:22.029042661190033, Ranking:4.83564493060112, Reg:4.405931010842323, Gen:11.764312148094177, Disc:0.012082443630788475, Recon_One:0.0, T_Reg:10.252648085355759, T_MSE:805.859619140625, CI:0, \n",
      "Iteration: 1600 epochs:80, Training: RAE:0.04043368622660637, Loss: 21.37342071533203, Ranking:6.59783935546875, Reg:4.028622150421143, Gen:12.321054458618164, Disc:0.00839649885892868, Recon_One:0.0, T_Reg:9.043970108032227,T_MSE:553.7769165039062,  CI:0.0 Validation RAE:0.02246059593744576 Loss:21.431310147047043, Ranking:4.82594745606184, Reg:4.406305477023125, Gen:12.26539170742035, Disc:0.009859766985755414, Recon_One:0.0, T_Reg:9.156058609485626, T_MSE:774.3070545196533, CI:0, \n",
      "it:1600, trainCI:0.0, train_ranking:3.805490732192993, train_RAE:0.03255646675825119,  train_Gen:12.382392883300781, train_Disc:0.006583389826118946, train_reg:4.028624534606934, train_t_reg:9.431783676147461, train_t_mse:218.87022399902344, train_layer_one_recon:0.0\n",
      "Iteration: 1620 epochs:81, Training: RAE:0.023234132677316666, Loss: 20.692567825317383, Ranking:5.011534214019775, Reg:4.029009819030762, Gen:12.248235702514648, Disc:0.007293766364455223, Recon_One:0.0, T_Reg:8.43703842163086,T_MSE:242.97457885742188,  CI:0.0 Validation RAE:0.022263763239607215 Loss:22.595428496599197, Ranking:4.820509143173695, Reg:4.406729489564896, Gen:11.503693699836731, Disc:0.013563623535446823, Recon_One:0.0, T_Reg:11.078171245753765, T_MSE:1591.4442749023438, CI:0, \n",
      "Iteration: 1640 epochs:82, Training: RAE:0.05047712102532387, Loss: 25.129175186157227, Ranking:2.647963762283325, Reg:4.029116630554199, Gen:12.814727783203125, Disc:0.006132928654551506, Recon_One:0.0, T_Reg:12.308314323425293,T_MSE:415.08587646484375,  CI:0.0 Validation RAE:0.030345651146490127 Loss:22.753880262374878, Ranking:4.83104807138443, Reg:4.406846314668655, Gen:11.802587807178497, Disc:0.011684961413266137, Recon_One:0.0, T_Reg:10.939607232809067, T_MSE:712.3116436004639, CI:0, \n",
      "Iteration: 1660 epochs:83, Training: RAE:0.01679551601409912, Loss: 21.1129207611084, Ranking:1.938963770866394, Reg:4.029417037963867, Gen:12.250657081604004, Disc:0.008636531420052052, Recon_One:0.0, T_Reg:8.85362720489502,T_MSE:943.0529174804688,  CI:0.0 Validation RAE:0.02326925835222937 Loss:20.43115520477295, Ranking:4.824170671403408, Reg:4.40717488527298, Gen:11.903741031885147, Disc:0.011320877354592085, Recon_One:0.0, T_Reg:8.516093507409096, T_MSE:483.0989074707031, CI:0, \n",
      "Iteration: 1680 epochs:84, Training: RAE:0.02159782312810421, Loss: 18.687877655029297, Ranking:4.3449554443359375, Reg:4.029436111450195, Gen:12.675200462341309, Disc:0.0056838421151041985, Recon_One:0.0, T_Reg:6.006992816925049,T_MSE:520.4769897460938,  CI:0.0 Validation RAE:0.03292960225371644 Loss:22.939976811408997, Ranking:4.830264873802662, Reg:4.407195746898651, Gen:12.245335549116135, Disc:0.009563987841829658, Recon_One:0.0, T_Reg:10.685077354311943, T_MSE:607.249192237854, CI:0, \n",
      "Iteration: 1700 epochs:85, Training: RAE:0.044734422117471695, Loss: 20.421489715576172, Ranking:3.8192951679229736, Reg:4.029725074768066, Gen:12.24685001373291, Disc:0.010001346468925476, Recon_One:0.0, T_Reg:8.16463851928711,T_MSE:299.90472412109375,  CI:0.0 Validation RAE:0.02615594322560355 Loss:22.48453164100647, Ranking:4.829924516379833, Reg:4.407511800527573, Gen:12.084925502538681, Disc:0.010271127888699993, Recon_One:0.0, T_Reg:10.38933502137661, T_MSE:549.9564256668091, CI:0, \n",
      "it:1700, trainCI:0.0, train_ranking:3.8023927211761475, train_RAE:0.022618884220719337,  train_Gen:12.672281265258789, train_Disc:0.006856818683445454, train_reg:4.029720783233643, train_t_reg:8.144979476928711, train_t_mse:441.19671630859375, train_layer_one_recon:0.0\n",
      "Iteration: 1720 epochs:86, Training: RAE:0.02872166782617569, Loss: 22.84021759033203, Ranking:2.5035362243652344, Reg:4.030099868774414, Gen:13.204097747802734, Disc:0.004782767500728369, Recon_One:0.0, T_Reg:9.631336212158203,T_MSE:630.3165283203125,  CI:0.0 Validation RAE:0.0297833121730946 Loss:23.813366293907166, Ranking:4.830629900097847, Reg:4.407921731472015, Gen:12.264458984136581, Disc:0.00961715065932367, Recon_One:0.0, T_Reg:11.539290830492973, T_MSE:1119.3216934204102, CI:0, \n",
      "Iteration: 1740 epochs:87, Training: RAE:0.028416968882083893, Loss: 23.84857749938965, Ranking:2.201502799987793, Reg:4.030176162719727, Gen:12.798407554626465, Disc:0.006753569468855858, Recon_One:0.0, T_Reg:11.043416023254395,T_MSE:195.09921264648438,  CI:0.0 Validation RAE:0.030768487777095288 Loss:24.23718398809433, Ranking:4.835373729467392, Reg:4.408005177974701, Gen:12.480792164802551, Disc:0.00915940961567685, Recon_One:0.0, T_Reg:11.7472323179245, T_MSE:961.1815052032471, CI:0, \n",
      "Iteration: 1760 epochs:88, Training: RAE:0.02162250690162182, Loss: 24.109500885009766, Ranking:2.2344253063201904, Reg:4.030463695526123, Gen:12.917327880859375, Disc:0.00594302685931325, Recon_One:0.0, T_Reg:11.186230659484863,T_MSE:336.2679748535156,  CI:0.0 Validation RAE:0.025774180161533877 Loss:21.106680303812027, Ranking:4.823857016861439, Reg:4.408319666981697, Gen:12.343026161193848, Disc:0.00965972722042352, Recon_One:0.0, T_Reg:8.753994271159172, T_MSE:647.3972806930542, CI:0, \n",
      "Iteration: 1780 epochs:89, Training: RAE:0.01950831152498722, Loss: 17.405975341796875, Ranking:5.525977611541748, Reg:4.030659198760986, Gen:13.097639083862305, Disc:0.004373473115265369, Recon_One:0.0, T_Reg:4.303961753845215,T_MSE:144.1291046142578,  CI:0.0 Validation RAE:0.026553098461590707 Loss:20.368464767932892, Ranking:4.823238104581833, Reg:4.408533498644829, Gen:12.32028552889824, Disc:0.009773820551345125, Recon_One:0.0, T_Reg:8.038405254483223, T_MSE:513.4795160293579, CI:0, \n",
      "Iteration: 1800 epochs:90, Training: RAE:0.02332714945077896, Loss: 20.80050277709961, Ranking:2.9889392852783203, Reg:4.030839920043945, Gen:12.448426246643066, Disc:0.00756079750135541, Recon_One:0.0, T_Reg:8.344515800476074,T_MSE:372.9791259765625,  CI:0.0 Validation RAE:0.024385687400354072 Loss:21.99076157808304, Ranking:4.826734408736229, Reg:4.408731162548065, Gen:12.271843165159225, Disc:0.009798778308322653, Recon_One:0.0, T_Reg:9.709119498729706, T_MSE:548.7185001373291, CI:0, \n",
      "it:1800, trainCI:0.0, train_ranking:3.150214433670044, train_RAE:0.0171053409576416,  train_Gen:13.645350456237793, train_Disc:0.004975190851837397, train_reg:4.030851364135742, train_t_reg:7.121356010437012, train_t_mse:504.85968017578125, train_layer_one_recon:0.0\n",
      "Iteration: 1820 epochs:91, Training: RAE:0.01782839186489582, Loss: 17.845657348632812, Ranking:3.3886189460754395, Reg:4.031002044677734, Gen:12.593631744384766, Disc:0.006038703490048647, Recon_One:0.0, T_Reg:5.245986461639404,T_MSE:260.1116638183594,  CI:0.0 Validation RAE:0.021802196715725586 Loss:25.972568958997726, Ranking:4.819534748792648, Reg:4.408908486366272, Gen:12.664847403764725, Disc:0.008170760775101371, Recon_One:0.0, T_Reg:13.299550160765648, T_MSE:1560.4172763824463, CI:0, \n",
      "Iteration: 1840 epochs:92, Training: RAE:0.027073532342910767, Loss: 20.641685485839844, Ranking:4.2477126121521, Reg:4.031131267547607, Gen:13.087697982788086, Disc:0.006026626564562321, Recon_One:0.0, T_Reg:7.54796028137207,T_MSE:240.9313201904297,  CI:0.0 Validation RAE:0.02566186455078423 Loss:22.8327054977417, Ranking:4.822278991341591, Reg:4.409049823880196, Gen:12.564124554395676, Disc:0.008354124787729234, Recon_One:0.0, T_Reg:10.260226488113403, T_MSE:705.8816366195679, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1860 epochs:93, Training: RAE:0.019781751558184624, Loss: 17.423809051513672, Ranking:6.527237892150879, Reg:4.0317206382751465, Gen:12.571147918701172, Disc:0.007044174708425999, Recon_One:0.0, T_Reg:4.845617771148682,T_MSE:193.41110229492188,  CI:0.0 Validation RAE:0.01816543799941428 Loss:23.246837973594666, Ranking:4.8172565549612045, Reg:4.4096944481134415, Gen:12.346733063459396, Disc:0.009241579769877717, Recon_One:0.0, T_Reg:10.890863001346588, T_MSE:1510.4921216964722, CI:0, \n",
      "Iteration: 1880 epochs:94, Training: RAE:0.03528738394379616, Loss: 22.743892669677734, Ranking:4.274746894836426, Reg:4.032073497772217, Gen:13.026618957519531, Disc:0.005909848026931286, Recon_One:0.0, T_Reg:9.711363792419434,T_MSE:615.4679565429688,  CI:0.0 Validation RAE:0.024736892868531868 Loss:26.349007189273834, Ranking:4.834755077958107, Reg:4.410080388188362, Gen:12.36224827170372, Disc:0.009204739442793652, Recon_One:0.0, T_Reg:13.97755341231823, T_MSE:1505.7955522537231, CI:0, \n",
      "Iteration: 1900 epochs:95, Training: RAE:0.008688613772392273, Loss: 20.2473201751709, Ranking:1.426044225692749, Reg:4.032634258270264, Gen:13.40652847290039, Disc:0.006071954499930143, Recon_One:0.0, T_Reg:6.834720134735107,T_MSE:893.9571533203125,  CI:0.0 Validation RAE:0.020497833960689604 Loss:20.51968652009964, Ranking:4.820355653762817, Reg:4.410693719983101, Gen:12.581049591302872, Disc:0.008311939993291162, Recon_One:0.0, T_Reg:7.930324979126453, T_MSE:627.0231828689575, CI:0, *\n",
      "it:1900, trainCI:0.0, train_ranking:4.968285083770752, train_RAE:0.0229839738458395,  train_Gen:13.02334976196289, train_Disc:0.006043093279004097, train_reg:4.032652854919434, train_t_reg:7.448244094848633, train_t_mse:385.53997802734375, train_layer_one_recon:0.0\n",
      "Iteration: 1920 epochs:96, Training: RAE:0.027049247175455093, Loss: 20.801448822021484, Ranking:2.334984540939331, Reg:4.032723426818848, Gen:12.7838773727417, Disc:0.0048879701644182205, Recon_One:0.0, T_Reg:8.01268482208252,T_MSE:466.4134216308594,  CI:0.0 Validation RAE:0.027774764224886894 Loss:24.613220006227493, Ranking:4.828268937766552, Reg:4.410791248083115, Gen:12.632592409849167, Disc:0.00819885908276774, Recon_One:0.0, T_Reg:11.9724290817976, T_MSE:1410.6654977798462, CI:0, \n",
      "Iteration: 1940 epochs:97, Training: RAE:0.014273867011070251, Loss: 17.45003890991211, Ranking:3.558410882949829, Reg:4.032946586608887, Gen:13.390083312988281, Disc:0.004214768297970295, Recon_One:0.0, T_Reg:4.0557403564453125,T_MSE:199.08541870117188,  CI:0.0 Validation RAE:0.026809304079506546 Loss:23.09686353802681, Ranking:4.820581845939159, Reg:4.41103532910347, Gen:12.533380150794983, Disc:0.008690996139193885, Recon_One:0.0, T_Reg:10.55479222536087, T_MSE:809.6619091033936, CI:0, \n",
      "Iteration: 1960 epochs:98, Training: RAE:0.024731937795877457, Loss: 19.246231079101562, Ranking:6.725224018096924, Reg:4.03333044052124, Gen:13.30842399597168, Disc:0.004381171427667141, Recon_One:0.0, T_Reg:5.933426380157471,T_MSE:252.1587677001953,  CI:0.0 Validation RAE:0.024113314429996535 Loss:22.503572046756744, Ranking:4.821807257831097, Reg:4.4114551693201065, Gen:13.077103167772293, Disc:0.006664938991889358, Recon_One:0.0, T_Reg:9.419804006814957, T_MSE:673.9980230331421, CI:0, \n",
      "Iteration: 1980 epochs:99, Training: RAE:0.02893262356519699, Loss: 19.379024505615234, Ranking:6.782736301422119, Reg:4.033326625823975, Gen:13.226150512695312, Disc:0.004645140841603279, Recon_One:0.0, T_Reg:6.148227691650391,T_MSE:647.0560913085938,  CI:0.0 Validation RAE:0.02334493084345013 Loss:22.265900552272797, Ranking:4.826017759740353, Reg:4.411450996994972, Gen:13.000605463981628, Disc:0.007002007041592151, Recon_One:0.0, T_Reg:9.258293092250824, T_MSE:643.1894540786743, CI:0, \n",
      "Iteration: 2000 epochs:100, Training: RAE:0.03654823824763298, Loss: 25.814573287963867, Ranking:5.298811912536621, Reg:4.033481597900391, Gen:13.222503662109375, Disc:0.004039604216814041, Recon_One:0.0, T_Reg:12.588029861450195,T_MSE:720.1007080078125,  CI:0.0 Validation RAE:0.018950326717458665 Loss:21.504259705543518, Ranking:4.825996063649654, Reg:4.411620497703552, Gen:13.029230535030365, Disc:0.006886867980938405, Recon_One:0.0, T_Reg:8.468141809105873, T_MSE:824.3322649002075, CI:0, \n",
      "it:2000, trainCI:0.0, train_ranking:2.5480618476867676, train_RAE:0.02609335444867611,  train_Gen:12.806741714477539, train_Disc:0.004994579590857029, train_reg:4.0334978103637695, train_t_reg:9.401975631713867, train_t_mse:236.724609375, train_layer_one_recon:0.0\n",
      "Iteration: 2020 epochs:101, Training: RAE:0.03199755400419235, Loss: 23.26077651977539, Ranking:3.9740190505981445, Reg:4.033559799194336, Gen:13.235574722290039, Disc:0.00452680978924036, Recon_One:0.0, T_Reg:10.020673751831055,T_MSE:352.0957946777344,  CI:0.0 Validation RAE:0.026411750353872776 Loss:21.825497448444366, Ranking:4.827591456472874, Reg:4.411706030368805, Gen:12.89699912071228, Disc:0.007467606439604424, Recon_One:0.0, T_Reg:8.921031430363655, T_MSE:647.2188920974731, CI:0, \n",
      "Iteration: 2040 epochs:102, Training: RAE:0.02617139182984829, Loss: 21.315872192382812, Ranking:3.38761830329895, Reg:4.033966064453125, Gen:13.509251594543457, Disc:0.004854620900005102, Recon_One:0.0, T_Reg:7.801766395568848,T_MSE:681.5762329101562,  CI:0.0 Validation RAE:0.02074992674170062 Loss:22.715656340122223, Ranking:4.821962781250477, Reg:4.4121503829956055, Gen:12.674617320299149, Disc:0.007979395391885191, Recon_One:0.0, T_Reg:10.033059656620026, T_MSE:771.7574634552002, CI:0, \n",
      "Iteration: 2060 epochs:103, Training: RAE:0.024562574923038483, Loss: 20.025381088256836, Ranking:5.626918315887451, Reg:4.0340986251831055, Gen:13.220613479614258, Disc:0.004280604422092438, Recon_One:0.0, T_Reg:6.800487518310547,T_MSE:285.0186462402344,  CI:0.0 Validation RAE:0.0192522109427955 Loss:21.99142497777939, Ranking:4.825934782624245, Reg:4.412295371294022, Gen:12.964349210262299, Disc:0.007191631477326155, Recon_One:0.0, T_Reg:9.019883304834366, T_MSE:634.0142741203308, CI:0, \n",
      "Iteration: 2080 epochs:104, Training: RAE:0.030977662652730942, Loss: 23.627859115600586, Ranking:3.8508875370025635, Reg:4.03420877456665, Gen:13.511491775512695, Disc:0.003965957090258598, Recon_One:0.0, T_Reg:10.112401008605957,T_MSE:346.5968017578125,  CI:0.0 Validation RAE:0.019968390552094206 Loss:21.21341609954834, Ranking:4.830245263874531, Reg:4.412415847182274, Gen:12.77207699418068, Disc:0.007480710657546297, Recon_One:0.0, T_Reg:8.433858647942543, T_MSE:564.0648994445801, CI:0, \n",
      "Iteration: 2100 epochs:105, Training: RAE:0.01750587671995163, Loss: 22.785911560058594, Ranking:2.1434178352355957, Reg:4.034456729888916, Gen:13.674382209777832, Disc:0.00332839647307992, Recon_One:0.0, T_Reg:9.108200073242188,T_MSE:224.32557678222656,  CI:0.0 Validation RAE:0.021650735405273736 Loss:22.981994211673737, Ranking:4.820141144096851, Reg:4.412687048316002, Gen:13.363291501998901, Disc:0.0061900411965325475, Recon_One:0.0, T_Reg:9.612513065338135, T_MSE:679.2750263214111, CI:0, \n",
      "it:2100, trainCI:0.0, train_ranking:5.890676975250244, train_RAE:0.026737142354249954,  train_Gen:12.851900100708008, train_Disc:0.00500510074198246, train_reg:4.034466743469238, train_t_reg:7.331773281097412, train_t_mse:264.23321533203125, train_layer_one_recon:0.0\n",
      "Iteration: 2120 epochs:106, Training: RAE:0.023263264447450638, Loss: 22.62785530090332, Ranking:2.6395421028137207, Reg:4.034790515899658, Gen:13.845505714416504, Disc:0.004253741819411516, Recon_One:0.0, T_Reg:8.778096199035645,T_MSE:479.7655029296875,  CI:0.0 Validation RAE:0.02225553328753449 Loss:23.248952507972717, Ranking:4.819823682308197, Reg:4.413052126765251, Gen:13.395837098360062, Disc:0.0058112502010772005, Recon_One:0.0, T_Reg:9.847304664552212, T_MSE:968.7413244247437, CI:0, \n",
      "Iteration: 2140 epochs:107, Training: RAE:0.01539796032011509, Loss: 18.540424346923828, Ranking:3.2043302059173584, Reg:4.034986972808838, Gen:12.734333038330078, Disc:0.006014849990606308, Recon_One:0.0, T_Reg:5.800076484680176,T_MSE:189.13058471679688,  CI:0.0 Validation RAE:0.02084522321820259 Loss:24.551903307437897, Ranking:4.8230486288666725, Reg:4.413267001509666, Gen:13.273776352405548, Disc:0.006561498943483457, Recon_One:0.0, T_Reg:11.271566078066826, T_MSE:1124.1447744369507, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2160 epochs:108, Training: RAE:0.02000981755554676, Loss: 19.465253829956055, Ranking:3.1852591037750244, Reg:4.035140037536621, Gen:13.431283950805664, Disc:0.0033973942045122385, Recon_One:0.0, T_Reg:6.030572414398193,T_MSE:314.75433349609375,  CI:0.0 Validation RAE:0.018215207033790648 Loss:23.403710305690765, Ranking:4.820209674537182, Reg:4.413434416055679, Gen:13.327501088380814, Disc:0.006008876516716555, Recon_One:0.0, T_Reg:10.070200443267822, T_MSE:787.7659339904785, CI:0, \n",
      "Iteration: 2180 epochs:109, Training: RAE:0.021380605176091194, Loss: 26.507793426513672, Ranking:1.6034259796142578, Reg:4.035414695739746, Gen:13.49603271484375, Disc:0.004017166793346405, Recon_One:0.0, T_Reg:13.007744789123535,T_MSE:464.1894836425781,  CI:0.0 Validation RAE:0.027615819009952247 Loss:22.90419912338257, Ranking:4.819558583199978, Reg:4.413734823465347, Gen:12.899964392185211, Disc:0.00744152971310541, Recon_One:0.0, T_Reg:9.996793076395988, T_MSE:574.2985715866089, CI:0, \n",
      "Iteration: 2200 epochs:110, Training: RAE:0.009437458589673042, Loss: 18.79505157470703, Ranking:3.666656494140625, Reg:4.0358357429504395, Gen:13.231109619140625, Disc:0.004074644297361374, Recon_One:0.0, T_Reg:5.559866428375244,T_MSE:359.7977294921875,  CI:0.0 Validation RAE:0.021430429711472243 Loss:25.860379934310913, Ranking:4.820450261235237, Reg:4.414195343852043, Gen:13.30550041794777, Disc:0.006190728468936868, Recon_One:0.0, T_Reg:12.54868882894516, T_MSE:1572.1521844863892, CI:0, \n",
      "it:2200, trainCI:0.0, train_ranking:2.610882043838501, train_RAE:0.01865166798233986,  train_Gen:12.983390808105469, train_Disc:0.0044290111400187016, train_reg:4.035839080810547, train_t_reg:8.426126480102539, train_t_mse:410.15045166015625, train_layer_one_recon:0.0\n",
      "Iteration: 2220 epochs:111, Training: RAE:0.01673041842877865, Loss: 19.093233108520508, Ranking:3.1420042514801025, Reg:4.035921573638916, Gen:14.101590156555176, Disc:0.0030557895079255104, Recon_One:0.0, T_Reg:4.988586902618408,T_MSE:281.34356689453125,  CI:0.0 Validation RAE:0.022906217578565702 Loss:24.250173687934875, Ranking:4.81979525834322, Reg:4.414289221167564, Gen:13.563623607158661, Disc:0.005504794651642442, Recon_One:0.0, T_Reg:10.681045949459076, T_MSE:845.8670434951782, CI:0, \n",
      "Iteration: 2240 epochs:112, Training: RAE:0.02558005228638649, Loss: 23.23377799987793, Ranking:3.5548157691955566, Reg:4.036233901977539, Gen:13.604740142822266, Disc:0.004024480003863573, Recon_One:0.0, T_Reg:9.62501335144043,T_MSE:471.4504089355469,  CI:0.0 Validation RAE:0.021050993615062907 Loss:24.029640436172485, Ranking:4.816225625574589, Reg:4.414630830287933, Gen:13.387994170188904, Disc:0.006183611374581233, Recon_One:0.0, T_Reg:10.635462671518326, T_MSE:1074.753363609314, CI:0, \n",
      "Iteration: 2260 epochs:113, Training: RAE:0.011459668166935444, Loss: 20.49011993408203, Ranking:4.731881618499756, Reg:4.036211013793945, Gen:14.008481979370117, Disc:0.004293303936719894, Recon_One:0.0, T_Reg:6.477344036102295,T_MSE:520.49609375,  CI:0.0 Validation RAE:0.0216093955677934 Loss:23.061900079250336, Ranking:4.818795047700405, Reg:4.414605796337128, Gen:13.175904035568237, Disc:0.009067192368092947, Recon_One:0.0, T_Reg:9.876929268240929, T_MSE:768.95130443573, CI:0, \n",
      "Iteration: 2280 epochs:114, Training: RAE:0.021315857768058777, Loss: 20.570240020751953, Ranking:3.69093656539917, Reg:4.036360740661621, Gen:13.722749710083008, Disc:0.0036129779182374477, Recon_One:0.0, T_Reg:6.843876838684082,T_MSE:514.0770874023438,  CI:0.0 Validation RAE:0.02019607461988926 Loss:23.873378098011017, Ranking:4.825290314853191, Reg:4.414769560098648, Gen:13.255264788866043, Disc:0.00626007177925203, Recon_One:0.0, T_Reg:10.611853882670403, T_MSE:1109.708869934082, CI:0, \n",
      "Iteration: 2300 epochs:115, Training: RAE:0.019714662805199623, Loss: 22.317110061645508, Ranking:1.838822603225708, Reg:4.036472320556641, Gen:14.160430908203125, Disc:0.0030597662553191185, Recon_One:0.0, T_Reg:8.153619766235352,T_MSE:392.296142578125,  CI:0.0 Validation RAE:0.022999100299784914 Loss:23.059997707605362, Ranking:4.8299660831689835, Reg:4.414891600608826, Gen:13.126910299062729, Disc:0.006642338863457553, Recon_One:0.0, T_Reg:9.926444754004478, T_MSE:799.4732713699341, CI:0, \n",
      "it:2300, trainCI:0.0, train_ranking:2.3153367042541504, train_RAE:0.01831650920212269,  train_Gen:14.969562530517578, train_Disc:0.0021560140885412693, train_reg:4.036528587341309, train_t_reg:2.6789000034332275, train_t_mse:388.83868408203125, train_layer_one_recon:0.0\n",
      "Iteration: 2320 epochs:116, Training: RAE:0.024369658902287483, Loss: 20.761207580566406, Ranking:4.722869396209717, Reg:4.036740303039551, Gen:13.550928115844727, Disc:0.004857223480939865, Recon_One:0.0, T_Reg:7.205422878265381,T_MSE:333.1307678222656,  CI:0.0 Validation RAE:0.020693843602202833 Loss:23.1779608130455, Ranking:4.820798493921757, Reg:4.415184706449509, Gen:13.487235635519028, Disc:0.00560015530209057, Recon_One:0.0, T_Reg:9.685124978423119, T_MSE:920.5571641921997, CI:0, \n",
      "Iteration: 2340 epochs:117, Training: RAE:0.0325203537940979, Loss: 23.139692306518555, Ranking:4.60495662689209, Reg:4.036764144897461, Gen:13.766345024108887, Disc:0.0035154763609170914, Recon_One:0.0, T_Reg:9.369832038879395,T_MSE:349.7820739746094,  CI:0.0 Validation RAE:0.0189246351365 Loss:19.922213941812515, Ranking:4.826850138604641, Reg:4.415210783481598, Gen:13.181826025247574, Disc:0.006483448654762469, Recon_One:0.0, T_Reg:6.733904801309109, T_MSE:543.3451929092407, CI:0, *\n",
      "Iteration: 2360 epochs:118, Training: RAE:0.013999341055750847, Loss: 21.668594360351562, Ranking:3.8890604972839355, Reg:4.037080764770508, Gen:14.112894058227539, Disc:0.002617473714053631, Recon_One:0.0, T_Reg:7.5530829429626465,T_MSE:608.088134765625,  CI:0.0 Validation RAE:0.01786063375766389 Loss:21.45778501033783, Ranking:4.821589045226574, Reg:4.415557086467743, Gen:12.992666572332382, Disc:0.007153595986892469, Recon_One:0.0, T_Reg:8.457965612411499, T_MSE:800.4753036499023, CI:0, \n",
      "Iteration: 2380 epochs:119, Training: RAE:0.02664714865386486, Loss: 21.52053451538086, Ranking:4.193548679351807, Reg:4.0372419357299805, Gen:14.53763198852539, Disc:0.0023616706021130085, Recon_One:0.0, T_Reg:6.980541706085205,T_MSE:301.6806335449219,  CI:0.0 Validation RAE:0.024359411734621972 Loss:25.402192294597626, Ranking:4.818259686231613, Reg:4.415733367204666, Gen:13.42662113904953, Disc:0.006260640890104696, Recon_One:0.0, T_Reg:11.96931006014347, T_MSE:1224.0359659194946, CI:0, \n",
      "Iteration: 2400 epochs:120, Training: RAE:0.01555988285690546, Loss: 17.459468841552734, Ranking:4.341875076293945, Reg:4.037544250488281, Gen:14.3427734375, Disc:0.0026480304077267647, Recon_One:0.0, T_Reg:3.114046573638916,T_MSE:373.41302490234375,  CI:0.0 Validation RAE:0.01826792399515398 Loss:22.40636396408081, Ranking:4.8141763880848885, Reg:4.416064023971558, Gen:13.908574342727661, Disc:0.004654002601455431, Recon_One:0.0, T_Reg:8.493135079741478, T_MSE:843.3728876113892, CI:0, \n",
      "it:2400, trainCI:0.0, train_ranking:5.0043416023254395, train_RAE:0.020145358517766,  train_Gen:13.914133071899414, train_Disc:0.0036440580151975155, train_reg:4.037533760070801, train_t_reg:7.2441301345825195, train_t_mse:328.3854064941406, train_layer_one_recon:0.0\n",
      "Iteration: 2420 epochs:121, Training: RAE:0.022780094295740128, Loss: 19.730388641357422, Ranking:2.46127986907959, Reg:4.037540435791016, Gen:14.355463027954102, Disc:0.002419736236333847, Recon_One:0.0, T_Reg:5.372505187988281,T_MSE:432.082763671875,  CI:0.0 Validation RAE:0.020215271186316386 Loss:23.5873401761055, Ranking:4.823199979960918, Reg:4.416059851646423, Gen:13.632166147232056, Disc:0.005296448842273094, Recon_One:0.0, T_Reg:9.949877470731735, T_MSE:1101.0910692214966, CI:0, \n",
      "Iteration: 2440 epochs:122, Training: RAE:0.02220531553030014, Loss: 24.085399627685547, Ranking:4.748827934265137, Reg:4.037827968597412, Gen:14.01761245727539, Disc:0.0030090278014540672, Recon_One:0.0, T_Reg:10.064777374267578,T_MSE:760.8952026367188,  CI:0.0 Validation RAE:0.01797108058235608 Loss:24.4872727394104, Ranking:4.820142395794392, Reg:4.4163743406534195, Gen:13.703247964382172, Disc:0.005420823144959286, Recon_One:0.0, T_Reg:10.778603672981262, T_MSE:1686.1005601882935, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2460 epochs:123, Training: RAE:0.01197133306413889, Loss: 22.486156463623047, Ranking:3.321845293045044, Reg:4.03791618347168, Gen:13.921943664550781, Disc:0.004621407482773066, Recon_One:0.0, T_Reg:8.559590339660645,T_MSE:810.1785278320312,  CI:0.0 Validation RAE:0.018840064061805606 Loss:22.5766778588295, Ranking:4.819108493626118, Reg:4.41647082567215, Gen:13.70860293507576, Disc:0.00699400129087735, Recon_One:0.0, T_Reg:8.861081063747406, T_MSE:624.1119613647461, CI:0, \n",
      "Iteration: 2480 epochs:124, Training: RAE:0.028519796207547188, Loss: 26.83820915222168, Ranking:5.8248724937438965, Reg:4.038102149963379, Gen:13.912857055664062, Disc:0.0034025926142930984, Recon_One:0.0, T_Reg:12.92194938659668,T_MSE:1438.0718994140625,  CI:0.0 Validation RAE:0.018503128027077764 Loss:23.13135302066803, Ranking:4.819657623767853, Reg:4.416674226522446, Gen:13.668300986289978, Disc:0.005267392916721292, Recon_One:0.0, T_Reg:9.457784995436668, T_MSE:963.7504091262817, CI:0, \n",
      "Iteration: 2500 epochs:125, Training: RAE:0.0141215855255723, Loss: 16.90883445739746, Ranking:2.867727041244507, Reg:4.038302421569824, Gen:13.598560333251953, Disc:0.0071477629244327545, Recon_One:0.0, T_Reg:3.303126811981201,T_MSE:262.9017333984375,  CI:0.0 Validation RAE:0.018931073311250657 Loss:21.617764472961426, Ranking:4.820392839610577, Reg:4.416893273591995, Gen:13.78025197982788, Disc:0.004921837302390486, Recon_One:0.0, T_Reg:7.832590714097023, T_MSE:547.2406692504883, CI:0, \n",
      "it:2500, trainCI:0.0, train_ranking:3.7849693298339844, train_RAE:0.020900338888168335,  train_Gen:14.580501556396484, train_Disc:0.002207255456596613, train_reg:4.038311958312988, train_t_reg:8.088573455810547, train_t_mse:375.62188720703125, train_layer_one_recon:0.0\n",
      "Iteration: 2520 epochs:126, Training: RAE:0.013940389268100262, Loss: 20.50136375427246, Ranking:4.411202907562256, Reg:4.038393497467041, Gen:14.313913345336914, Disc:0.0032464470714330673, Recon_One:0.0, T_Reg:6.184203624725342,T_MSE:192.75729370117188,  CI:0.0 Validation RAE:0.01885912148281932 Loss:23.42573308944702, Ranking:4.827635318040848, Reg:4.416992887854576, Gen:13.59931805729866, Disc:0.005340627612895332, Recon_One:0.0, T_Reg:9.821074873209, T_MSE:823.0957279205322, CI:0, \n",
      "Iteration: 2540 epochs:127, Training: RAE:0.012570932507514954, Loss: 20.500478744506836, Ranking:3.8001327514648438, Reg:4.038687705993652, Gen:14.637765884399414, Disc:0.0024930881336331367, Recon_One:0.0, T_Reg:5.860220432281494,T_MSE:954.744384765625,  CI:0.0 Validation RAE:0.020161657623248175 Loss:22.982733130455017, Ranking:4.821942336857319, Reg:4.417314678430557, Gen:13.590250343084335, Disc:0.005254194678855129, Recon_One:0.0, T_Reg:9.387228786945343, T_MSE:783.7346267700195, CI:0, \n",
      "Iteration: 2560 epochs:128, Training: RAE:0.030864650383591652, Loss: 23.701927185058594, Ranking:5.507547855377197, Reg:4.038993835449219, Gen:15.077522277832031, Disc:0.0020407370757311583, Recon_One:0.0, T_Reg:8.62236499786377,T_MSE:454.6986999511719,  CI:0.0 Validation RAE:0.019294580619316548 Loss:24.32948875427246, Ranking:4.826559379696846, Reg:4.417649507522583, Gen:13.610488414764404, Disc:0.005305020546074957, Recon_One:0.0, T_Reg:10.713695228099823, T_MSE:975.3307175636292, CI:0, \n",
      "Iteration: 2580 epochs:129, Training: RAE:0.025082342326641083, Loss: 21.07242774963379, Ranking:4.319615364074707, Reg:4.039059638977051, Gen:14.617149353027344, Disc:0.0022558062337338924, Recon_One:0.0, T_Reg:6.453022480010986,T_MSE:254.28378295898438,  CI:0.0 Validation RAE:0.01707417023135349 Loss:22.768668234348297, Ranking:4.82091698795557, Reg:4.417721480131149, Gen:14.220180690288544, Disc:0.004075010692758951, Recon_One:0.0, T_Reg:8.544411912560463, T_MSE:775.1248369216919, CI:0, \n",
      "Iteration: 2600 epochs:130, Training: RAE:0.029415661469101906, Loss: 24.020893096923828, Ranking:3.5327274799346924, Reg:4.038971900939941, Gen:14.107324600219727, Disc:0.0031435186974704266, Recon_One:0.0, T_Reg:9.910425186157227,T_MSE:410.920654296875,  CI:0.0 Validation RAE:0.020505937398411334 Loss:22.44027078151703, Ranking:4.825265489518642, Reg:4.417625516653061, Gen:13.592008143663406, Disc:0.005437324914964847, Recon_One:0.0, T_Reg:8.842825159430504, T_MSE:575.3485221862793, CI:0, \n",
      "it:2600, trainCI:0.0, train_ranking:3.9111037254333496, train_RAE:0.02348293550312519,  train_Gen:14.450408935546875, train_Disc:0.0024453550577163696, train_reg:4.038978099822998, train_t_reg:5.584012985229492, train_t_mse:216.5763702392578, train_layer_one_recon:0.0\n",
      "Iteration: 2620 epochs:131, Training: RAE:0.01661086082458496, Loss: 21.365154266357422, Ranking:2.8380680084228516, Reg:4.039417266845703, Gen:14.767724990844727, Disc:0.002473494503647089, Recon_One:0.0, T_Reg:6.594955921173096,T_MSE:470.0568542480469,  CI:0.0 Validation RAE:0.021890634729061276 Loss:22.556710362434387, Ranking:4.82522089779377, Reg:4.418112635612488, Gen:13.95972141623497, Disc:0.004617407917976379, Recon_One:0.0, T_Reg:8.592371068894863, T_MSE:613.7255220413208, CI:0, \n",
      "Iteration: 2640 epochs:132, Training: RAE:0.01843070611357689, Loss: 20.18976593017578, Ranking:4.428925037384033, Reg:4.03972864151001, Gen:15.238571166992188, Disc:0.0016700539272278547, Recon_One:0.0, T_Reg:4.949524402618408,T_MSE:415.4888916015625,  CI:0.0 Validation RAE:0.018865977093810216 Loss:22.566136062145233, Ranking:4.8243245258927345, Reg:4.418453201651573, Gen:14.24005326628685, Disc:0.004120153702388052, Recon_One:0.0, T_Reg:8.321962624788284, T_MSE:726.4218397140503, CI:0, \n",
      "Iteration: 2660 epochs:133, Training: RAE:0.02220156043767929, Loss: 22.595897674560547, Ranking:4.465612411499023, Reg:4.039667129516602, Gen:14.200469017028809, Disc:0.0029525391291826963, Recon_One:0.0, T_Reg:8.392477035522461,T_MSE:558.9240112304688,  CI:0.0 Validation RAE:0.02019790082704276 Loss:22.94649440050125, Ranking:4.819091960787773, Reg:4.418385922908783, Gen:13.940877944231033, Disc:0.004669153408030979, Recon_One:0.0, T_Reg:9.000946789979935, T_MSE:697.0671758651733, CI:0, \n",
      "Iteration: 2680 epochs:134, Training: RAE:0.018157798796892166, Loss: 19.95881462097168, Ranking:5.691722393035889, Reg:4.039925575256348, Gen:14.234132766723633, Disc:0.00255730003118515, Recon_One:0.0, T_Reg:5.722125053405762,T_MSE:165.47621154785156,  CI:0.0 Validation RAE:0.02161184945725836 Loss:23.55502885580063, Ranking:4.8210132122039795, Reg:4.41866859793663, Gen:14.314240962266922, Disc:0.004066475456056651, Recon_One:0.0, T_Reg:9.236721441149712, T_MSE:521.7603330612183, CI:0, \n",
      "Iteration: 2700 epochs:135, Training: RAE:0.018436260521411896, Loss: 21.87085723876953, Ranking:4.1240010261535645, Reg:4.040402412414551, Gen:14.293283462524414, Disc:0.002194194123148918, Recon_One:0.0, T_Reg:7.575379371643066,T_MSE:468.4625549316406,  CI:0.0 Validation RAE:0.019483658659737557 Loss:22.561317026615143, Ranking:4.821871563792229, Reg:4.419190138578415, Gen:13.89389443397522, Disc:0.004543989663943648, Recon_One:0.0, T_Reg:8.662878826260567, T_MSE:652.2013692855835, CI:0, \n",
      "it:2700, trainCI:0.0, train_ranking:2.7700769901275635, train_RAE:0.0229624155908823,  train_Gen:14.457122802734375, train_Disc:0.0022680803667753935, train_reg:4.04043436050415, train_t_reg:9.820943832397461, train_t_mse:413.5020751953125, train_layer_one_recon:0.0\n",
      "Iteration: 2720 epochs:136, Training: RAE:0.01863276958465576, Loss: 18.309764862060547, Ranking:2.844907760620117, Reg:4.040708541870117, Gen:14.334896087646484, Disc:0.0040825833566486835, Recon_One:0.0, T_Reg:3.9707863330841064,T_MSE:264.9431457519531,  CI:0.0 Validation RAE:0.018939797504572198 Loss:22.76054847240448, Ranking:4.815432205796242, Reg:4.419524967670441, Gen:13.756224393844604, Disc:0.005170248434296809, Recon_One:0.0, T_Reg:8.999153889715672, T_MSE:901.2734785079956, CI:0, \n",
      "Iteration: 2740 epochs:137, Training: RAE:0.015130165964365005, Loss: 25.609241485595703, Ranking:1.378483772277832, Reg:4.040987491607666, Gen:14.755086898803711, Disc:0.0023818728514015675, Recon_One:0.0, T_Reg:10.851771354675293,T_MSE:281.4259338378906,  CI:0.0 Validation RAE:0.019371491769561544 Loss:23.326255679130554, Ranking:4.81824242323637, Reg:4.419830068945885, Gen:14.504826098680496, Disc:0.003970714889874216, Recon_One:0.0, T_Reg:8.817458778619766, T_MSE:719.3506298065186, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2760 epochs:138, Training: RAE:0.02150295302271843, Loss: 19.155942916870117, Ranking:4.8046441078186035, Reg:4.0409321784973145, Gen:14.536741256713867, Disc:0.003224210347980261, Recon_One:0.0, T_Reg:4.615976810455322,T_MSE:143.26852416992188,  CI:0.0 Validation RAE:0.022542541584698483 Loss:22.9004465341568, Ranking:4.818563848733902, Reg:4.419769570231438, Gen:14.108545541763306, Disc:0.004217070010781754, Recon_One:0.0, T_Reg:8.78768339753151, T_MSE:655.8289422988892, CI:0, \n",
      "Iteration: 2780 epochs:139, Training: RAE:0.019856246188282967, Loss: 23.75255012512207, Ranking:2.286072015762329, Reg:4.041248321533203, Gen:14.87413215637207, Disc:0.0018574283458292484, Recon_One:0.0, T_Reg:8.87656021118164,T_MSE:355.45166015625,  CI:0.0 Validation RAE:0.017516102379886433 Loss:22.725296914577484, Ranking:4.8179473876953125, Reg:4.420115351676941, Gen:13.995432138442993, Disc:0.004312075361667667, Recon_One:0.0, T_Reg:8.725552782416344, T_MSE:647.1978702545166, CI:0, \n",
      "Iteration: 2800 epochs:140, Training: RAE:0.016518816351890564, Loss: 19.352312088012695, Ranking:3.2999086380004883, Reg:4.0414628982543945, Gen:14.54509162902832, Disc:0.0028306124731898308, Recon_One:0.0, T_Reg:4.8043904304504395,T_MSE:225.56607055664062,  CI:0.0 Validation RAE:0.015964954538503662 Loss:21.461634814739227, Ranking:4.817721508443356, Reg:4.420350044965744, Gen:14.15947711467743, Disc:0.004292200435884297, Recon_One:0.0, T_Reg:7.297865211963654, T_MSE:704.2476139068604, CI:0, \n",
      "it:2800, trainCI:0.0, train_ranking:1.6581040620803833, train_RAE:0.014487840235233307,  train_Gen:14.407035827636719, train_Disc:0.002651226706802845, train_reg:4.041460037231445, train_t_reg:13.467973709106445, train_t_mse:862.814453125, train_layer_one_recon:0.0\n",
      "Iteration: 2820 epochs:141, Training: RAE:0.01768280752003193, Loss: 21.863664627075195, Ranking:2.749464988708496, Reg:4.041758060455322, Gen:14.38235092163086, Disc:0.003697317559272051, Recon_One:0.0, T_Reg:7.477616310119629,T_MSE:298.93548583984375,  CI:0.0 Validation RAE:0.017944009567145258 Loss:22.567615151405334, Ranking:4.823509931564331, Reg:4.420672878623009, Gen:14.497946351766586, Disc:0.003571914254280273, Recon_One:0.0, T_Reg:8.06609681993723, T_MSE:794.2524242401123, CI:0, \n",
      "Iteration: 2840 epochs:142, Training: RAE:0.021766887977719307, Loss: 21.740203857421875, Ranking:3.201495885848999, Reg:4.041665554046631, Gen:15.199077606201172, Disc:0.0015425197780132294, Recon_One:0.0, T_Reg:6.539583683013916,T_MSE:288.9983215332031,  CI:0.0 Validation RAE:0.02125102502759546 Loss:23.153319478034973, Ranking:4.829512864351273, Reg:4.4205716997385025, Gen:14.235785186290741, Disc:0.004162398137850687, Recon_One:0.0, T_Reg:8.91337177157402, T_MSE:637.3229780197144, CI:0, \n",
      "Iteration: 2860 epochs:143, Training: RAE:0.02786673605442047, Loss: 23.719436645507812, Ranking:4.970144748687744, Reg:4.041933536529541, Gen:14.578807830810547, Disc:0.002632245421409607, Recon_One:0.0, T_Reg:9.137996673583984,T_MSE:359.45220947265625,  CI:0.0 Validation RAE:0.01857443037442863 Loss:21.917981207370758, Ranking:4.8257173001766205, Reg:4.4208648055791855, Gen:14.468962877988815, Disc:0.0038186274759937078, Recon_One:0.0, T_Reg:7.445199400186539, T_MSE:614.0847992897034, CI:0, \n",
      "Iteration: 2880 epochs:144, Training: RAE:0.01833387278020382, Loss: 21.662683486938477, Ranking:4.942845821380615, Reg:4.042419910430908, Gen:15.14862060546875, Disc:0.0017184983007609844, Recon_One:0.0, T_Reg:6.512344837188721,T_MSE:476.6955871582031,  CI:0.0 Validation RAE:0.018200353923020884 Loss:23.822030544281006, Ranking:4.821171499788761, Reg:4.421396777033806, Gen:14.658877730369568, Disc:0.003374995940248482, Recon_One:0.0, T_Reg:9.159777089953423, T_MSE:678.6782402992249, CI:0, \n",
      "Iteration: 2900 epochs:145, Training: RAE:0.021305367350578308, Loss: 21.29790496826172, Ranking:2.1943557262420654, Reg:4.0425028800964355, Gen:14.889521598815918, Disc:0.002158695599064231, Recon_One:0.0, T_Reg:6.406225204467773,T_MSE:411.7796630859375,  CI:0.0 Validation RAE:0.018694526923354715 Loss:25.539254009723663, Ranking:4.8225985914468765, Reg:4.421487525105476, Gen:14.42973467707634, Disc:0.0037145276583032683, Recon_One:0.0, T_Reg:11.105804607272148, T_MSE:1047.4429121017456, CI:0, \n",
      "it:2900, trainCI:0.0, train_ranking:4.584461212158203, train_RAE:0.022652409970760345,  train_Gen:14.760576248168945, train_Disc:0.0020648082718253136, train_reg:4.04252290725708, train_t_reg:6.144323825836182, train_t_mse:238.87860107421875, train_layer_one_recon:0.0\n",
      "Iteration: 2920 epochs:146, Training: RAE:0.01788458228111267, Loss: 18.353862762451172, Ranking:3.7244465351104736, Reg:4.042745590209961, Gen:15.041290283203125, Disc:0.001747639151290059, Recon_One:0.0, T_Reg:3.3108246326446533,T_MSE:341.0413513183594,  CI:0.0 Validation RAE:0.01838867980404757 Loss:24.56873446702957, Ranking:4.826513014733791, Reg:4.421752989292145, Gen:14.479216575622559, Disc:0.0035387118768994696, Recon_One:0.0, T_Reg:10.085979551076889, T_MSE:869.6120328903198, CI:0, \n",
      "Iteration: 2940 epochs:147, Training: RAE:0.011043612845242023, Loss: 24.156757354736328, Ranking:2.707611322402954, Reg:4.043096542358398, Gen:15.748104095458984, Disc:0.0013605165295302868, Recon_One:0.0, T_Reg:8.407293319702148,T_MSE:465.6622009277344,  CI:0.0 Validation RAE:0.015197359374724329 Loss:23.468442678451538, Ranking:4.818721771240234, Reg:4.422136843204498, Gen:14.625770956277847, Disc:0.003691115678520873, Recon_One:0.0, T_Reg:8.838980440050364, T_MSE:1094.8743047714233, CI:0, \n",
      "Iteration: 2960 epochs:148, Training: RAE:0.012874149717390537, Loss: 24.452171325683594, Ranking:0.17577610909938812, Reg:4.042994499206543, Gen:15.261510848999023, Disc:0.001437879167497158, Recon_One:0.0, T_Reg:9.189221382141113,T_MSE:133.17514038085938,  CI:0.0 Validation RAE:0.01715997293649707 Loss:22.808463037014008, Ranking:4.825428262352943, Reg:4.422025233507156, Gen:15.040951937437057, Disc:0.0029598183682537638, Recon_One:0.0, T_Reg:7.764551512897015, T_MSE:600.0383195877075, CI:0, \n",
      "Iteration: 2980 epochs:149, Training: RAE:0.021418500691652298, Loss: 20.597543716430664, Ranking:4.698121547698975, Reg:4.043163299560547, Gen:14.039173126220703, Disc:0.002883646171540022, Recon_One:0.0, T_Reg:6.555487155914307,T_MSE:213.18382263183594,  CI:0.0 Validation RAE:0.020309331855969504 Loss:23.138330399990082, Ranking:4.819812938570976, Reg:4.422209858894348, Gen:14.348759859800339, Disc:0.003914269422239158, Recon_One:0.0, T_Reg:8.785656586289406, T_MSE:655.8488759994507, CI:0, \n",
      "Iteration: 3000 epochs:150, Training: RAE:0.010019872337579727, Loss: 20.062463760375977, Ranking:2.7442922592163086, Reg:4.043292045593262, Gen:14.988448143005371, Disc:0.001685276161879301, Recon_One:0.0, T_Reg:5.072330474853516,T_MSE:402.7174987792969,  CI:0.0 Validation RAE:0.017719367548124865 Loss:23.498316943645477, Ranking:4.819290824234486, Reg:4.42235067486763, Gen:14.655298084020615, Disc:0.0033003196731442586, Recon_One:0.0, T_Reg:8.839718654751778, T_MSE:761.5430974960327, CI:0, \n",
      "it:3000, trainCI:0.0, train_ranking:6.043493747711182, train_RAE:0.017092198133468628,  train_Gen:15.145066261291504, train_Disc:0.002669813809916377, train_reg:4.043298244476318, train_t_reg:5.814220905303955, train_t_mse:278.21319580078125, train_layer_one_recon:0.0\n",
      "Iteration: 3020 epochs:151, Training: RAE:0.016640955582261086, Loss: 21.90990447998047, Ranking:3.932995319366455, Reg:4.043281078338623, Gen:15.154104232788086, Disc:0.0017411578446626663, Recon_One:0.0, T_Reg:6.754058361053467,T_MSE:252.94969177246094,  CI:0.0 Validation RAE:0.021145871025510132 Loss:25.815347611904144, Ranking:4.816940136253834, Reg:4.422338679432869, Gen:14.680981665849686, Disc:0.0034254640122526325, Recon_One:0.0, T_Reg:11.13094025850296, T_MSE:1275.2618713378906, CI:0, \n",
      "Iteration: 3040 epochs:152, Training: RAE:0.00953457411378622, Loss: 24.801673889160156, Ranking:3.846683979034424, Reg:4.0436224937438965, Gen:15.135549545288086, Disc:0.0015747444704174995, Recon_One:0.0, T_Reg:9.66455078125,T_MSE:404.1036682128906,  CI:0.0 Validation RAE:0.014412068499950692 Loss:24.454361021518707, Ranking:4.820741280913353, Reg:4.422712102532387, Gen:14.704274505376816, Disc:0.003409200653550215, Recon_One:0.0, T_Reg:9.746677204966545, T_MSE:839.2666387557983, CI:0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3060 epochs:153, Training: RAE:0.021489061415195465, Loss: nan, Ranking:6.039180755615234, Reg:nan, Gen:nan, Disc:nan, Recon_One:0.0, T_Reg:5.935276508331299,T_MSE:479.5718688964844,  CI:0.0 Validation RAE:0.01896293374011293 Loss:nan, Ranking:4.8259870409965515, Reg:nan, Gen:nan, Disc:nan, Recon_One:0.0, T_Reg:9.374196842312813, T_MSE:914.67702293396, CI:0, \n",
      "No improvement found in a while, stopping optimization.\n",
      "Time usage: 0:41:13\n",
      "finished enqueueing\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\raibe\\Desktop\\DATE\\summaries\\ndb_d\\d10\\DATE_AE_model\n",
      "Test observed_death:(83,), percentage:0.0415\n",
      "observed_samples:(83, 200), empirical_observed:(83,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\ipykernel_launcher.py:83: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n",
      "C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n",
      "C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed_samples:(1917, 200), empirical_observed:(1917,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\ipykernel_launcher.py:83: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n",
      "C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n",
      "C:\\Users\\raibe\\Anaconda3\\envs\\Date\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":Test RAE:0.02425400372594595, Loss:20.1986074924469, Gen:12.721100568771362, Disc:0.006093005381990224, Reg:4.238602352142334, Ranking5.998115795850754, Recon:0.0, T_Reg:7.4714138269424435,T_MSE:335.8472595214844, CI:0, Observed: CI:0, Correlation:SpearmanrResult(correlation=0.9454927198044526, pvalue=3.178245051943946e-41)\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\raibe\\Desktop\\DATE\\summaries\\ndb_d\\d10\\DATE_AE_model\n",
      "Valid observed_death:(55,), percentage:0.034375\n",
      ":Valid RAE:0.01999002613592893, Loss:20.835349202156067, Gen:13.181351840496063, Disc:0.0064834975491976365, Reg:4.415210783481598, Ranking4.823435664176941, Recon:0.0, T_Reg:7.647513881325722,T_MSE:598.9677810668945, CI:0, Observed: CI:0, Correlation:SpearmanrResult(correlation=0.8626634321594305, pvalue=2.5652684958368715e-17)\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\raibe\\Desktop\\DATE\\summaries\\ndb_d\\d10\\DATE_AE_model\n",
      "Train observed_death:(180,), percentage:0.028125\n",
      ":Train RAE:0.02090722457069205, Loss:19.419534921646118, Gen:12.529852710664272, Disc:0.005948240999714471, Reg:4.194450244307518, Ranking3.702960911206901, Recon:0.0, T_Reg:6.883733883500099,T_MSE:316.6328907608986, CI:0, Observed: CI:0, Correlation:SpearmanrResult(correlation=0.9538132338618522, pvalue=6.715391727882695e-95)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    r_epochs = 600\n",
    "    \n",
    "    # Two date models to choose\n",
    "    simple = True\n",
    "    if simple:\n",
    "        model = DATE\n",
    "    else:\n",
    "        model = DATE_AE\n",
    "\n",
    "    \n",
    "    data_set = generate_data()\n",
    "    train_data, valid_data, test_data, end_t, covariates, one_hot_indices, imputation_values \\\n",
    "        = data_set['train'], \\\n",
    "          data_set['valid'], \\\n",
    "          data_set['test'], \\\n",
    "          data_set['end_t'], \\\n",
    "          data_set['covariates'], \\\n",
    "          data_set[\n",
    "              'one_hot_indices'], \\\n",
    "          data_set[\n",
    "              'imputation_values']\n",
    "\n",
    "    print(\"imputation_values:{}, one_hot_indices:{}\".format(imputation_values, one_hot_indices))\n",
    "    print(\"end_t:{}\".format(end_t))\n",
    "    train = {'x': train_data['x'], 'e': train_data['e'], 't': train_data['t']}\n",
    "    valid = {'x': valid_data['x'], 'e': valid_data['e'], 't': valid_data['t']}\n",
    "    test = {'x': test_data['x'], 'e': test_data['e'], 't': test_data['t']}\n",
    "\n",
    "    perfomance_record = []\n",
    "\n",
    "    date = model(batch_size=350,\n",
    "                 learning_rate=3e-4,\n",
    "                 beta1=0.9,\n",
    "                 beta2=0.999,\n",
    "                 require_improvement=10000,\n",
    "                 num_iterations=40000, seed=31415,\n",
    "                 l2_reg=0.001,\n",
    "                 hidden_dim=[50, 50],\n",
    "                 train_data=train, test_data=test, valid_data=valid,\n",
    "                 input_dim=train['x'].shape[1],\n",
    "                 num_examples=train['x'].shape[0], keep_prob=0.8,\n",
    "                 latent_dim=50, end_t=end_t,\n",
    "                 path_large_data='C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE',\n",
    "                 covariates=covariates,\n",
    "                 categorical_indices=one_hot_indices,\n",
    "                 disc_updates=1,\n",
    "                 sample_size=200, imputation_values=imputation_values,\n",
    "                 max_epochs=r_epochs,  gen_updates=2)\n",
    "\n",
    "    with date.session:\n",
    "        date.train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.load('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\Test_empirical_time.npy')\n",
    "ee = np.load('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\Test_data_e.npy')\n",
    "pp1 = np.load('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\ndb_d\\\\d10\\\\Test_predicted_time.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910623446535048\n"
     ]
    }
   ],
   "source": [
    "#print(concordance_index(tt, pp1, ee24))\n",
    "#print(concordance_index(tt, pp1, ee48))\n",
    "print(concordance_index(tt, pp1, ee))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NewDate)",
   "language": "python",
   "name": "newdate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
