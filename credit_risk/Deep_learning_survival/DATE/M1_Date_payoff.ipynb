{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lifelines\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "from scipy.stats.stats import spearmanr\n",
    "\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "seed = 31415\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### C(t)-INDEX CALCULATION\n",
    "def c_index(Prediction, Time_survival, Death, Time):\n",
    "    '''\n",
    "        Raiber: okay, so it does not make a diffrence if the predictions are scores or risks(softmax probabilities) \n",
    "        This is a cause-specific c(t)-index\n",
    "        - Prediction      : risk at Time (higher --> more risky) <class 'numpy.ndarray'> \n",
    "          shape (7997,)\n",
    "        - Time_survival   : survival/censoring time   <class 'numpy.ndarray'>\n",
    "        shape (7997, 1)\n",
    "        - Death           :   <class 'numpy.ndarray'>\n",
    "            > 1: death\n",
    "            > 0: censored (including death from other cause)\n",
    "        shape (7997,)\n",
    "        - Time            : time of evaluation (time-horizon when evaluating C-index)   <class 'int'>\n",
    "        scalar value \n",
    "    '''\n",
    "    N = len(Prediction)# N = 7997 \n",
    "    #A.shape (7997, 7997) for validation \n",
    "    A = np.zeros((N,N))\n",
    "    Q = np.zeros((N,N))\n",
    "    N_t = np.zeros((N,N))\n",
    "    Num = 0\n",
    "    Den = 0\n",
    "    for i in range(N):\n",
    "        # np.where return the index of the values that fullfil the condition \n",
    "        # let assume i = 0, then np.where(Time_survival[i] < Time_survival) will return the indexes \n",
    "        # from 0 until 7996 (because the length is 7996) that have a bigger time then the index 0\n",
    "        # and the assign to A[0, list of biiger times that the value in 0 ] the value 1 \n",
    "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
    "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
    "  \n",
    "        if (Time_survival[i]<=Time and Death[i]==1):\n",
    "            N_t[i,:] = 1\n",
    "\n",
    "    Num  = np.sum(((A)*N_t)*Q)\n",
    "    Den  = np.sum((A)*N_t)\n",
    "\n",
    "    if Num == 0 and Den == 0:\n",
    "        result = -1 # not able to compute c-index!\n",
    "    else:\n",
    "        result = float(Num/Den)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
    "def CensoringProb(Y, T):\n",
    "\n",
    "    T = T.reshape([-1]) # (N,) - np array\n",
    "    Y = Y.reshape([-1]) # (N,) - np array\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
    "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
    "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
    "    G = CensoringProb(Y_train, T_train)\n",
    "    N = len(Prediction)\n",
    "\n",
    "    W = np.zeros(len(Y_test))\n",
    "    Y_tilde = (T_test > Time).astype(float)\n",
    "\n",
    "    for i in range(N):\n",
    "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
    "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
    "\n",
    "        if len(tmp_idx1) == 0:\n",
    "            G1 = G[1, -1]\n",
    "        else:\n",
    "            G1 = G[1, tmp_idx1[0]]\n",
    "\n",
    "        if len(tmp_idx2) == 0:\n",
    "            G2 = G[1, -1]\n",
    "        else:\n",
    "            G2 = G[1, tmp_idx2[0]]\n",
    "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
    "\n",
    "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
    "\n",
    "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raiber used \n",
    "def f_get_Normalization(X, norm_mode): # raiber added functions \n",
    "    num_Patient, num_Feature = np.shape(X)\n",
    "\n",
    "    if norm_mode == 'standard': #zero mean unit variance\n",
    "        for j in range(num_Feature):\n",
    "            if np.std(X[:,j]) != 0:\n",
    "                X[:,j] = (X[:,j] - np.mean(X[:, j]))/np.std(X[:,j])\n",
    "            else:\n",
    "                X[:,j] = (X[:,j] - np.mean(X[:, j]))\n",
    "    elif norm_mode == 'normal': #min-max normalization\n",
    "        for j in range(num_Feature):\n",
    "            X[:,j] = (X[:,j] - np.min(X[:,j]))/(np.max(X[:,j]) - np.min(X[:,j]))\n",
    "    else:\n",
    "        print(\"INPUT MODE ERROR!\")\n",
    "\n",
    "    return X\n",
    " \n",
    "def formatted_data(x, t, e, idx):\n",
    "    death_time = np.array(t[idx], dtype=float)\n",
    "    censoring = np.array(e[idx], dtype=float)\n",
    "    covariates = np.array(x[idx])\n",
    "\n",
    "    print(\"observed fold:{}\".format(sum(e[idx]) / len(e[idx])))\n",
    "    survival_data = {'x': covariates, 't': death_time, 'e': censoring}\n",
    "    return survival_data\n",
    "\n",
    "\n",
    "def risk_set(data_t):\n",
    "    size = len(data_t)\n",
    "    risk_set = np.zeros(shape=(size, size))\n",
    "    for idx in range(size):\n",
    "        temp = np.zeros(shape=size)\n",
    "        t_i = data_t[idx]\n",
    "        at_risk = data_t > t_i\n",
    "        temp[at_risk] = 1\n",
    "        # temp[idx] = 0\n",
    "        risk_set[idx] = temp\n",
    "    return risk_set\n",
    "\n",
    "def one_hot_encoder(data, encode):\n",
    "    print(\"Encoding data:{}\".format(data.shape))\n",
    "    data_encoded = data.copy()\n",
    "    encoded = pandas.get_dummies(data_encoded, prefix=encode, columns=encode)\n",
    "    print(\"head of data:{}, data shape:{}\".format(data_encoded.head(), data_encoded.shape))\n",
    "    print(\"Encoded:{}, one_hot:{}{}\".format(encode, encoded.shape, encoded[0:5]))\n",
    "    return encoded\n",
    "\n",
    "def get_train_median_mode(x, categorial):\n",
    "    categorical_flat = flatten_nested(categorial)\n",
    "    print(\"categorical_flat:{}\".format(categorical_flat))\n",
    "    imputation_values = []\n",
    "    print(\"len covariates:{}, categorical:{}\".format(x.shape[1], len(categorical_flat)))\n",
    "    median = np.nanmedian(x, axis=0)\n",
    "    mode = []\n",
    "    for idx in np.arange(x.shape[1]):\n",
    "        a = x[:, idx]\n",
    "        (_, idx, counts) = np.unique(a, return_index=True, return_counts=True)\n",
    "        index = idx[np.argmax(counts)]\n",
    "        mode_idx = a[index]\n",
    "        mode.append(mode_idx)\n",
    "    for i in np.arange(x.shape[1]):\n",
    "        if i in categorical_flat:\n",
    "            imputation_values.append(mode[i])\n",
    "        else:\n",
    "            imputation_values.append(median[i])\n",
    "    print(\"imputation_values:{}\".format(imputation_values))\n",
    "    return imputation_values\n",
    "\n",
    "\n",
    "def missing_proportion(dataset):\n",
    "    missing = 0\n",
    "    columns = np.array(dataset.columns.values)\n",
    "    for column in columns:\n",
    "        missing += dataset[column].isnull().sum()\n",
    "    return 100 * (missing / (dataset.shape[0] * dataset.shape[1]))\n",
    "\n",
    "\n",
    "def one_hot_indices(dataset, one_hot_encoder_list):\n",
    "    indices_by_category = []\n",
    "    for colunm in one_hot_encoder_list:\n",
    "        values = dataset.filter(regex=\"{}_.*\".format(colunm)).columns.values\n",
    "        # print(\"values:{}\".format(values, len(values)))\n",
    "        indices_one_hot = []\n",
    "        for value in values:\n",
    "            indice = dataset.columns.get_loc(value)\n",
    "            # print(\"column:{}, indice:{}\".format(colunm, indice))\n",
    "            indices_one_hot.append(indice)\n",
    "        indices_by_category.append(indices_one_hot)\n",
    "    # print(\"one_hot_indices:{}\".format(indices_by_category))\n",
    "    return indices_by_category\n",
    "\n",
    "def flatten_nested(list_of_lists):\n",
    "    flattened = [val for sublist in list_of_lists for val in sublist]\n",
    "    return flattened\n",
    "\n",
    "def get_missing_mask(data, imputation_values=None):\n",
    "    copy = data\n",
    "    for i in np.arange(len(data)):\n",
    "        row = data[i]\n",
    "        indices = np.isnan(row)\n",
    "        # print(\"indices:{}, {}\".format(indices, np.where(indices)))\n",
    "        if imputation_values is None:\n",
    "            copy[i][indices] = 0\n",
    "        else:\n",
    "            for idx in np.arange(len(indices)):\n",
    "                if indices[idx]:\n",
    "                    # print(\"idx:{}, imputation_values:{}\".format(idx, np.array(imputation_values)[idx]))\n",
    "                    copy[i][idx] = imputation_values[idx]\n",
    "    # print(\"copy;{}\".format(copy))\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fontsize = 18\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)  # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "\n",
    "font = {'family': 'normal',\n",
    "        'weight': 'bold',\n",
    "        'size': 24}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          # 'figure.figsize': (15, 5),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize': 'x-large',\n",
    "          'xtick.labelsize': 'x-large',\n",
    "          'ytick.labelsize': 'x-large'}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "# We'll hack a bit with the t-SNE code in sklearn 0.15.2.\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('paper')\n",
    "sns.set()\n",
    "title_fontsize = 18\n",
    "label_fontsize = 18\n",
    "\n",
    "\n",
    "def plot_cost(training, validation, name, model, epochs, best_epoch):\n",
    "    x = np.arange(start=0, stop=len(training), step=1).tolist()\n",
    "    constant = 1e-10\n",
    "    plt.figure()\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.ylim(min(min(training), min(validation), 0) - constant, max(max(training), max(validation)) + constant)\n",
    "    plt.plot(x, training, color='blue', linestyle='-', label='training')\n",
    "    plt.plot(x, validation, color='green', linestyle='-', label='validation')\n",
    "    plt.axvline(x=best_epoch, color='red')\n",
    "    title = 'Training {} {}: epochs={}, best epoch={} '.format(model, name, epochs, best_epoch)\n",
    "    plt.title(title, fontsize=title_fontsize)\n",
    "    plt.ylabel(name)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.savefig('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\plots\\\\mort_p\\\\{}_{}'.format(model, name))\n",
    "\n",
    "\n",
    "def box_plots(empirical, predicted, name='data', time='days', log_domain=True):\n",
    "    plt.figure()\n",
    "    if log_domain:\n",
    "        plt.yscale('log')\n",
    "    plt.boxplot(x=predicted, sym='o', notch=0, whis='range')\n",
    "    plt.scatter(x=np.arange(start=1, stop=len(predicted) + 1), y=empirical, color='purple', label='empirical')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.xticks(fontsize=5)\n",
    "    plt.ylabel('t ({})'.format(time))\n",
    "    plt.xlabel('Observation index')\n",
    "    plt.savefig('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\plots\\\\mort_p\\\\{}_box_plot'.format(name))\n",
    "\n",
    "\n",
    "def hist_plots(samples, name, xlabel, empirical=None):\n",
    "    plt.figure()\n",
    "    plt.axvline(x=np.mean(samples), color='grey', label='mean', linestyle='--', )\n",
    "    if empirical:\n",
    "        plt.axvline(x=empirical, color='purple', label='empirical', linestyle='--', )\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.hist(samples, bins=25)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.savefig(\"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\plots\\\\mort_p\\\\{}_hist\".format(name))\n",
    "    plt.figure()\n",
    "    plt.boxplot(x=samples, sym='o', notch=0, whis='range')\n",
    "    plt.scatter(x=1, y=np.mean(samples), color='purple', label='mean')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.savefig('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\plots\\\\mort_p\\\\{}_box_plot'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raiber: we draw form the uniform distribution a tensor with shape batch_size (128) and dim(feature dim 17) and values between zero and 1\n",
    "# shape: (128, 17) \n",
    "def uniform(dim, batch_size):\n",
    "    ones = np.ones(shape=dim, dtype=np.float32)\n",
    "    noise = tf.distributions.Uniform(low=0 * ones, high=ones).sample(sample_shape=[batch_size])\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. tf_helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_variables():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "\n",
    "def mlp_neuron(layer_input, weights, biases, activation=True): # if activation is set to false from the called function, we stay false\n",
    "    mlp = tf.add(tf.matmul(layer_input, weights), biases)\n",
    "    if activation:\n",
    "        return tf.nn.relu(mlp)\n",
    "    else:\n",
    "        return mlp\n",
    "\n",
    "\n",
    "def normalized_mlp(layer_input, weights, biases, is_training, batch_norm, layer, activation=tf.nn.relu):\n",
    "    mlp = tf.add(tf.matmul(layer_input, weights), biases)\n",
    "    if batch_norm:\n",
    "        norm = batch_norm_wrapper(mlp, is_training, layer=layer)\n",
    "        # norm = tf_batch_norm(is_training=is_training, inputs=mlp, layer=layer)\n",
    "        return activation(norm)\n",
    "    else:\n",
    "        return activation(mlp)\n",
    "\n",
    "\n",
    "def dropout_normalised_mlp(layer_input, weights, biases, is_training, batch_norm, layer, keep_prob=1,\n",
    "                           activation=tf.nn.relu):\n",
    "    mlp = normalized_mlp(layer_input, weights, biases, is_training, batch_norm,\n",
    "                         layer=layer, activation=activation)  # apply DropOut to hidden layer\n",
    "    drop_out = tf.cond(is_training, lambda: tf.nn.dropout(mlp, keep_prob), lambda: mlp)\n",
    "    return drop_out\n",
    "\n",
    "\n",
    "def create_nn_weights(layer, network, shape): # layer: the name of the layer, exp: h0_z hiddem layer 0 \n",
    "                                              # network: string (in this example would be the \"decoder\")\n",
    "                                              # shape: [input_shape, hidden_dim] in one of the examples ][34,50]\n",
    "    # raiber: create the name of the weights and variables\n",
    "    h_vars = {}\n",
    "    w_h = 'W_' + network + '_' + layer\n",
    "    b_h = 'b_' + network + '_' + layer\n",
    "    # get the values of the weights and variables \n",
    "    h_vars[w_h] = create_weights(shape=shape, name=w_h)\n",
    "    h_vars[b_h] = create_biases([shape[1]], name=b_h)\n",
    "    variable_summaries(h_vars[w_h], w_h)\n",
    "    variable_summaries(h_vars[b_h], b_h)\n",
    "\n",
    "    return h_vars[w_h], h_vars[b_h]\n",
    "\n",
    "\n",
    "def create_biases(shape, name):\n",
    "    print(\"name:{}, shape{}\".format(name, shape))\n",
    "    return tf.Variable(tf.constant(shape=shape, value=0.0), name=name)\n",
    "\n",
    "\n",
    "def create_weights(shape, name):\n",
    "    print(\"name:{}, shape{}\".format(name, shape))\n",
    "    # initialize weights using Glorot and Bengio(2010) scheme\n",
    "    a = tf.sqrt(6.0 / (shape[0] + shape[1]))\n",
    "    # return tf.Variable(tf.random_normal(shape, stddev=tf.square(0.0001)), name=name)\n",
    "    return tf.Variable(tf.random_uniform(shape, minval=-a, maxval=a, dtype=tf.float32), name=name)\n",
    "\n",
    "\n",
    "def variable_summaries(var, summary_name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(summary_name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "\n",
    "\n",
    "def batch_norm_wrapper(inputs, is_training, layer):\n",
    "    # http://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "    # raiber: get_shape()[-1]] gives the dim of the cols \n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False, name='{}_batch_norm_mean'.format(layer))\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False, name='{}_batch_norm_var'.format(layer))\n",
    "    print(\"batch inputs {}, shape for var{}\".format(inputs.get_shape(), inputs.get_shape()[-1]))\n",
    "\n",
    "    offset = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), name='{}_batch_norm_offset'.format(layer))\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]), name='{}_batch_norm_scale'.format(layer))\n",
    "    epsilon = 1e-5\n",
    "    alpha = 0.9  # use numbers closer to 1 if you have more data\n",
    "\n",
    "    def batch_norm():\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs, [0])\n",
    "        print(\"batch mean {}, var {}\".format(batch_mean.shape, batch_var.shape))\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                                pop_mean * alpha + batch_mean * (1 - alpha))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * alpha + batch_var * (1 - alpha))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs, mean=batch_mean, variance=batch_var, offset=offset, scale=scale,\n",
    "                                              variance_epsilon=epsilon)\n",
    "\n",
    "    def pop_norm():\n",
    "        return tf.nn.batch_normalization(inputs, pop_mean, pop_var, offset=offset, scale=scale,\n",
    "                                          variance_epsilon=epsilon)\n",
    "\n",
    "    return tf.cond(is_training, batch_norm, pop_norm)\n",
    "\n",
    "\n",
    "def hidden_mlp_layers(batch_norm, hidden_dim, is_training, keep_prob, layer_input, size):\n",
    "    tmp = layer_input\n",
    "    for i in np.arange(size):\n",
    "        input_shape = tmp.get_shape().as_list()[1]\n",
    "        print(\"layer input shape:{}\".format(input_shape))\n",
    "        w_hi, b_hi = create_nn_weights('h{}_z'.format(i), 'decoder', [input_shape, hidden_dim[i]])\n",
    "        h_i = dropout_normalised_mlp(layer_input=tmp, weights=w_hi, biases=b_hi,\n",
    "                                     is_training=is_training,\n",
    "                                     batch_norm=batch_norm, keep_prob=keep_prob,\n",
    "                                     layer='h{}_z_decoder'.format(i))\n",
    "\n",
    "        tmp = h_i\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def hidden_mlp_layers_noise(batch_norm, hidden_dim, is_training, keep_prob, layer_input, noise_alpha, size,\n",
    "                            batch_size):\n",
    "    # hidden_dim = [50,50]\n",
    "    # layer input shape is (?,34) 34 = 17 + 17 (x + noise)\n",
    "    # size = len(hiiden_dim) = 2\n",
    "    tmp = layer_input\n",
    "    for i in np.arange(size):\n",
    "        input_shape = tmp.get_shape().as_list()[1] # in the second loop input_shape would be 100\n",
    "        print(\"layer input shape:{}\".format(input_shape))\n",
    "        w_hi, b_hi = create_nn_weights('h{}_z'.format(i), 'decoder', [input_shape, hidden_dim[i]])\n",
    "        h_i = dropout_normalised_mlp(layer_input=tmp, weights=w_hi, biases=b_hi,\n",
    "                                     is_training=is_training,\n",
    "                                     batch_norm=batch_norm, keep_prob=keep_prob,\n",
    "                                     layer='h{}_z_decoder'.format(i)) # h_i shape is (?,50)\n",
    "                                                                      # second loop also (?,50)\n",
    "\n",
    "        # noise = standard_gaussian(dim=hidden_dim[i], batch_size=batch_size) * tf.gather(noise_alpha, i + 1)\n",
    "        noise = uniform(dim=hidden_dim[i], batch_size=batch_size) * tf.gather(noise_alpha, i + 1) # noise shape (350 batch_size),50), the same for the second loop\n",
    "        tmp = tf.concat([h_i, noise], axis=1) # the shape become (?, 100) because 50 (h_i) and 50 (noise)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generated_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_distribution(predicted, empirical, data, time='days', cens=False):\n",
    "    predicted_samples = np.transpose(predicted)\n",
    "    print(\"observed_samples:{}, empirical_observed:{}\".format(predicted_samples.shape,\n",
    "                                                              empirical.shape))\n",
    "\n",
    "    best_samples, diff, worst_samples = get_best_worst_indices(cens, empirical, predicted_samples)\n",
    "\n",
    "    predicted_best = predicted_samples[best_samples]\n",
    "    predicted_worst = predicted_samples[worst_samples]\n",
    "    hist_plots(samples=diff, name='{}_absolute_error'.format(data), xlabel=r'|\\tilde{t}-t|')\n",
    "\n",
    "    box_plots(empirical=empirical[best_samples], predicted=list(predicted_best), name=('%s_best' % data),\n",
    "              time=time)\n",
    "    box_plots(empirical=empirical[worst_samples], predicted=list(predicted_worst), name=('%s_worst' % data),\n",
    "              time=time)\n",
    "\n",
    "\n",
    "def get_best_worst_indices(cens, empirical, predicted, size=50):\n",
    "    diff = compute_relative_error(cens=cens, empirical=empirical, predicted=predicted)\n",
    "    indices = sorted(range(len(abs(diff))), key=lambda k: diff[k])\n",
    "    best_samples = indices[0:size]\n",
    "    worst_samples = indices[len(indices) - size - 1: len(indices) - 1]\n",
    "    return best_samples, diff, worst_samples\n",
    "\n",
    "\n",
    "def compute_relative_error(cens, empirical, predicted, relative=False):\n",
    "    predicted_median = np.median(predicted, axis=1)\n",
    "    if cens:\n",
    "        diff = np.minimum(0, predicted_median - empirical)\n",
    "    else:\n",
    "        diff = predicted_median - empirical\n",
    "    if relative:\n",
    "        return diff\n",
    "    else:\n",
    "        return abs(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_metrics(e, risk_set, predicted, batch_size, empirical):\n",
    "    partial_likelihood = tf.constant(0.0, shape=())\n",
    "    rel_abs_err = tf.constant(0.0, shape=())\n",
    "    total_cens_loss = tf.constant(0.0, shape=())\n",
    "    total_obs_loss = tf.constant(0.0, shape=())\n",
    "    predicted = tf.squeeze(predicted)\n",
    "    observed = tf.reduce_sum(e) # compute sum across the tensor\n",
    "    censored = tf.subtract(tf.cast(batch_size, dtype=tf.float32), observed) # what is left from the batch which is not bserved, it is then censored \n",
    "\n",
    "    def condition(i, likelihood, rae, recon_loss, obs_recon_loss):\n",
    "        return i < batch_size\n",
    "\n",
    "    def body(i, likelihood, rae, cens_recon_loss, obs_recon_loss):\n",
    "        # get edges for observation i\n",
    "        pred_t_i = tf.gather(predicted, i)\n",
    "        emp_t_i = tf.gather(empirical, i)\n",
    "        e_i = tf.gather(e, i)\n",
    "        censored = tf.equal(e_i, 0)\n",
    "        obs_at_risk = tf.gather(risk_set, i)\n",
    "        print(\"obs_at_risk:{}, g_theta:{}\".format(obs_at_risk.shape, predicted.shape))\n",
    "        risk_hazard_list = tf.multiply(predicted, obs_at_risk)\n",
    "        num_adjacent = tf.reduce_sum(obs_at_risk)\n",
    "        # calculate partial likelihood\n",
    "        risk = tf.subtract(pred_t_i, risk_hazard_list)\n",
    "        activated_risk = tf.nn.sigmoid(risk)\n",
    "        # logistic = map((lambda ele: log(1 + exp(ele * -1)) * 1 / log(2)), x)\n",
    "        constant = 1e-8\n",
    "        log_activated_risk = tf.div(tf.log(activated_risk + constant), tf.log(2.0))\n",
    "        obs_likelihood = tf.add(log_activated_risk, num_adjacent)\n",
    "        uncensored_likelihood = tf.cond(censored, lambda: tf.constant(0.0), lambda: obs_likelihood)\n",
    "        cumulative_likelihood = tf.reduce_sum(uncensored_likelihood)\n",
    "        updated_likelihood = tf.add(cumulative_likelihood, likelihood)\n",
    "\n",
    "        # RElative absolute error\n",
    "        abs_error_i = tf.abs(tf.subtract(pred_t_i, emp_t_i))\n",
    "        pred_great_empirical = tf.greater(pred_t_i, emp_t_i)\n",
    "        min_rea_i = tf.minimum(tf.div(abs_error_i, pred_t_i), tf.constant(1.0))\n",
    "        rea_i = tf.cond(tf.logical_and(censored, pred_great_empirical), lambda: tf.constant(0.0), lambda: min_rea_i)\n",
    "        cumulative_rae = tf.add(rea_i, rae)\n",
    "\n",
    "        # Censored generated t loss\n",
    "        diff_time = tf.subtract(pred_t_i, emp_t_i)\n",
    "        # logistic = map((lambda ele: log(1 + exp(ele * -1)) * 1 / log(2)), x)\n",
    "        # logistic = tf.div(tf.nn.sigmoid(diff_time) + constant, tf.log(2.0))\n",
    "        # hinge = map(lambda ele: max(0, 1 - ele), x)\n",
    "        hinge = tf.nn.relu(1.0 - diff_time)\n",
    "        censored_loss_i = tf.cond(censored, lambda: hinge, lambda: tf.constant(0.0))\n",
    "        # Sum over all edges and normalize by number of edges\n",
    "        # L1 recon\n",
    "        observed_loss_i = tf.cond(censored, lambda: tf.constant(0.0),\n",
    "                                  lambda: tf.losses.absolute_difference(labels=emp_t_i, predictions=pred_t_i))\n",
    "        # add observation risk to total risk\n",
    "        cum_cens_loss = tf.add(cens_recon_loss, censored_loss_i)\n",
    "        cum_obs_loss = tf.add(obs_recon_loss, observed_loss_i)\n",
    "        return [i + 1, tf.reshape(updated_likelihood, shape=()), tf.reshape(cumulative_rae, shape=()),\n",
    "                tf.reshape(cum_cens_loss, shape=()), tf.reshape(cum_obs_loss, shape=())]\n",
    "\n",
    "    # Relevant Functions\n",
    "    idx = tf.constant(0, shape=())\n",
    "    _, total_likelihood, total_rel_abs_err, batch_cens_loss, batch_obs_loss = \\\n",
    "        tf.while_loop(condition, body,\n",
    "                      loop_vars=[idx,\n",
    "                                 partial_likelihood,\n",
    "                                 rel_abs_err,\n",
    "                                 total_cens_loss,\n",
    "                                 total_obs_loss],\n",
    "                      shape_invariants=[\n",
    "                          idx.get_shape(),\n",
    "                          partial_likelihood.get_shape(),\n",
    "                          rel_abs_err.get_shape(),\n",
    "                          total_cens_loss.get_shape(),\n",
    "                          total_obs_loss.get_shape()])\n",
    "    square_batch_size = tf.pow(batch_size, tf.constant(2))\n",
    "\n",
    "    def normarlize_loss(cost, size):\n",
    "        cast_size = tf.cast(size, dtype=tf.float32)\n",
    "        norm = tf.cond(tf.greater(cast_size, tf.constant(0.0)), lambda: tf.div(cost, cast_size), lambda: 0.0)\n",
    "        return norm\n",
    "\n",
    "    total_recon_loss = tf.add(normarlize_loss(batch_cens_loss, size=censored),\n",
    "                              normarlize_loss(batch_obs_loss, size=observed))\n",
    "    normalized_log_likelihood = normarlize_loss(total_likelihood, size=square_batch_size)\n",
    "    return normalized_log_likelihood, normarlize_loss(total_rel_abs_err, size=batch_size), total_recon_loss\n",
    "\n",
    "\n",
    "def l2_loss(scale):\n",
    "    l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "    return l2 * scale\n",
    "\n",
    "\n",
    "def l1_loss(scale):\n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "        scale=scale, scope=None\n",
    "    )\n",
    "    weights = tf.trainable_variables()  # all vars of your graph\n",
    "    l1 = tf.contrib.layers.apply_regularization(l1_regularizer, weights)\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of data:   id  time  duration  orig_time  first_time  mat_time  balance_time  \\\n",
      "0   1    48        24         -7          25       113      29087.21   \n",
      "1   2    26         2         18          25       138     105654.77   \n",
      "2   3    29         5         -6          25       114      44378.60   \n",
      "3   4    60        36         -2          25       119      52686.35   \n",
      "4   5    27         3         18          25       138      52100.71   \n",
      "\n",
      "    LTV_time  interest_rate_time  hpi_time  ...  REtype_SF_orig_time  \\\n",
      "0  26.658065               9.200    146.45  ...                    1   \n",
      "1  65.469851               7.680    225.10  ...                    1   \n",
      "2  31.459735              11.375    217.37  ...                    1   \n",
      "3  34.898842              10.500    189.82  ...                    1   \n",
      "4  66.346343               9.155    222.39  ...                    1   \n",
      "\n",
      "   investor_orig_time  balance_orig_time  FICO_orig_time  LTV_orig_time  \\\n",
      "0                   0            45000.0             715           69.4   \n",
      "1                   0           107200.0             558           80.0   \n",
      "2                   0            48600.0             680           83.6   \n",
      "3                   0            63750.0             587           81.8   \n",
      "4                   0            52800.0             527           80.0   \n",
      "\n",
      "   Interest_Rate_orig_time  hpi_orig_time  default_time  payoff_time  \\\n",
      "0                    9.200          87.03             1            0   \n",
      "1                    7.680         186.91             0            1   \n",
      "2                    8.750          89.58             0            1   \n",
      "3                   10.500          97.99             0            0   \n",
      "4                    9.155         186.91             0            1   \n",
      "\n",
      "   status_time  \n",
      "0            1  \n",
      "1            2  \n",
      "2            2  \n",
      "3            0  \n",
      "4            2  \n",
      "\n",
      "[5 rows x 24 columns], data shape:(49982, 24)\n",
      "missing:0.0\n",
      "head of dataset data:   orig_time  mat_time  balance_time  LTV_time  interest_rate_time  hpi_time  \\\n",
      "0  -3.477627 -1.389979     -1.033823 -2.095055            0.988138 -1.667711   \n",
      "1  -0.303878  0.003805     -0.656808 -0.550770            0.301260  1.324293   \n",
      "2  -3.350677 -1.334228     -0.958529 -1.904001            1.971006  1.030228   \n",
      "3  -2.842878 -1.055471     -0.917622 -1.767162            1.575599 -0.017829   \n",
      "4  -0.303878  0.003805     -0.920506 -0.515895            0.967802  1.221199   \n",
      "\n",
      "   gdp_time  uer_time  balance_orig_time  FICO_orig_time  LTV_orig_time  \\\n",
      "0  0.534808  1.372840          -0.991474        0.737401      -1.039149   \n",
      "1  0.240467 -0.776909          -0.693650       -1.419995       0.029385   \n",
      "2  0.001466 -0.896339          -0.974236        0.256453       0.392283   \n",
      "3  0.597611 -0.179756          -0.901696       -1.021495       0.210834   \n",
      "4  0.350143 -0.956054          -0.954126       -1.845978       0.029385   \n",
      "\n",
      "   Interest_Rate_orig_time  hpi_orig_time  REtype_CO_orig_time  \\\n",
      "0                 1.143066      -3.185900                    0   \n",
      "1                 0.680615      -0.276286                    0   \n",
      "2                 1.006156      -3.111615                    0   \n",
      "3                 1.538583      -2.866623                    0   \n",
      "4                 1.129375      -0.276286                    0   \n",
      "\n",
      "   REtype_PU_orig_time  REtype_SF_orig_time  investor_orig_time  \n",
      "0                    0                    1                   0  \n",
      "1                    0                    1                   0  \n",
      "2                    0                    1                   0  \n",
      "3                    0                    1                   0  \n",
      "4                    0                    1                   0  , data shape:(49982, 17)\n",
      "data description:          orig_time      mat_time  balance_time      LTV_time  \\\n",
      "count  4.998200e+04  4.998200e+04  4.998200e+04  4.998200e+04   \n",
      "mean  -2.047100e-16 -2.536129e-16 -5.572661e-17 -2.115337e-16   \n",
      "std    1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
      "min   -7.666977e+00 -6.686357e+00 -1.177047e+00 -3.155755e+00   \n",
      "25%   -4.308278e-01 -1.076980e-01 -6.554586e-01 -6.046198e-01   \n",
      "50%    2.039221e-01  1.710588e-01 -2.727104e-01 -2.360233e-02   \n",
      "75%    5.847720e-01  3.940642e-01  3.966436e-01  6.538653e-01   \n",
      "max    5.028021e+00  5.077178e+00  4.160276e+01  2.139352e+01   \n",
      "\n",
      "       interest_rate_time      hpi_time      gdp_time      uer_time  \\\n",
      "count        4.998200e+04  4.998200e+04  4.998200e+04  4.998200e+04   \n",
      "mean        -2.729467e-17  8.609193e-16 -2.729467e-17 -2.881222e-16   \n",
      "std          1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
      "min         -3.169282e+00 -3.136894e+00 -3.043250e+00 -1.314346e+00   \n",
      "25%         -5.144079e-01 -1.072734e+00 -2.403500e-01 -7.769087e-01   \n",
      "50%         -6.027861e-03 -1.782909e-02  3.045201e-01 -2.991867e-01   \n",
      "75%          6.040282e-01  1.030228e+00  5.976112e-01  2.979657e-01   \n",
      "max          1.377672e+01  1.369563e+00  1.794764e+00  2.387999e+00   \n",
      "\n",
      "       balance_orig_time  FICO_orig_time  LTV_orig_time  \\\n",
      "count       4.998200e+04    4.998200e+04   4.998200e+04   \n",
      "mean       -2.445147e-17    4.139691e-16  -6.255028e-17   \n",
      "std         1.000010e+00    1.000010e+00   1.000010e+00   \n",
      "min        -1.183999e+00   -3.591133e+00  -2.984686e+00   \n",
      "25%        -6.610904e-01   -6.779607e-01  -4.746403e-01   \n",
      "50%        -2.876136e-01    2.284970e-02   2.938497e-02   \n",
      "75%         3.970938e-01    7.374014e-01   5.334103e-01   \n",
      "max         3.709837e+01    2.455074e+00   1.399089e+01   \n",
      "\n",
      "       Interest_Rate_orig_time  hpi_orig_time  REtype_CO_orig_time  \\\n",
      "count             4.998200e+04   4.998200e+04         49982.000000   \n",
      "mean             -2.365538e-16  -2.604366e-16             0.064983   \n",
      "std               1.000010e+00   1.000010e+00             0.246499   \n",
      "min              -1.655980e+00  -3.515664e+00             0.000000   \n",
      "25%              -7.432477e-01  -4.936038e-01             0.000000   \n",
      "50%               2.835761e-01   4.758790e-01             0.000000   \n",
      "75%               6.638812e-01   7.572854e-01             0.000000   \n",
      "max               4.352841e+00   8.708967e-01             1.000000   \n",
      "\n",
      "       REtype_PU_orig_time  REtype_SF_orig_time  investor_orig_time  \n",
      "count         49982.000000         49982.000000        49982.000000  \n",
      "mean              0.115902             0.623625            0.118443  \n",
      "std               0.320110             0.484481            0.323135  \n",
      "min               0.000000             0.000000            0.000000  \n",
      "25%               0.000000             0.000000            0.000000  \n",
      "50%               0.000000             1.000000            0.000000  \n",
      "75%               0.000000             1.000000            0.000000  \n",
      "max               1.000000             1.000000            1.000000  \n",
      "columns:['orig_time' 'mat_time' 'balance_time' 'LTV_time' 'interest_rate_time'\n",
      " 'hpi_time' 'gdp_time' 'uer_time' 'balance_orig_time' 'FICO_orig_time'\n",
      " 'LTV_orig_time' 'Interest_Rate_orig_time' 'hpi_orig_time'\n",
      " 'REtype_CO_orig_time' 'REtype_PU_orig_time' 'REtype_SF_orig_time'\n",
      " 'investor_orig_time']\n",
      "x:[-3.47762744 -1.38997903 -1.03382322 -2.09505531  0.9881376  -1.66771114\n",
      "  0.5348077   1.37284007 -0.99147372  0.73740143 -1.03914866  1.14306563\n",
      " -3.18589953  0.          0.          1.          0.        ], t:24, e:0, len:49982\n",
      "x_shape:(49982, 17)\n",
      "end_time:60\n",
      "observed percent:0.5318314593253571\n",
      "shuffled x:[ 0.71172201  0.33831283 -0.21521574  0.10943108  0.16343216  1.03022833\n",
      "  0.00146605 -0.89633915 -0.26271517  1.43821178  0.02938497  0.58782017\n",
      "  0.83623068  0.          0.          1.          1.        ], t:4, e:1, len:49982\n",
      "num_examples:39985\n",
      "test:9997, valid:7997, train:39985, all: 57979\n",
      "categorical_flat:[13, 14, 15, 16]\n",
      "len covariates:17, categorical:4\n",
      "imputation_values:[0.20392207819158592, 0.17105877772832692, -0.2710399070327385, -0.023597524056526372, -0.006027861145139642, -0.017829093326131175, 0.3045201165680782, -0.29918672172861266, -0.28681878460652066, 0.022849696519850204, 0.029384971240994304, 0.28357607893547515, 0.47587904145774995, 0.0, 0.0, 1.0, 0.0]\n",
      "imputation_values:[0.20392207819158592, 0.17105877772832692, -0.2710399070327385, -0.023597524056526372, -0.006027861145139642, -0.017829093326131175, 0.3045201165680782, -0.29918672172861266, -0.28681878460652066, 0.022849696519850204, 0.029384971240994304, 0.28357607893547515, 0.47587904145774995, 0.0, 0.0, 1.0, 0.0]\n",
      "observed fold:0.533199949981243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed fold:0.526257877363209\n",
      "observed fold:0.5333249968738277\n"
     ]
    }
   ],
   "source": [
    "def generate_data():\n",
    "    import pdb\n",
    "    np.random.seed(31415)\n",
    "    data_frame = pandas.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\datasets\\\\mortgage\\\\WideFormatMortgageAfterRemovingNull.csv', sep=',')\n",
    "    print(\"head of data:{}, data shape:{}\".format(data_frame.head(), data_frame.shape))\n",
    "    # x_data = data_frame[['age', 'sex', 'kappa', 'lambda', 'flc.grp', 'creatinine', 'mgus']]\n",
    "    # Preprocess\n",
    "    to_drop = ['id','time', 'duration','first_time', 'default_time', 'payoff_time', 'status_time']\n",
    "    print(\"missing:{}\".format(missing_proportion(data_frame.drop(labels=to_drop, axis=1))))\n",
    "    one_hot_encoder_list = ['REtype_CO_orig_time', 'REtype_PU_orig_time', 'REtype_SF_orig_time', 'investor_orig_time']\n",
    "    #data_frame = one_hot_encoder(data_frame, encode=one_hot_encoder_list)\n",
    "    data_frame = data_frame[['id', 'time', 'duration', 'orig_time', 'first_time', 'mat_time',\n",
    "           'balance_time', 'LTV_time', 'interest_rate_time', 'hpi_time',\n",
    "           'gdp_time', 'uer_time', 'balance_orig_time',\n",
    "           'FICO_orig_time', 'LTV_orig_time', 'Interest_Rate_orig_time',\n",
    "           'hpi_orig_time', 'default_time', 'payoff_time', 'status_time', 'REtype_CO_orig_time', 'REtype_PU_orig_time',\n",
    "           'REtype_SF_orig_time', 'investor_orig_time']]\n",
    "    t_data = data_frame[['duration']]\n",
    "    e_data = data_frame[['payoff_time']]\n",
    "    dataset1 = data_frame.drop(labels=to_drop, axis=1)\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    ll_n = f_get_Normalization(np.asarray(dataset1.iloc[:,:13]), 'standard')\n",
    "    ll_p = pandas.DataFrame(ll_n, columns=dataset1.iloc[:,:13].columns)\n",
    "    ll = pandas.concat([ll_p, dataset1.iloc[:,13:].reindex(ll_p.index)], axis=1)\n",
    "    dataset = ll\n",
    "\n",
    "    print(\"head of dataset data:{}, data shape:{}\".format(dataset.head(), dataset.shape))\n",
    "    encoded_indices = [[13], [14], [15], [16]]\n",
    "    print(\"data description:{}\".format(dataset.describe()))\n",
    "    covariates = np.array(dataset.columns.values)\n",
    "    print(\"columns:{}\".format(covariates))\n",
    "    x = np.array(dataset).reshape(dataset.shape)\n",
    "    t = np.array(t_data).reshape(len(t_data))\n",
    "    e = np.array(e_data).reshape(len(e_data))\n",
    "\n",
    "    print(\"x:{}, t:{}, e:{}, len:{}\".format(x[0], t[0], e[0], len(t)))\n",
    "    idx = np.arange(0, x.shape[0])\n",
    "    print(\"x_shape:{}\".format(x.shape))\n",
    "\n",
    "    np.random.shuffle(idx)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "    e = e[idx]\n",
    "    end_time = max(t)\n",
    "    print(\"end_time:{}\".format(end_time))\n",
    "    print(\"observed percent:{}\".format(sum(e) / len(e)))\n",
    "    print(\"shuffled x:{}, t:{}, e:{}, len:{}\".format(x[0], t[0], e[0], len(t)))\n",
    "\n",
    "    num_examples = int(0.80 * len(e))\n",
    "    print(\"num_examples:{}\".format(num_examples))\n",
    "    vali_example = int(0.20 * num_examples)\n",
    "    train_idx = idx[0: num_examples - vali_example]\n",
    "    valid_idx = idx[num_examples - vali_example: num_examples]\n",
    "    split = int(len(t) - num_examples)\n",
    "    test_idx = idx[num_examples: num_examples + split]\n",
    "\n",
    "    print(\"test:{}, valid:{}, train:{}, all: {}\".format(len(test_idx), len(valid_idx), num_examples,\n",
    "                                                        len(test_idx) + len(valid_idx) + num_examples))\n",
    "    # print(\"test_idx:{}, valid_idx:{},train_idx:{} \".format(test_idx, valid_idx, train_idx))\n",
    "\n",
    "    imputation_values = get_train_median_mode(x=np.array(x[train_idx]), categorial=encoded_indices)\n",
    "    print(\"imputation_values:{}\".format(imputation_values))\n",
    "    preprocessed = {\n",
    "        'train': formatted_data(x=x, t=t, e=e, idx=train_idx),\n",
    "        'test': formatted_data(x=x, t=t, e=e, idx=test_idx),\n",
    "        'valid': formatted_data(x=x, t=t, e=e, idx=valid_idx),\n",
    "        'end_t': end_time,\n",
    "        'covariates': covariates,\n",
    "        'one_hot_indices': encoded_indices,\n",
    "        'imputation_values': imputation_values\n",
    "    }\n",
    "    return preprocessed\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. risk_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raiber: it return the network output after applying the relu fuction to it\n",
    "def pt_given_x(x, hidden_dim, is_training, batch_norm, batch_size, input_dim, noise_alpha, keep_prob=0.9, reuse=False):\n",
    "    size = len(hidden_dim)\n",
    "    with tf.variable_scope('generate_t_given_x', reuse=reuse):\n",
    "        # Variables\n",
    "        # first we add the noise to the input, then using the function hidden_mlp_layers_noise we add the noise to each hidden layer \n",
    "        noise = uniform(dim=input_dim, batch_size=batch_size) * tf.gather(noise_alpha, 0) # tf.gather give us the value of noise_alpha inside the index 0\n",
    "        x_plus_noise = tf.concat([x, noise], axis=1) # the layer input shape would be 17 + 17 = 34 (17 is the number of features)\n",
    "        hidden_x = hidden_mlp_layers_noise(batch_norm=batch_norm, hidden_dim=hidden_dim,\n",
    "                                           is_training=is_training, keep_prob=keep_prob,\n",
    "                                           layer_input=x_plus_noise, size=size, batch_size=batch_size,\n",
    "                                           noise_alpha=noise_alpha) # hidden_x shape (?,100)\n",
    "\n",
    "        w_t, b_t = create_nn_weights('t', 'encoder', [hidden_x.get_shape().as_list()[1], 1])\n",
    "        # name:W_encoder_t, shape[100, 1]\n",
    "        #name:b_encoder_t, shape[1]\n",
    "        t_mu = mlp_neuron(hidden_x, w_t, b_t, activation=False) # mlp = tf.add(tf.matmul(layer_input, weights), biases)\n",
    "        # no activation is applied \n",
    "        logit = tf.nn.sigmoid(t_mu) \n",
    "        return tf.exp(t_mu), logit\n",
    "\n",
    "def discriminator(pair_one, pair_two, hidden_dim, is_training, batch_norm, scope, keep_prob=1, reuse=False):\n",
    "    size = len(hidden_dim)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Variables\n",
    "        print(\"scope:{}, pair_one:{}, pair_two:{}\".format(scope, pair_one.shape, pair_two.shape))\n",
    "        # create one structure for the input feature \n",
    "        hidden_pair_one = hidden_mlp_layers(batch_norm=batch_norm, hidden_dim=hidden_dim,\n",
    "                                            is_training=is_training, keep_prob=keep_prob,\n",
    "                                            layer_input=pair_one, size=size) # shape=(?, 50)\n",
    "\n",
    "        # this structure is for the time \n",
    "        hidden_pair_two = hidden_mlp_layers(batch_norm=batch_norm, hidden_dim=hidden_dim,\n",
    "                                            is_training=is_training, keep_prob=keep_prob,\n",
    "                                            layer_input=pair_two, size=size) #shape=(?, 50)\n",
    "        hidden_pairs = tf.concat([hidden_pair_one, hidden_pair_two], axis=1) #shape=(?, 100)\n",
    "        print(\"hidden_pairs:{}\".format(hidden_pairs.get_shape()))\n",
    "        #name:W_discriminator_logits, shape[100, 1]\n",
    "        #name:b_discriminator_logits, shape[1]\n",
    "        w_logit, b_logit = create_nn_weights('logits', 'discriminator', [hidden_dim[size - 1] * 2, 1])\n",
    "        f = mlp_neuron(layer_input=hidden_pairs, weights=w_logit, biases=b_logit, activation=False) \n",
    "        logit = tf.nn.sigmoid(f)\n",
    "\n",
    "    return tf.squeeze(logit), tf.squeeze(f)\n",
    "\n",
    "def discriminator_one(pair_one, pair_two, hidden_dim, is_training, batch_norm, keep_prob=1, reuse=False):\n",
    "    score, f = discriminator(pair_one=pair_one, pair_two=pair_two, scope='Discriminator_one', batch_norm=batch_norm,\n",
    "                             is_training=is_training,\n",
    "                             keep_prob=keep_prob, reuse=reuse, hidden_dim=hidden_dim)\n",
    "    return score, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. DATE-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATE_AE(object):\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 learning_rate,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 require_improvement,\n",
    "                 seed,\n",
    "                 num_iterations,\n",
    "                 hidden_dim,\n",
    "                 latent_dim,\n",
    "                 input_dim,\n",
    "                 num_examples,\n",
    "                 keep_prob,\n",
    "                 train_data,\n",
    "                 valid_data,\n",
    "                 test_data,\n",
    "                 end_t,\n",
    "                 gen_updates,\n",
    "                 covariates,\n",
    "                 imputation_values,\n",
    "                 sample_size,\n",
    "                 disc_updates,\n",
    "                 categorical_indices,\n",
    "                 l2_reg,\n",
    "                 max_epochs,\n",
    "                 path_large_data=\"\"\n",
    "                 ):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.disc_updates = disc_updates\n",
    "        self.gen_updates = gen_updates\n",
    "        self.latent_dim = latent_dim\n",
    "        self.path_large_data = path_large_data\n",
    "        self.seed = seed\n",
    "        self.require_improvement = require_improvement\n",
    "        self.num_iterations = num_iterations\n",
    "        self.learning_rate, self.beta1, self.beta2 = learning_rate, beta1, beta2\n",
    "        self.l2_reg = l2_reg\n",
    "        self.log_file = 'model.log'\n",
    "        logging.basicConfig(filename=self.log_file, filemode='w', level=logging.DEBUG)\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        self.batch_norm = True\n",
    "        self.covariates = covariates\n",
    "        self.sample_size = sample_size\n",
    "        self.z_sample_size = 10  # num of z_samples\n",
    "\n",
    "        self.config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        # self.config.gpu_options.per_process_gpu_memory_fraction = gpu_memory_fraction\n",
    "        # Load Data\n",
    "        self.train_x, self.train_t, self.train_e = train_data['x'], train_data['t'], train_data['e']\n",
    "        self.valid_x, self.valid_t, self.valid_e = valid_data['x'], valid_data['t'], valid_data['e']\n",
    "\n",
    "        self.test_x, self.test_t, self.test_e = test_data['x'], test_data['t'], test_data['e']\n",
    "        self.end_t = end_t\n",
    "        self.keep_prob = keep_prob\n",
    "        self.input_dim = input_dim\n",
    "        self.imputation_values = imputation_values\n",
    "        self.imputation_values = np.zeros(shape=self.input_dim)\n",
    "        self.num_examples = num_examples\n",
    "        self.categorical_indices = categorical_indices\n",
    "        self.continuous_indices = np.setdiff1d(np.arange(input_dim), flatten_nested(categorical_indices))\n",
    "        print_features = \"input_dim:{}, continuous:{}, size:{}, categorical:{}, \" \\\n",
    "                         \"size{}\".format(self.input_dim,\n",
    "                                         self.continuous_indices,\n",
    "                                         len(\n",
    "                                             self.continuous_indices),\n",
    "                                         self.categorical_indices,\n",
    "                                         len(\n",
    "                                             self.categorical_indices))\n",
    "        print(print_features)\n",
    "        logging.debug(print_features)\n",
    "        print_model = \"model is DATE_AE\"\n",
    "        print(print_model)\n",
    "        logging.debug(\"Imputation values:{}\".format(imputation_values))\n",
    "        logging.debug(print_model)\n",
    "        self.model = 'DATE_AE'\n",
    "\n",
    "        self._build_graph()\n",
    "        self.train_cost, self.train_ci, self.train_t_rae, self.train_gen, self.train_disc, self.train_ranking, \\\n",
    "        self.train_layer_one_recon = [], [], [], [], [], [], []\n",
    "        self.valid_cost, self.valid_ci, self.valid_t_rae, self.valid_gen, self.valid_disc, self.valid_ranking, \\\n",
    "        self.valid_layer_one_recon = [], [], [], [], [], [], []\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self.G = tf.Graph()\n",
    "        with self.G.as_default():\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, self.input_dim], name='x')\n",
    "            self.e = tf.placeholder(tf.float32, shape=[None], name='e')\n",
    "            self.t = tf.placeholder(tf.float32, shape=[None], name='t')\n",
    "            self.t_lab = tf.placeholder(tf.float32, shape=[None], name='t_lab')\n",
    "            # are used to feed data into our queue\n",
    "            self.batch_size_tensor = tf.placeholder(tf.int32, shape=[], name='batch_size')\n",
    "            self.risk_set = tf.placeholder(tf.float32, shape=[None, None])\n",
    "            self.impute_mask = tf.placeholder(tf.float32, shape=[None, self.input_dim], name='impute_mask')\n",
    "            self.is_training = tf.placeholder(tf.bool)\n",
    "            self.noise_dim = len(self.hidden_dim) + 1\n",
    "            self.noise_alpha = tf.placeholder(tf.float32, shape=[self.noise_dim])\n",
    "\n",
    "            self._objective()\n",
    "            self.session = tf.Session(config=self.config)\n",
    "\n",
    "            self.capacity = 1400\n",
    "            self.coord = tf.train.Coordinator()\n",
    "            enqueue_thread = threading.Thread(target=self.enqueue)\n",
    "            self.queue = tf.RandomShuffleQueue(capacity=self.capacity, dtypes=[tf.float32, tf.float32, tf.float32],\n",
    "                                               shapes=[[self.input_dim], [], []], min_after_dequeue=self.batch_size)\n",
    "            # self.queue = tf.FIFOQueue(capacity=self.capacity, dtypes=[tf.float32, tf.float32, tf.float32],\n",
    "            #                           shapes=[[self.input_dim], [], []])\n",
    "            self.enqueue_op = self.queue.enqueue_many([self.x, self.t, self.e])\n",
    "            # enqueue_thread.isDaemon()\n",
    "            enqueue_thread.start()\n",
    "            dequeue_op = self.queue.dequeue()\n",
    "            self.x_batch, self.t_batch, self.e_batch = tf.train.batch(dequeue_op, batch_size=self.batch_size,\n",
    "                                                                      capacity=self.capacity)\n",
    "            self.threads = tf.train.start_queue_runners(coord=self.coord, sess=self.session)\n",
    "\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.current_dir = os.getcwd()\n",
    "            self.save_path = \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\summaries\\\\mort_p\\\\{0}_model\".format(self.model)\n",
    "            self.train_writer = tf.summary.FileWriter(self.save_path, self.session.graph)\n",
    "\n",
    "    def _objective(self):\n",
    "        self.num_batches = self.num_examples / self.batch_size\n",
    "        logging.debug(\"num batches:{}, batch_size:{} epochs:{}\".format(self.num_batches, self.batch_size,\n",
    "                                                                       int(self.num_iterations / self.num_batches)))\n",
    "        self._build_model()\n",
    "        self.reg_loss = l2_loss(self.l2_reg) + l1_loss(self.l2_reg)\n",
    "        self.cost = self.t_regularization_loss + self.disc_one_loss + self.disc_two_loss + self.gen_one_loss + \\\n",
    "                    self.gen_two_loss + self.layer_one_recon\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1,\n",
    "                                           beta2=self.beta2)\n",
    "\n",
    "        dvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Discriminator_one\")\n",
    "        dvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Discriminator_two\")\n",
    "        genvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generate_t_given_x\")\n",
    "        genvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generate_t_given_z\")\n",
    "\n",
    "        self.disc_solver = optimizer.minimize(self.disc_one_loss + self.disc_two_loss, var_list=dvars1 + dvars2)\n",
    "        self.gen_solver = optimizer.minimize(\n",
    "            self.gen_one_loss + self.gen_two_loss + self.t_regularization_loss + self.layer_one_recon,\n",
    "            var_list=genvars1 + genvars2)\n",
    "\n",
    "    def _build_model(self):\n",
    "        self._denoising_date()\n",
    "        self._risk_date()\n",
    "\n",
    "    @staticmethod\n",
    "    def log(x):\n",
    "        return tf.log(x + 1e-8)\n",
    "\n",
    "    def _risk_date(self):\n",
    "        def expand_t_dim(t):\n",
    "            return tf.expand_dims(t, axis=1)\n",
    "\n",
    "        indices_lab = tf.where(tf.equal(tf.constant(1.0, dtype=tf.float32), self.e))\n",
    "        z_lab = tf.squeeze(tf.gather(self.z_real, indices_lab), axis=[1])\n",
    "        t_lab_exp = expand_t_dim(self.t_lab)\n",
    "\n",
    "        t_gen = pt_given_z(z=self.z_real, hidden_dim=self.hidden_dim, is_training=self.is_training,\n",
    "                           batch_norm=self.batch_norm, keep_prob=self.keep_prob, batch_size=self.batch_size_tensor,\n",
    "                           latent_dim=self.latent_dim, noise_alpha=self.noise_alpha)\n",
    "\n",
    "        # Discriminator B\n",
    "        d_two_real, f_two_real = discriminator_two(pair_one=z_lab, pair_two=t_lab_exp, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   keep_prob=self.keep_prob)  # (z_nc, t_nc)\n",
    "        d_two_fake, f_two_fake = discriminator_two(pair_one=self.z_real, pair_two=t_gen, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   reuse=True, keep_prob=self.keep_prob)  # (z, t_gen)\n",
    "\n",
    "        # Discriminator loss\n",
    "        self.disc_two_loss = -tf.reduce_mean(self.log(d_two_real)) - tf.reduce_mean(self.log(1 - d_two_fake))\n",
    "\n",
    "        # Generator loss\n",
    "        self.gen_two_loss = tf.reduce_mean(f_two_real) - tf.reduce_mean(f_two_fake)\n",
    "        self.disc_logit = d_two_fake # added b raiber \n",
    "        self.disc_f = f_two_fake # added by raiber \n",
    "        self.predicted_time = tf.squeeze(t_gen)\n",
    "        self.ranking_partial_lik, self.total_rae, self.total_t_recon_loss = \\\n",
    "            batch_metrics(e=self.e,\n",
    "                          risk_set=self.risk_set,\n",
    "                          predicted=self.predicted_time,\n",
    "                          batch_size=self.batch_size_tensor,\n",
    "                          empirical=self.t)\n",
    "\n",
    "        # self.t_regularization_loss = tf.add(self.ranking_partial_lik, self.total_t_recon_loss)\n",
    "        self.t_regularization_loss = self.total_t_recon_loss\n",
    "        self.t_mse = tf.losses.mean_squared_error(labels=self.t_lab,\n",
    "                                                  predictions=tf.gather(self.predicted_time, indices_lab))\n",
    "\n",
    "    def _denoising_date(self):\n",
    "        self.z_real = generate_z_given_x(latent_dim=self.latent_dim,\n",
    "                                         is_training=self.is_training,\n",
    "                                         batch_norm=self.batch_norm,\n",
    "                                         input_dim=self.input_dim, batch_size=self.batch_size_tensor,\n",
    "                                         hidden_dim=self.hidden_dim, x=self.impute_mask, keep_prob=self.keep_prob,\n",
    "                                         reuse=True, sample_size=self.z_sample_size)\n",
    "\n",
    "        z_ones = np.ones(shape=self.latent_dim, dtype=np.float32)\n",
    "        print(\"z_ones:{}\".format(z_ones.shape))\n",
    "\n",
    "        z_fake = tf.distributions.Uniform(low=-z_ones, high=z_ones).sample(sample_shape=[self.batch_size_tensor])\n",
    "        x_fake = generate_x_given_z(z=z_fake, latent_dim=self.latent_dim,\n",
    "                                    is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                    hidden_dim=self.hidden_dim, keep_prob=self.keep_prob,\n",
    "                                    batch_size=self.batch_size_tensor, input_dim=self.input_dim)\n",
    "\n",
    "        self.x_recon = generate_x_given_z(z=self.z_real, latent_dim=self.latent_dim,\n",
    "                                          is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                          hidden_dim=self.hidden_dim, reuse=True, keep_prob=self.keep_prob,\n",
    "                                          batch_size=self.batch_size_tensor, input_dim=self.input_dim)\n",
    "\n",
    "        z_rec = generate_z_given_x(x=x_fake, latent_dim=self.latent_dim,\n",
    "                                   is_training=self.is_training,\n",
    "                                   batch_norm=self.batch_norm,\n",
    "                                   input_dim=self.input_dim, batch_size=self.batch_size_tensor,\n",
    "                                   hidden_dim=self.hidden_dim, reuse=True, keep_prob=self.keep_prob,\n",
    "                                   sample_size=self.z_sample_size)\n",
    "        # Reconstruction Loss\n",
    "\n",
    "        self.x_recon_loss = x_reconstruction(x_recon=self.x_recon, x=self.x,\n",
    "                                             categorical_indices=self.categorical_indices,\n",
    "                                             continuous_indices=self.continuous_indices,\n",
    "                                             batch_size=self.batch_size_tensor)\n",
    "\n",
    "        self.z_recon_loss = tf.losses.mean_squared_error(z_fake, z_rec)\n",
    "        self.layer_one_recon = tf.add(self.x_recon_loss, self.z_recon_loss)\n",
    "\n",
    "        d_one_real, f_one_real = discriminator_one(pair_one=self.impute_mask, pair_two=self.z_real,\n",
    "                                                   hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   keep_prob=self.keep_prob)  # real\n",
    "        d_one_fake, f_one_fake = discriminator_one(pair_one=x_fake, pair_two=z_fake, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   reuse=True, keep_prob=self.keep_prob)  # fake\n",
    "\n",
    "        self.disc_one_loss = -tf.reduce_mean(self.log(d_one_real)) - tf.reduce_mean(self.log(1 - d_one_fake))\n",
    "\n",
    "        # Generator loss\n",
    "        self.gen_one_loss = tf.reduce_mean(f_one_real) - tf.reduce_mean(f_one_fake)\n",
    "\n",
    "    def predict_concordance_index(self, x, t, e, outcomes=None):\n",
    "        input_size = x.shape[0]\n",
    "        i = 0\n",
    "        num_batches = input_size / self.batch_size\n",
    "        predicted_time = np.zeros(shape=input_size, dtype=np.int)\n",
    "        total_ranking = 0.0\n",
    "        total_rae = 0.0\n",
    "        total_cost = 0.0\n",
    "        total_gen_loss = 0.0\n",
    "        total_disc_loss = 0.0\n",
    "        total_layer_one_recon = 0.0\n",
    "        total_t_reg_loss = 0.0\n",
    "        total_reg = 0.0\n",
    "        total_mse = 0.0\n",
    "        while i < input_size:\n",
    "            # The ending index for the next batch is denoted j.\n",
    "            j = min(i + self.batch_size, input_size)\n",
    "            feed_dict = self.batch_feed_dict(e=e, i=i, j=j, t=t, x=x, outcomes=outcomes)\n",
    "            cost, ranking, gen_loss, rae, reg, disc_loss, layer_one_recon, t_reg_loss, t_mse = self.session.run(\n",
    "                [self.cost, self.ranking_partial_lik, self.gen_one_loss, self.total_rae,\n",
    "                 self.reg_loss,\n",
    "                 self.disc_one_loss, self.layer_one_recon, self.t_regularization_loss, self.t_mse],\n",
    "                feed_dict=feed_dict)\n",
    "\n",
    "            temp_pred_time = []\n",
    "            for p in range(self.sample_size):\n",
    "                gen_time = self.session.run(self.predicted_time, feed_dict=feed_dict)\n",
    "                temp_pred_time.append(gen_time)\n",
    "\n",
    "            temp_pred_time = np.array(temp_pred_time)\n",
    "            # print(\"temp_pred_time:{}\".format(temp_pred_time.shape))\n",
    "            predicted_time[i:j] = np.median(temp_pred_time, axis=0)\n",
    "\n",
    "            total_ranking += ranking\n",
    "            total_cost += cost\n",
    "            total_rae += rae\n",
    "            total_gen_loss += gen_loss\n",
    "            total_reg += reg\n",
    "            total_layer_one_recon += layer_one_recon\n",
    "            total_disc_loss += disc_loss\n",
    "            total_t_reg_loss += t_reg_loss\n",
    "            total_mse += t_mse\n",
    "            i = j\n",
    "\n",
    "        predicted_event_times = predicted_time.reshape(input_size)\n",
    "        #RAIBER NEW CHANGE\n",
    "        ci_index = concordance_index(event_times=t, predicted_scores=predicted_event_times.tolist(),\n",
    "                                    event_observed=e)\n",
    "        \n",
    "        #ci_index = 0\n",
    "        def batch_average(total):\n",
    "            return total / num_batches\n",
    "\n",
    "        return ci_index, batch_average(total_cost), batch_average(total_rae), batch_average(\n",
    "            total_ranking), batch_average(\n",
    "            total_gen_loss), batch_average(total_reg), batch_average(total_disc_loss), batch_average(\n",
    "            total_layer_one_recon), batch_average(total_t_reg_loss), batch_average(total_mse)\n",
    "\n",
    "    def batch_feed_dict(self, e, i, j, t, x, outcomes):\n",
    "        batch_x = x[i:j, :]\n",
    "        batch_t = t[i:j]\n",
    "        batch_risk = risk_set(batch_t)\n",
    "        batch_impute_mask = get_missing_mask(batch_x, self.imputation_values)\n",
    "        batch_e = e[i:j]\n",
    "        idx_observed = batch_e == 1\n",
    "        feed_dict = {self.x: batch_x,\n",
    "                     self.impute_mask: batch_impute_mask,\n",
    "                     self.t: batch_t,\n",
    "                     self.t_lab: batch_t[idx_observed],\n",
    "                     self.e: batch_e,\n",
    "                     self.risk_set: batch_risk,\n",
    "                     self.batch_size_tensor: len(batch_t),\n",
    "                     self.is_training: False,\n",
    "                     self.noise_alpha: np.ones(shape=self.noise_dim)}\n",
    "        # TODO replace with abstract methods\n",
    "\n",
    "        updated_feed_dic = self.outcomes_function(idx=i, j=j, feed_dict=feed_dict, outcomes=outcomes)\n",
    "        return updated_feed_dic\n",
    "\n",
    "    def outcomes_function(self, idx, j, feed_dict, outcomes):\n",
    "        return feed_dict\n",
    "\n",
    "    def train_neural_network(self):\n",
    "        train_print = \"Training {0} Model:\".format(self.model)\n",
    "        params_print = \"Parameters:, l2_reg:{}, learning_rate:{},\" \\\n",
    "                       \" momentum: beta1={} beta2={}, batch_size:{}, batch_norm:{},\" \\\n",
    "                       \" hidden_dim:{}, latent_dim:{}, num_of_batches:{}, keep_prob:{}, disc_update:{}\" \\\n",
    "            .format(self.l2_reg, self.learning_rate, self.beta1, self.beta2, self.batch_size,\n",
    "                    self.batch_norm, self.hidden_dim, self.latent_dim, self.num_batches, self.keep_prob,\n",
    "                    self.disc_updates)\n",
    "        print(train_print)\n",
    "        print(params_print)\n",
    "        logging.debug(train_print)\n",
    "        logging.debug(params_print)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        best_ci = 0\n",
    "        best_t_reg = np.inf\n",
    "        best_validation_epoch = 0\n",
    "        last_improvement = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        epochs = 0\n",
    "        show_all_variables()\n",
    "        j = 0\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            # Batch Training\n",
    "            run_options = tf.RunOptions(timeout_in_ms=4000)\n",
    "            x_batch, t_batch, e_batch = self.session.run([self.x_batch, self.t_batch, self.e_batch],\n",
    "                                                         options=run_options)\n",
    "            risk_batch = risk_set(data_t=t_batch)\n",
    "            batch_impute_mask = get_missing_mask(x_batch, self.imputation_values)\n",
    "            batch_size = len(t_batch)\n",
    "            idx_observed = e_batch == 1\n",
    "            # TODO simplify batch processing\n",
    "            feed_dict_train = {self.x: x_batch,\n",
    "                               self.impute_mask: batch_impute_mask,\n",
    "                               self.t: t_batch,\n",
    "                               self.t_lab: t_batch[idx_observed],\n",
    "                               self.e: e_batch,\n",
    "                               self.risk_set: risk_batch, self.batch_size_tensor: batch_size, self.is_training: True,\n",
    "                               self.noise_alpha: np.ones(shape=self.noise_dim)}\n",
    "            for k in range(self.disc_updates):\n",
    "                _ = self.session.run([self.disc_solver], feed_dict=feed_dict_train)\n",
    "\n",
    "            for m in range(self.gen_updates):\n",
    "                _ = self.session.run([self.gen_solver], feed_dict=feed_dict_train)\n",
    "\n",
    "            summary, train_time, train_cost, train_ranking, train_rae, train_reg, train_gen, train_layer_one_recon, \\\n",
    "            train_t_reg, train_t_mse, train_disc = self.session.run(\n",
    "                [self.merged, self.predicted_time, self.cost, self.ranking_partial_lik, self.total_rae,\n",
    "                 self.reg_loss, self.gen_one_loss, self.layer_one_recon, self.t_regularization_loss, self.t_mse,\n",
    "                 self.disc_one_loss],\n",
    "                feed_dict=feed_dict_train)\n",
    "            try:\n",
    "                #RAIBER NEW CHANGE\n",
    "                #train_ci = 0.0\n",
    "                train_ci = concordance_index(event_times=t_batch,\n",
    "                                             predicted_scores=train_time.reshape(t_batch.shape),\n",
    "                                             event_observed=e_batch)\n",
    "            except IndexError:\n",
    "                train_ci = 0.0\n",
    "                print(\"C-Index IndexError\")\n",
    "\n",
    "            tf.verify_tensor_all_finite(train_cost, \"Training Cost has Nan or Infinite\")\n",
    "            if j >= self.num_examples:\n",
    "                epochs += 1\n",
    "                is_epoch = True\n",
    "                # idx = 0\n",
    "                j = 0\n",
    "            else:\n",
    "                # idx = j\n",
    "                j += self.batch_size\n",
    "                is_epoch = False\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                train_print = \"it:{}, trainCI:{}, train_ranking:{}, train_RAE:{},  train_Gen:{}, train_Disc:{}, \" \\\n",
    "                              \"train_reg:{}, train_t_reg:{}, train_t_mse:{}, train_layer_one_recon:{}\".format(\n",
    "                    i, train_ci, train_ranking, train_rae, train_gen, train_disc, train_reg, train_t_reg, train_t_mse,\n",
    "                    train_layer_one_recon)\n",
    "                print(train_print)\n",
    "                logging.debug(train_print)\n",
    "\n",
    "            if is_epoch or (i == (self.num_iterations - 1)):\n",
    "                improved_str = ''\n",
    "                # Calculate  Vaid CI the CI\n",
    "                self.train_ci.append(train_ci)\n",
    "                self.train_cost.append(train_cost)\n",
    "                self.train_t_rae.append(train_rae)\n",
    "                self.train_gen.append(train_gen)\n",
    "                self.train_disc.append(train_disc)\n",
    "                self.train_ranking.append(train_ranking)\n",
    "                self.train_layer_one_recon.append(train_layer_one_recon)\n",
    "\n",
    "                self.train_writer.add_summary(summary, i)\n",
    "                valid_ci, valid_cost, valid_rae, valid_ranking, valid_gen, valid_reg, valid_disc, \\\n",
    "                valid_layer_one_recon, valid_t_reg, valid_t_mse = \\\n",
    "                    self.predict_concordance_index(\n",
    "                        x=self.valid_x,\n",
    "                        e=self.valid_e,\n",
    "                        t=self.valid_t)\n",
    "                self.valid_cost.append(valid_cost)\n",
    "                self.valid_ci.append(valid_ci)\n",
    "                self.valid_t_rae.append(valid_rae)\n",
    "                self.valid_gen.append(valid_gen)\n",
    "                self.valid_disc.append(valid_disc)\n",
    "                self.valid_ranking.append(valid_ranking)\n",
    "                self.valid_layer_one_recon.append(valid_layer_one_recon)\n",
    "                tf.verify_tensor_all_finite(valid_cost, \"Validation Cost has Nan or Infinite\")\n",
    "\n",
    "                if valid_t_reg < best_t_reg:\n",
    "                    self.saver.save(sess=self.session, save_path=self.save_path)\n",
    "                    best_validation_epoch = epochs\n",
    "                    best_t_reg = valid_t_reg\n",
    "                    last_improvement = i\n",
    "                    improved_str = '*'\n",
    "                    # Save  Best Perfoming all variables of the TensorFlow graph to file.\n",
    "                # update best validation accuracy\n",
    "                optimization_print = \"Iteration: {} epochs:{}, Training: RAE:{}, Loss: {},\" \\\n",
    "                                     \" Ranking:{}, Reg:{}, Gen:{}, Disc:{}, Recon_One:{}, T_Reg:{},T_MSE:{},  CI:{}\" \\\n",
    "                                     \" Validation RAE:{} Loss:{}, Ranking:{}, Reg:{}, Gen:{}, Disc:{}, \" \\\n",
    "                                     \"Recon_One:{}, T_Reg:{}, T_MSE:{}, CI:{}, {}\" \\\n",
    "                    .format(i + 1, epochs, train_rae, train_cost, train_ranking, train_reg, train_gen,\n",
    "                            train_disc, train_layer_one_recon, train_t_reg, train_t_mse,\n",
    "                            train_ci, valid_rae, valid_cost, valid_ranking, valid_reg, valid_gen, valid_disc,\n",
    "                            valid_layer_one_recon, valid_t_reg, valid_t_mse, valid_ci, improved_str)\n",
    "\n",
    "                print(optimization_print)\n",
    "                logging.debug(optimization_print)\n",
    "                if i - last_improvement > self.require_improvement or math.isnan(\n",
    "                        train_cost) or epochs >= self.max_epochs:\n",
    "                    # if i - last_improvement > self.require_improvement:\n",
    "                    print(\"No improvement found in a while, stopping optimization.\")\n",
    "                    # Break out from the for-loop.\n",
    "                    break\n",
    "        # Ending time.\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_dif = end_time - start_time\n",
    "        time_dif_print = \"Time usage: \" + str(timedelta(seconds=int(round(time_dif))))\n",
    "        print(time_dif_print)\n",
    "        logging.debug(time_dif_print)\n",
    "        # shutdown everything to avoid zombies\n",
    "        self.session.run(self.queue.close(cancel_pending_enqueues=True))\n",
    "        self.coord.request_stop()\n",
    "        self.coord.join(self.threads)\n",
    "        return best_validation_epoch, epochs\n",
    "\n",
    "    def train_test(self, train=True):\n",
    "\n",
    "        def get_dict(x, t, e):\n",
    "            observed_idx = e == 1\n",
    "            feed_dict = {self.x: x,\n",
    "                         self.impute_mask: get_missing_mask(x, self.imputation_values),\n",
    "                         self.t: t,\n",
    "                         self.t_lab: t[observed_idx],\n",
    "                         self.e: e,\n",
    "                         self.batch_size_tensor: len(t),\n",
    "                         self.is_training: False, self.noise_alpha: np.ones(shape=self.noise_dim)}\n",
    "            return {'feed_dict': feed_dict, 'outcomes': {}}\n",
    "\n",
    "        session_dict = {'Test': get_dict(x=self.test_x, t=self.test_t, e=self.test_e),\n",
    "                        'Train': get_dict(x=self.train_x, t=self.train_t, e=self.train_e),\n",
    "                        'Valid': get_dict(x=self.valid_x, t=self.valid_t, e=self.valid_e)}\n",
    "\n",
    "        if train:\n",
    "            best_epoch, epochs = self.train_neural_network()\n",
    "            self.time_related_metrics(best_epoch, epochs, session_dict=session_dict)\n",
    "        else:\n",
    "            self.generate_statistics(data_x=self.test_x, data_e=self.test_e, data_t=self.test_t, name='Test',\n",
    "                                     session_dict=session_dict['Test'])\n",
    "\n",
    "        self.session.close()\n",
    "\n",
    "    def time_related_metrics(self, best_epoch, epochs, session_dict):\n",
    "        #plot_cost(training=self.train_cost, validation=self.valid_cost, model=self.model, name=\"Cost\",\n",
    "        #          epochs=epochs,\n",
    "        #          best_epoch=best_epoch)\n",
    "        #plot_cost(training=self.train_ci, validation=self.valid_ci, model=self.model, name=\"CI\",\n",
    "        #          epochs=epochs,\n",
    "        #          best_epoch=best_epoch)\n",
    "        #plot_cost(training=self.train_t_rae, validation=self.valid_t_rae, model=self.model, name=\"RAE\",\n",
    "        #          epochs=epochs,\n",
    "        #          best_epoch=best_epoch)\n",
    "        #plot_cost(training=self.train_ranking, validation=self.valid_ranking, model=self.model, name=\"Rank\",\n",
    "        #          epochs=epochs,\n",
    "        #          best_epoch=best_epoch)\n",
    "        #plot_cost(training=self.train_gen, validation=self.valid_gen, model=self.model, name=\"Gen_Loss\",\n",
    "        #          epochs=epochs, best_epoch=best_epoch)\n",
    "\n",
    "        #plot_cost(training=self.train_disc, validation=self.valid_disc, model=self.model, name=\"Disc_Loss\",\n",
    "        #          epochs=epochs, best_epoch=best_epoch)\n",
    "\n",
    "        #plot_cost(training=self.train_layer_one_recon, validation=self.valid_layer_one_recon, model=self.model,\n",
    "        #          name=\"Recon\",\n",
    "        #         epochs=epochs, best_epoch=best_epoch)\n",
    "         # TEST\n",
    "        self.generate_statistics(data_x=self.test_x, data_e=self.test_e, data_t=self.test_t, name='Test',\n",
    "                                 session_dict=session_dict['Test'], t_train_R=self.train_t, y_train_R=self.train_e)\n",
    "\n",
    "        # VALID\n",
    "        self.generate_statistics(data_x=self.valid_x, data_e=self.valid_e, data_t=self.valid_t, name='Valid',\n",
    "                                 session_dict=session_dict['Valid'], t_train_R=self.train_t, y_train_R=self.train_e)\n",
    "        # TRAIN\n",
    "        self.generate_statistics(data_x=self.train_x, data_e=self.train_e, data_t=self.train_t, name='Train',\n",
    "                                 session_dict=session_dict['Train'], t_train_R=self.train_t, y_train_R=self.train_e)\n",
    "      \n",
    "\n",
    "    def generate_statistics(self, data_x, data_e, data_t, name, session_dict, t_train_R, y_train_R, save=True):\n",
    "        self.saver.restore(sess=self.session, save_path=self.save_path)\n",
    "        ci, cost, rae, ranking, gen, reg, disc, layer_one_recon, t_reg, t_mse = \\\n",
    "            self.predict_concordance_index(x=data_x,\n",
    "                                           e=data_e,\n",
    "                                           t=data_t,\n",
    "                                           outcomes=\n",
    "                                           session_dict[\n",
    "                                               'outcomes'])\n",
    "\n",
    "        observed_idx = self.extract_observed_death(name=name, observed_e=data_e, observed_t=data_t, save=save)\n",
    "\n",
    "        median_predicted_time, median_disc_prob, median_disc_score, median_prob_t_gen = self.median_predict_time(session_dict)\n",
    "\n",
    "        if name == 'Test':\n",
    "            self.save_time_samples(x=data_x[observed_idx], e=data_e[observed_idx],\n",
    "                                   t=data_t[observed_idx], name='obs_samples_predicted', cens=False)\n",
    "\n",
    "            self.save_time_samples(x=data_x[np.logical_not(observed_idx)], e=data_e[np.logical_not(observed_idx)],\n",
    "                                   t=data_t[np.logical_not(observed_idx)], name='cen_samples_predicted', cens=True)\n",
    "            \n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\{}_predicted_time'.format(name), median_predicted_time)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\{}_disc_prob'.format(name), median_disc_prob)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\{}_disc_score'.format(name), median_disc_score)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\{}_prob_t_gen'.format(name), median_prob_t_gen)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\{}_empirical_time'.format(name), data_t)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\{}_data_e'.format(name), data_e)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\{}_t_train'.format(name), t_train_R)\n",
    "            np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\{}_y_train'.format(name), y_train_R)\n",
    "        \n",
    "        observed_empirical = data_t[observed_idx]\n",
    "        observed_predicted = median_predicted_time[observed_idx]\n",
    "        #RAIBER NEW CHANGE\n",
    "        observed_ci = concordance_index(event_times=observed_empirical, predicted_scores=observed_predicted,\n",
    "                                        event_observed=data_e[observed_idx])\n",
    "        observed_ci = 0\n",
    "\n",
    "        corr = spearmanr(observed_empirical, observed_predicted)\n",
    "        results = \":{} RAE:{}, Loss:{}, Gen:{}, Disc:{}, Reg:{}, Ranking{}, Recon:{}, T_Reg:{},T_MSE:{},\" \\\n",
    "                  \" CI:{}, Observed: CI:{}, \" \\\n",
    "                  \"Correlation:{}\".format(name, rae, cost, gen, disc, reg, ranking, layer_one_recon, t_reg, t_mse, ci,\n",
    "                                          observed_ci, corr)\n",
    "        logging.debug(results)\n",
    "        print(results)\n",
    "\n",
    "    def median_predict_time(self, session_dict):\n",
    "        #pdb.set_trace()\n",
    "        predicted_time = []\n",
    "        disc_prob = []\n",
    "        disc_score = []\n",
    "        prob_t_gen = []\n",
    "        for p in range(self.sample_size):\n",
    "            gen_time, disc_prob1, disc_score1, prob_t_gen1 = self.session.run([self.predicted_time, self.disc_logit, self.disc_f, self.probability_t_gen], feed_dict=session_dict['feed_dict'])\n",
    "            predicted_time.append(gen_time)\n",
    "            disc_prob.append(disc_prob1)\n",
    "            disc_score.append(disc_score1)\n",
    "            prob_t_gen.append(prob_t_gen1)\n",
    "        predicted_time = np.array(predicted_time)\n",
    "        disc_prob = np.array(disc_prob)\n",
    "        disc_score = np.array(disc_score)\n",
    "        prob_t_gen = np.array(prob_t_gen)\n",
    "        # print(\"predicted_time_shape:{}\".format(predicted_time.shape))\n",
    "        return np.median(predicted_time, axis=0), np.median(disc_prob, axis=0), np.median(disc_score, axis=0), np.median(prob_t_gen, axis=0)\n",
    "\n",
    "    def save_time_samples(self, x, t, e, name, cens=False):\n",
    "        predicted_time = self.generate_time_samples(e, x)\n",
    "        plot_predicted_distribution(predicted=predicted_time, empirical=t, data='Test_' + name, cens=cens)\n",
    "        return\n",
    "\n",
    "    def generate_time_samples(self, e, x):\n",
    "        # observed = e == 1\n",
    "        feed_dict = {self.x: x,\n",
    "                     self.impute_mask: get_missing_mask(x, self.imputation_values),\n",
    "                     # self.t: t,\n",
    "                     # self.t_lab: t[observed],\n",
    "                     self.e: e,\n",
    "                     # self.risk_set: risk_set(t),\n",
    "                     self.batch_size_tensor: len(x),\n",
    "                     self.is_training: False, self.noise_alpha: np.ones(shape=self.noise_dim)}\n",
    "        predicted_time = []\n",
    "        for p in range(self.sample_size):\n",
    "            gen_time = self.session.run(self.predicted_time, feed_dict=feed_dict)\n",
    "            predicted_time.append(gen_time)\n",
    "        predicted_time = np.array(predicted_time)\n",
    "        return predicted_time\n",
    "\n",
    "    def enqueue(self):\n",
    "        \"\"\" Iterates over our data puts small junks into our queue.\"\"\"\n",
    "        # TensorFlow Input Pipelines for Large Data Sets\n",
    "        # ischlag.github.io\n",
    "        # http://ischlag.github.io/2016/11/07/tensorflow-input-pipeline-for-large-datasets/\n",
    "        # http://web.stanford.edu/class/cs20si/lectures/slides_09.pdf\n",
    "        under = 0\n",
    "        max = len(self.train_x)\n",
    "        try:\n",
    "            while not self.coord.should_stop():\n",
    "                # print(\"starting to write into queue\")\n",
    "                upper = under + self.capacity\n",
    "                # print(\"try to enqueue \", under, \" to \", upper)\n",
    "                if upper <= max:\n",
    "                    curr_x = self.train_x[under:upper]\n",
    "                    curr_t = self.train_t[under:upper]\n",
    "                    curr_e = self.train_e[under:upper]\n",
    "                    under = upper\n",
    "                else:\n",
    "                    rest = upper - max\n",
    "                    curr_x = np.concatenate((self.train_x[under:max], self.train_x[0:rest]))\n",
    "                    curr_t = np.concatenate((self.train_t[under:max], self.train_t[0:rest]))\n",
    "                    curr_e = np.concatenate((self.train_e[under:max], self.train_e[0:rest]))\n",
    "                    under = rest\n",
    "\n",
    "                self.session.run(self.enqueue_op,\n",
    "                                 feed_dict={self.x: curr_x, self.t: curr_t, self.e: curr_e})\n",
    "        except tf.errors.CancelledError:\n",
    "            print(\"finished enqueueing\")\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_observed_death(name, observed_e, observed_t, save=False):\n",
    "        idx_observed = observed_e == 1\n",
    "        observed_death = observed_t[idx_observed]\n",
    "        if save:\n",
    "            death_observed_print = \"{} observed_death:{}, percentage:{}\".format(name, observed_death.shape, float(\n",
    "                len(observed_death) / len(observed_t)))\n",
    "            logging.debug(death_observed_print)\n",
    "            print(death_observed_print)\n",
    "        return idx_observed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATE(DATE_AE):\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 learning_rate,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 require_improvement,\n",
    "                 seed,\n",
    "                 num_iterations,\n",
    "                 hidden_dim,\n",
    "                 latent_dim,\n",
    "                 input_dim,\n",
    "                 num_examples,\n",
    "                 keep_prob,\n",
    "                 train_data,\n",
    "                 valid_data,\n",
    "                 test_data,\n",
    "                 end_t,\n",
    "                 covariates,\n",
    "                 imputation_values,\n",
    "                 sample_size,\n",
    "                 disc_updates,\n",
    "                 categorical_indices,\n",
    "                 l2_reg,\n",
    "                 gen_updates,\n",
    "                 max_epochs,\n",
    "                 path_large_data=\"\"\n",
    "                 ):\n",
    "        DATE_AE.__init__(self, batch_size=batch_size,\n",
    "                         learning_rate=learning_rate,\n",
    "                         beta1=beta1,\n",
    "                         beta2=beta2,\n",
    "                         require_improvement=require_improvement,\n",
    "                         num_iterations=num_iterations, seed=seed,\n",
    "                         l2_reg=l2_reg,\n",
    "                         hidden_dim=hidden_dim,\n",
    "                         train_data=train_data, test_data=test_data, valid_data=valid_data,\n",
    "                         input_dim=input_dim,\n",
    "                         num_examples=num_examples, keep_prob=keep_prob,\n",
    "                         latent_dim=latent_dim, end_t=end_t,\n",
    "                         path_large_data=path_large_data,\n",
    "                         covariates=covariates,\n",
    "                         categorical_indices=categorical_indices,\n",
    "                         disc_updates=disc_updates,\n",
    "                         sample_size=sample_size, imputation_values=imputation_values,\n",
    "                         max_epochs=max_epochs, gen_updates=gen_updates)\n",
    "\n",
    "        print_model = \"model is DATE\"\n",
    "        print(print_model)\n",
    "        logging.debug(print_model)\n",
    "        self.model = 'DATE'\n",
    "        self.imputation_values = imputation_values\n",
    "\n",
    "    def _objective(self):\n",
    "        self.num_batches = self.num_examples / self.batch_size\n",
    "        logging.debug(\"num batches:{}, batch_size:{} epochs:{}\".format(self.num_batches, self.batch_size,\n",
    "                                                                       int(self.num_iterations / self.num_batches)))\n",
    "        self._build_model()\n",
    "        self.reg_loss = l2_loss(self.l2_reg) + l1_loss(self.l2_reg)\n",
    "        self.layer_one_recon = tf.constant(0.0)\n",
    "        self.cost = self.t_regularization_loss + self.disc_one_loss + self.gen_one_loss\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1,\n",
    "                                           beta2=self.beta2)\n",
    "\n",
    "        dvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Discriminator_one\")\n",
    "        genvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generate_t_given_x\")\n",
    "        self.disc_solver = optimizer.minimize(self.disc_one_loss, var_list=dvars1)\n",
    "        self.gen_solver = optimizer.minimize(self.gen_one_loss + self.t_regularization_loss, var_list=genvars1)\n",
    "\n",
    "    def _build_model(self):\n",
    "        self._risk_date()\n",
    "\n",
    "    @staticmethod\n",
    "    def log(x):\n",
    "        return tf.log(x + 1e-8)\n",
    "\n",
    "    def _risk_date(self):\n",
    "        def expand_t_dim(t):\n",
    "            return tf.expand_dims(t, axis=1)\n",
    "\n",
    "        indices_lab = tf.where(tf.equal(tf.constant(1.0, dtype=tf.float32), self.e))\n",
    "        x_lab = tf.squeeze(tf.gather(self.x, indices_lab), axis=[1])\n",
    "        t_lab_exp = expand_t_dim(self.t_lab)\n",
    "\n",
    "        t_gen, prob_t_gen = pt_given_x(x=self.x, hidden_dim=self.hidden_dim, is_training=self.is_training,\n",
    "                           batch_norm=self.batch_norm, keep_prob=self.keep_prob, batch_size=self.batch_size_tensor,\n",
    "                           input_dim=self.input_dim, noise_alpha=self.noise_alpha)\n",
    "\n",
    "        # Discriminator B\n",
    "        d_one_real, f_one_real = discriminator_one(pair_one=x_lab, pair_two=t_lab_exp, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   keep_prob=self.keep_prob)  # (x_nc, t_nc)\n",
    "        d_one_fake, f_one_fake = discriminator_one(pair_one=self.x, pair_two=t_gen, hidden_dim=self.hidden_dim,\n",
    "                                                   is_training=self.is_training, batch_norm=self.batch_norm,\n",
    "                                                   reuse=True, keep_prob=self.keep_prob)  # (x, t_gen)\n",
    "\n",
    "        # Discriminator loss\n",
    "        self.disc_one_loss = -tf.reduce_mean(self.log(d_one_real)) - tf.reduce_mean(self.log(1 - d_one_fake))\n",
    "\n",
    "        # Generator loss\n",
    "        self.gen_one_loss = tf.reduce_mean(f_one_real) - tf.reduce_mean(f_one_fake)\n",
    "        self.disc_logit = d_one_fake # added b raiber \n",
    "        self.disc_f = f_one_fake # added by raiber\n",
    "        self.probability_t_gen = tf.squeeze(prob_t_gen) # added by raiber\n",
    "        self.predicted_time = tf.squeeze(t_gen)\n",
    "        self.ranking_partial_lik, self.total_rae, self.total_t_recon_loss = \\\n",
    "            batch_metrics(e=self.e,\n",
    "                          risk_set=self.risk_set,\n",
    "                          predicted=self.predicted_time,\n",
    "                          batch_size=self.batch_size_tensor,\n",
    "                          empirical=self.t)\n",
    "\n",
    "        self.t_regularization_loss = self.total_t_recon_loss\n",
    "        self.t_mse = tf.losses.mean_squared_error(labels=self.t_lab,\n",
    "                                                  predictions=tf.gather(self.predicted_time, indices_lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of data:   id  time  duration  orig_time  first_time  mat_time  balance_time  \\\n",
      "0   1    48        24         -7          25       113      29087.21   \n",
      "1   2    26         2         18          25       138     105654.77   \n",
      "2   3    29         5         -6          25       114      44378.60   \n",
      "3   4    60        36         -2          25       119      52686.35   \n",
      "4   5    27         3         18          25       138      52100.71   \n",
      "\n",
      "    LTV_time  interest_rate_time  hpi_time  ...  REtype_SF_orig_time  \\\n",
      "0  26.658065               9.200    146.45  ...                    1   \n",
      "1  65.469851               7.680    225.10  ...                    1   \n",
      "2  31.459735              11.375    217.37  ...                    1   \n",
      "3  34.898842              10.500    189.82  ...                    1   \n",
      "4  66.346343               9.155    222.39  ...                    1   \n",
      "\n",
      "   investor_orig_time  balance_orig_time  FICO_orig_time  LTV_orig_time  \\\n",
      "0                   0            45000.0             715           69.4   \n",
      "1                   0           107200.0             558           80.0   \n",
      "2                   0            48600.0             680           83.6   \n",
      "3                   0            63750.0             587           81.8   \n",
      "4                   0            52800.0             527           80.0   \n",
      "\n",
      "   Interest_Rate_orig_time  hpi_orig_time  default_time  payoff_time  \\\n",
      "0                    9.200          87.03             1            0   \n",
      "1                    7.680         186.91             0            1   \n",
      "2                    8.750          89.58             0            1   \n",
      "3                   10.500          97.99             0            0   \n",
      "4                    9.155         186.91             0            1   \n",
      "\n",
      "   status_time  \n",
      "0            1  \n",
      "1            2  \n",
      "2            2  \n",
      "3            0  \n",
      "4            2  \n",
      "\n",
      "[5 rows x 24 columns], data shape:(49982, 24)\n",
      "missing:0.0\n",
      "head of dataset data:   orig_time  mat_time  balance_time  LTV_time  interest_rate_time  hpi_time  \\\n",
      "0  -3.477627 -1.389979     -1.033823 -2.095055            0.988138 -1.667711   \n",
      "1  -0.303878  0.003805     -0.656808 -0.550770            0.301260  1.324293   \n",
      "2  -3.350677 -1.334228     -0.958529 -1.904001            1.971006  1.030228   \n",
      "3  -2.842878 -1.055471     -0.917622 -1.767162            1.575599 -0.017829   \n",
      "4  -0.303878  0.003805     -0.920506 -0.515895            0.967802  1.221199   \n",
      "\n",
      "   gdp_time  uer_time  balance_orig_time  FICO_orig_time  LTV_orig_time  \\\n",
      "0  0.534808  1.372840          -0.991474        0.737401      -1.039149   \n",
      "1  0.240467 -0.776909          -0.693650       -1.419995       0.029385   \n",
      "2  0.001466 -0.896339          -0.974236        0.256453       0.392283   \n",
      "3  0.597611 -0.179756          -0.901696       -1.021495       0.210834   \n",
      "4  0.350143 -0.956054          -0.954126       -1.845978       0.029385   \n",
      "\n",
      "   Interest_Rate_orig_time  hpi_orig_time  REtype_CO_orig_time  \\\n",
      "0                 1.143066      -3.185900                    0   \n",
      "1                 0.680615      -0.276286                    0   \n",
      "2                 1.006156      -3.111615                    0   \n",
      "3                 1.538583      -2.866623                    0   \n",
      "4                 1.129375      -0.276286                    0   \n",
      "\n",
      "   REtype_PU_orig_time  REtype_SF_orig_time  investor_orig_time  \n",
      "0                    0                    1                   0  \n",
      "1                    0                    1                   0  \n",
      "2                    0                    1                   0  \n",
      "3                    0                    1                   0  \n",
      "4                    0                    1                   0  , data shape:(49982, 17)\n",
      "data description:          orig_time      mat_time  balance_time      LTV_time  \\\n",
      "count  4.998200e+04  4.998200e+04  4.998200e+04  4.998200e+04   \n",
      "mean  -2.047100e-16 -2.536129e-16 -5.572661e-17 -2.115337e-16   \n",
      "std    1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
      "min   -7.666977e+00 -6.686357e+00 -1.177047e+00 -3.155755e+00   \n",
      "25%   -4.308278e-01 -1.076980e-01 -6.554586e-01 -6.046198e-01   \n",
      "50%    2.039221e-01  1.710588e-01 -2.727104e-01 -2.360233e-02   \n",
      "75%    5.847720e-01  3.940642e-01  3.966436e-01  6.538653e-01   \n",
      "max    5.028021e+00  5.077178e+00  4.160276e+01  2.139352e+01   \n",
      "\n",
      "       interest_rate_time      hpi_time      gdp_time      uer_time  \\\n",
      "count        4.998200e+04  4.998200e+04  4.998200e+04  4.998200e+04   \n",
      "mean        -2.729467e-17  8.609193e-16 -2.729467e-17 -2.881222e-16   \n",
      "std          1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
      "min         -3.169282e+00 -3.136894e+00 -3.043250e+00 -1.314346e+00   \n",
      "25%         -5.144079e-01 -1.072734e+00 -2.403500e-01 -7.769087e-01   \n",
      "50%         -6.027861e-03 -1.782909e-02  3.045201e-01 -2.991867e-01   \n",
      "75%          6.040282e-01  1.030228e+00  5.976112e-01  2.979657e-01   \n",
      "max          1.377672e+01  1.369563e+00  1.794764e+00  2.387999e+00   \n",
      "\n",
      "       balance_orig_time  FICO_orig_time  LTV_orig_time  \\\n",
      "count       4.998200e+04    4.998200e+04   4.998200e+04   \n",
      "mean       -2.445147e-17    4.139691e-16  -6.255028e-17   \n",
      "std         1.000010e+00    1.000010e+00   1.000010e+00   \n",
      "min        -1.183999e+00   -3.591133e+00  -2.984686e+00   \n",
      "25%        -6.610904e-01   -6.779607e-01  -4.746403e-01   \n",
      "50%        -2.876136e-01    2.284970e-02   2.938497e-02   \n",
      "75%         3.970938e-01    7.374014e-01   5.334103e-01   \n",
      "max         3.709837e+01    2.455074e+00   1.399089e+01   \n",
      "\n",
      "       Interest_Rate_orig_time  hpi_orig_time  REtype_CO_orig_time  \\\n",
      "count             4.998200e+04   4.998200e+04         49982.000000   \n",
      "mean             -2.365538e-16  -2.604366e-16             0.064983   \n",
      "std               1.000010e+00   1.000010e+00             0.246499   \n",
      "min              -1.655980e+00  -3.515664e+00             0.000000   \n",
      "25%              -7.432477e-01  -4.936038e-01             0.000000   \n",
      "50%               2.835761e-01   4.758790e-01             0.000000   \n",
      "75%               6.638812e-01   7.572854e-01             0.000000   \n",
      "max               4.352841e+00   8.708967e-01             1.000000   \n",
      "\n",
      "       REtype_PU_orig_time  REtype_SF_orig_time  investor_orig_time  \n",
      "count         49982.000000         49982.000000        49982.000000  \n",
      "mean              0.115902             0.623625            0.118443  \n",
      "std               0.320110             0.484481            0.323135  \n",
      "min               0.000000             0.000000            0.000000  \n",
      "25%               0.000000             0.000000            0.000000  \n",
      "50%               0.000000             1.000000            0.000000  \n",
      "75%               0.000000             1.000000            0.000000  \n",
      "max               1.000000             1.000000            1.000000  \n",
      "columns:['orig_time' 'mat_time' 'balance_time' 'LTV_time' 'interest_rate_time'\n",
      " 'hpi_time' 'gdp_time' 'uer_time' 'balance_orig_time' 'FICO_orig_time'\n",
      " 'LTV_orig_time' 'Interest_Rate_orig_time' 'hpi_orig_time'\n",
      " 'REtype_CO_orig_time' 'REtype_PU_orig_time' 'REtype_SF_orig_time'\n",
      " 'investor_orig_time']\n",
      "x:[-3.47762744 -1.38997903 -1.03382322 -2.09505531  0.9881376  -1.66771114\n",
      "  0.5348077   1.37284007 -0.99147372  0.73740143 -1.03914866  1.14306563\n",
      " -3.18589953  0.          0.          1.          0.        ], t:24, e:0, len:49982\n",
      "x_shape:(49982, 17)\n",
      "end_time:60\n",
      "observed percent:0.5318314593253571\n",
      "shuffled x:[ 0.71172201  0.33831283 -0.21521574  0.10943108  0.16343216  1.03022833\n",
      "  0.00146605 -0.89633915 -0.26271517  1.43821178  0.02938497  0.58782017\n",
      "  0.83623068  0.          0.          1.          1.        ], t:4, e:1, len:49982\n",
      "num_examples:39985\n",
      "test:9997, valid:7997, train:39985, all: 57979\n",
      "categorical_flat:[13, 14, 15, 16]\n",
      "len covariates:17, categorical:4\n",
      "imputation_values:[0.20392207819158592, 0.17105877772832692, -0.2710399070327385, -0.023597524056526372, -0.006027861145139642, -0.017829093326131175, 0.3045201165680782, -0.29918672172861266, -0.28681878460652066, 0.022849696519850204, 0.029384971240994304, 0.28357607893547515, 0.47587904145774995, 0.0, 0.0, 1.0, 0.0]\n",
      "imputation_values:[0.20392207819158592, 0.17105877772832692, -0.2710399070327385, -0.023597524056526372, -0.006027861145139642, -0.017829093326131175, 0.3045201165680782, -0.29918672172861266, -0.28681878460652066, 0.022849696519850204, 0.029384971240994304, 0.28357607893547515, 0.47587904145774995, 0.0, 0.0, 1.0, 0.0]\n",
      "observed fold:0.533199949981243\n",
      "observed fold:0.526257877363209\n",
      "observed fold:0.5333249968738277\n",
      "imputation_values:[0.20392207819158592, 0.17105877772832692, -0.2710399070327385, -0.023597524056526372, -0.006027861145139642, -0.017829093326131175, 0.3045201165680782, -0.29918672172861266, -0.28681878460652066, 0.022849696519850204, 0.029384971240994304, 0.28357607893547515, 0.47587904145774995, 0.0, 0.0, 1.0, 0.0], one_hot_indices:[[13], [14], [15], [16]]\n",
      "end_t:60\n",
      "input_dim:17, continuous:[ 0  1  2  3  4  5  6  7  8  9 10 11 12], size:13, categorical:[[13], [14], [15], [16]], size4\n",
      "model is DATE_AE\n",
      "WARNING:tensorflow:From <ipython-input-19-09dbafe12efb>:81: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-001b2cc10fce>:5: Uniform.__init__ (from tensorflow.python.ops.distributions.uniform) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\tensorflow_core\\python\\ops\\distributions\\uniform.py:131: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "layer input shape:34\n",
      "name:W_decoder_h0_z, shape[34, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "WARNING:tensorflow:From <ipython-input-13-c815479674f8>:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "layer input shape:100\n",
      "name:W_decoder_h1_z, shape[100, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "name:W_encoder_t, shape[100, 1]\n",
      "name:b_encoder_t, shape[1]\n",
      "scope:Discriminator_one, pair_one:(?, 17), pair_two:(?, 1)\n",
      "layer input shape:17\n",
      "name:W_decoder_h0_z, shape[17, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:50\n",
      "name:W_decoder_h1_z, shape[50, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:1\n",
      "name:W_decoder_h0_z, shape[1, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:50\n",
      "name:W_decoder_h1_z, shape[50, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "hidden_pairs:(?, 100)\n",
      "name:W_discriminator_logits, shape[100, 1]\n",
      "name:b_discriminator_logits, shape[1]\n",
      "scope:Discriminator_one, pair_one:(?, 17), pair_two:(?, 1)\n",
      "layer input shape:17\n",
      "name:W_decoder_h0_z, shape[17, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:50\n",
      "name:W_decoder_h1_z, shape[50, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:1\n",
      "name:W_decoder_h0_z, shape[1, 50]\n",
      "name:b_decoder_h0_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "layer input shape:50\n",
      "name:W_decoder_h1_z, shape[50, 50]\n",
      "name:b_decoder_h1_z, shape[50]\n",
      "batch inputs (?, 50), shape for var50\n",
      "batch mean (50,), var (50,)\n",
      "hidden_pairs:(?, 100)\n",
      "name:W_discriminator_logits, shape[100, 1]\n",
      "name:b_discriminator_logits, shape[1]\n",
      "obs_at_risk:(?,), g_theta:<unknown>\n",
      "WARNING:tensorflow:From <ipython-input-15-011d4144dfda>:28: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From <ipython-input-18-9004f1a22ab7>:118: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-18-9004f1a22ab7>:119: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "model is DATE\n",
      "Training DATE Model:\n",
      "Parameters:, l2_reg:0.001, learning_rate:0.0003, momentum: beta1=0.9 beta2=0.999, batch_size:350, batch_norm:True, hidden_dim:[50, 50], latent_dim:50, num_of_batches:91.39428571428572, keep_prob:0.8, disc_update:1\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generate_t_given_x/W_decoder_h0_z:0 (float32_ref 34x50) [1700, bytes: 6800]\n",
      "generate_t_given_x/b_decoder_h0_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/h0_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/h0_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/W_decoder_h1_z:0 (float32_ref 100x50) [5000, bytes: 20000]\n",
      "generate_t_given_x/b_decoder_h1_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/h1_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/h1_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "generate_t_given_x/W_encoder_t:0 (float32_ref 100x1) [100, bytes: 400]\n",
      "generate_t_given_x/b_encoder_t:0 (float32_ref 1) [1, bytes: 4]\n",
      "Discriminator_one/W_decoder_h0_z:0 (float32_ref 17x50) [850, bytes: 3400]\n",
      "Discriminator_one/b_decoder_h0_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h0_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h0_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/W_decoder_h1_z:0 (float32_ref 50x50) [2500, bytes: 10000]\n",
      "Discriminator_one/b_decoder_h1_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h1_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h1_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/W_decoder_h0_z_2:0 (float32_ref 1x50) [50, bytes: 200]\n",
      "Discriminator_one/b_decoder_h0_z_2:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h0_z_decoder_batch_norm_offset_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h0_z_decoder_batch_norm_scale_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/W_decoder_h1_z_2:0 (float32_ref 50x50) [2500, bytes: 10000]\n",
      "Discriminator_one/b_decoder_h1_z_2:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h1_z_decoder_batch_norm_offset_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/h1_z_decoder_batch_norm_scale_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one/W_discriminator_logits:0 (float32_ref 100x1) [100, bytes: 400]\n",
      "Discriminator_one/b_discriminator_logits:0 (float32_ref 1) [1, bytes: 4]\n",
      "Discriminator_one_1/W_decoder_h0_z:0 (float32_ref 17x50) [850, bytes: 3400]\n",
      "Discriminator_one_1/b_decoder_h0_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h0_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h0_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/W_decoder_h1_z:0 (float32_ref 50x50) [2500, bytes: 10000]\n",
      "Discriminator_one_1/b_decoder_h1_z:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h1_z_decoder_batch_norm_offset:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h1_z_decoder_batch_norm_scale:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/W_decoder_h0_z_2:0 (float32_ref 1x50) [50, bytes: 200]\n",
      "Discriminator_one_1/b_decoder_h0_z_2:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h0_z_decoder_batch_norm_offset_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h0_z_decoder_batch_norm_scale_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/W_decoder_h1_z_2:0 (float32_ref 50x50) [2500, bytes: 10000]\n",
      "Discriminator_one_1/b_decoder_h1_z_2:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h1_z_decoder_batch_norm_offset_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/h1_z_decoder_batch_norm_scale_1:0 (float32_ref 50) [50, bytes: 200]\n",
      "Discriminator_one_1/W_discriminator_logits:0 (float32_ref 100x1) [100, bytes: 400]\n",
      "Discriminator_one_1/b_discriminator_logits:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 20303\n",
      "Total bytes of variables: 81212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:0, trainCI:0.5122450566252883, train_ranking:106.49032592773438, train_RAE:0.9140235185623169,  train_Gen:0.9928682446479797, train_Disc:1.2529733180999756, train_reg:3.214430093765259, train_t_reg:25.984174728393555, train_t_mse:141.7210235595703, train_layer_one_recon:0.0\n",
      "Iteration: 93 epochs:1, Training: RAE:0.48607322573661804, Loss: 18.12088966369629, Ranking:106.60243225097656, Reg:3.236544609069824, Gen:4.725134372711182, Disc:0.28422820568084717, Recon_One:0.0, T_Reg:13.111526489257812,T_MSE:132.01254272460938,  CI:0.6663797032460627 Validation RAE:0.48728336375013387 Loss:17.496787289820627, Ranking:105.12426733389279, Reg:3.2579947609118527, Gen:4.149912470084988, Disc:0.28932935817846583, Recon_One:0.0, T_Reg:13.057545541121957, T_MSE:130.3351924257516, CI:0.7509217719264398, *\n",
      "it:100, trainCI:0.6833419161823487, train_ranking:110.28694152832031, train_RAE:0.45404326915740967,  train_Gen:4.569278240203857, train_Disc:0.26786714792251587, train_reg:3.2377512454986572, train_t_reg:14.981627464294434, train_t_mse:413.5765075683594, train_layer_one_recon:0.0\n",
      "Iteration: 186 epochs:2, Training: RAE:0.42051440477371216, Loss: 18.42621421813965, Ranking:100.15372467041016, Reg:3.2447619438171387, Gen:6.929548263549805, Disc:0.10669660568237305, Recon_One:0.0, T_Reg:11.389968872070312,T_MSE:164.90357971191406,  CI:0.75 Validation RAE:0.432685502427838 Loss:16.669949212789206, Ranking:104.4302765900036, Reg:3.2662665559244672, Gen:5.852340059875473, Disc:0.1361200890385152, Recon_One:0.0, T_Reg:10.681489051483982, T_MSE:201.58133984387212, CI:0.7857115415047066, *\n",
      "it:200, trainCI:0.7648703257770766, train_ranking:96.59640502929688, train_RAE:0.3988790512084961,  train_Gen:7.179642677307129, train_Disc:0.09716443717479706, train_reg:3.2456393241882324, train_t_reg:12.891860961914062, train_t_mse:899.32421875, train_layer_one_recon:0.0\n",
      "Iteration: 279 epochs:3, Training: RAE:0.4334191381931305, Loss: 18.927413940429688, Ranking:101.70448303222656, Reg:3.2491583824157715, Gen:7.927552223205566, Disc:0.06410638988018036, Recon_One:0.0, T_Reg:10.935754776000977,T_MSE:185.67926025390625,  CI:0.7191678268407365 Validation RAE:0.3930418867311791 Loss:16.699578115995845, Ranking:104.03919141263057, Reg:3.270692131855316, Gen:6.95941595192237, Disc:0.08304867354485189, Recon_One:0.0, T_Reg:9.657113328790372, T_MSE:808.8627504589887, CI:0.7963081172968379, *\n",
      "it:300, trainCI:0.7422753150698262, train_ranking:110.09760284423828, train_RAE:0.40007486939430237,  train_Gen:8.32730484008789, train_Disc:0.055985916405916214, train_reg:3.2502002716064453, train_t_reg:9.641210556030273, train_t_mse:139.923583984375, train_layer_one_recon:0.0\n",
      "Iteration: 372 epochs:4, Training: RAE:0.41227003931999207, Loss: 20.880146026611328, Ranking:108.4123764038086, Reg:3.2522239685058594, Gen:9.372617721557617, Disc:0.03727169334888458, Recon_One:0.0, T_Reg:11.470256805419922,T_MSE:363.83343505859375,  CI:0.7499074096935878 Validation RAE:0.3908625824966922 Loss:16.715214302260474, Ranking:103.92068965711995, Reg:3.2737780350721732, Gen:7.85762709469382, Disc:0.05246845268600953, Recon_One:0.0, T_Reg:8.805118922488427, T_MSE:324.1996548094421, CI:0.8056986972001883, *\n",
      "it:400, trainCI:0.7288639365918098, train_ranking:105.11743927001953, train_RAE:0.38943150639533997,  train_Gen:9.38409423828125, train_Disc:0.033330101519823074, train_reg:3.253279447555542, train_t_reg:9.429091453552246, train_t_mse:187.8506317138672, train_layer_one_recon:0.0\n",
      "Iteration: 465 epochs:5, Training: RAE:0.4116806387901306, Loss: 18.576839447021484, Ranking:112.99384307861328, Reg:3.2546889781951904, Gen:9.412092208862305, Disc:0.029305286705493927, Recon_One:0.0, T_Reg:9.135440826416016,T_MSE:197.78709411621094,  CI:0.7708531708246048 Validation RAE:0.3898561544920394 Loss:16.846430854706732, Ranking:103.80866778288929, Reg:3.2762593815770016, Gen:8.315586009233671, Disc:0.03997340781607896, Recon_One:0.0, T_Reg:8.490871560265008, T_MSE:568.9859733948846, CI:0.8144488059596708, *\n",
      "it:500, trainCI:0.765909436641144, train_ranking:105.62069702148438, train_RAE:0.41379913687705994,  train_Gen:9.568892478942871, train_Disc:0.02710174024105072, train_reg:3.2551488876342773, train_t_reg:8.623979568481445, train_t_mse:151.5569305419922, train_layer_one_recon:0.0\n",
      "Iteration: 558 epochs:6, Training: RAE:0.3832608163356781, Loss: 20.662944793701172, Ranking:112.49983215332031, Reg:3.255772113800049, Gen:10.319917678833008, Disc:0.02055029571056366, Recon_One:0.0, T_Reg:10.322477340698242,T_MSE:1277.8526611328125,  CI:0.7861575405977318 Validation RAE:0.35626286936802043 Loss:17.341144797174994, Ranking:103.57244161720557, Reg:3.2773496956471666, Gen:9.07439322147844, Disc:0.028669056633427482, Recon_One:0.0, T_Reg:8.238082625529938, T_MSE:1949.5637653976198, CI:0.8230151474262729, *\n",
      "it:600, trainCI:0.7708393765987184, train_ranking:101.02787017822266, train_RAE:0.3716866970062256,  train_Gen:10.588180541992188, train_Disc:0.018211588263511658, train_reg:3.256910800933838, train_t_reg:9.93640422821045, train_t_mse:873.3710327148438, train_layer_one_recon:0.0\n",
      "Iteration: 651 epochs:7, Training: RAE:0.35667479038238525, Loss: 17.652402877807617, Ranking:102.42321014404297, Reg:3.2566282749176025, Gen:10.093759536743164, Disc:0.02004479616880417, Recon_One:0.0, T_Reg:7.538599014282227,T_MSE:170.99246215820312,  CI:0.809439696599735 Validation RAE:0.3457869302560139 Loss:17.13914707837827, Ranking:103.62788089829951, Reg:3.2782115309599473, Gen:9.275761475872514, Disc:0.024664554173378136, Recon_One:0.0, T_Reg:7.838721020126009, T_MSE:1260.3050912158465, CI:0.8260175478235895, *\n",
      "it:700, trainCI:0.7733146587035227, train_ranking:100.35601806640625, train_RAE:0.37164902687072754,  train_Gen:10.9208402633667, train_Disc:0.01541374996304512, train_reg:3.2564399242401123, train_t_reg:9.44533920288086, train_t_mse:316.5968017578125, train_layer_one_recon:0.0\n",
      "Iteration: 744 epochs:8, Training: RAE:0.350407212972641, Loss: 19.618032455444336, Ranking:103.49491119384766, Reg:3.2566959857940674, Gen:10.970205307006836, Disc:0.015329122543334961, Recon_One:0.0, T_Reg:8.632497787475586,T_MSE:243.53231811523438,  CI:0.7972360903803593 Validation RAE:0.3266892141217781 Loss:17.932643117614877, Ranking:103.51647742766208, Reg:3.278279690589251, Gen:9.830983471867441, Disc:0.019608699523655043, Recon_One:0.0, T_Reg:8.082051001087612, T_MSE:3770.4930253843972, CI:0.834377719140593, \n",
      "it:800, trainCI:0.7858897944289016, train_ranking:110.2139892578125, train_RAE:0.374188631772995,  train_Gen:10.593301773071289, train_Disc:0.014897670596837997, train_reg:3.2568840980529785, train_t_reg:11.928118705749512, train_t_mse:968.834716796875, train_layer_one_recon:0.0\n",
      "Iteration: 837 epochs:9, Training: RAE:0.360373854637146, Loss: 17.69063949584961, Ranking:110.36682891845703, Reg:3.2570695877075195, Gen:10.517578125, Disc:0.015701938420534134, Recon_One:0.0, T_Reg:7.157358646392822,T_MSE:173.59207153320312,  CI:0.8256126827139845 Validation RAE:0.33121523211505305 Loss:18.32213804276717, Ranking:103.5699229248169, Reg:3.2786557685438953, Gen:10.489742616064566, Disc:0.013093186793477369, Recon_One:0.0, T_Reg:7.819302350085154, T_MSE:3309.5158992804063, CI:0.8400553448819077, *\n",
      "it:900, trainCI:0.7920770646173828, train_ranking:98.45861053466797, train_RAE:0.35296982526779175,  train_Gen:11.594501495361328, train_Disc:0.01052825152873993, train_reg:3.2567262649536133, train_t_reg:8.397699356079102, train_t_mse:397.495361328125, train_layer_one_recon:0.0\n",
      "Iteration: 930 epochs:10, Training: RAE:0.36281463503837585, Loss: 19.364070892333984, Ranking:113.64260864257812, Reg:3.257230758666992, Gen:11.276633262634277, Disc:0.011072131805121899, Recon_One:0.0, T_Reg:8.076364517211914,T_MSE:214.6066436767578,  CI:0.7701201059201815 Validation RAE:0.3149360945646027 Loss:18.07045136986337, Ranking:103.51240003766365, Reg:3.278818007661534, Gen:10.523941010941083, Disc:0.013471241068894808, Recon_One:0.0, T_Reg:7.533039106970297, T_MSE:4675.827355794464, CI:0.8436936474194193, *\n",
      "it:1000, trainCI:0.8114765011316736, train_ranking:99.27394104003906, train_RAE:0.3218682110309601,  train_Gen:13.217111587524414, train_Disc:0.006416281685233116, train_reg:3.2570621967315674, train_t_reg:6.858822345733643, train_t_mse:202.39710998535156, train_layer_one_recon:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1023 epochs:11, Training: RAE:0.3387593924999237, Loss: 20.125991821289062, Ranking:99.84967041015625, Reg:3.257138252258301, Gen:12.144979476928711, Disc:0.007662579417228699, Recon_One:0.0, T_Reg:7.973349571228027,T_MSE:191.11106872558594,  CI:0.8184659773984131 Validation RAE:0.3125705043867378 Loss:18.589142475841314, Ranking:103.56528222657776, Reg:3.2787248881679782, Gen:11.627905372084417, Disc:0.007803957726854383, Recon_One:0.0, T_Reg:6.953433014026683, T_MSE:1964.0520249662868, CI:0.8502276911394786, *\n",
      "it:1100, trainCI:0.798614898025957, train_ranking:97.33397674560547, train_RAE:0.34362050890922546,  train_Gen:12.214051246643066, train_Disc:0.007257615216076374, train_reg:3.256826877593994, train_t_reg:7.433902740478516, train_t_mse:135.36563110351562, train_layer_one_recon:0.0\n",
      "Iteration: 1116 epochs:12, Training: RAE:0.3264888823032379, Loss: 19.0360050201416, Ranking:113.62300872802734, Reg:3.2570481300354004, Gen:11.773918151855469, Disc:0.009057383984327316, Recon_One:0.0, T_Reg:7.2530293464660645,T_MSE:255.63095092773438,  CI:0.7966581704899461 Validation RAE:0.28245188906086344 Loss:18.410020138720384, Ranking:103.33471752575551, Reg:3.2786341686613696, Gen:11.620820109749102, Disc:0.0076283361140638125, Recon_One:0.0, T_Reg:6.781571910457938, T_MSE:3306.160338538817, CI:0.8532852389198344, *\n",
      "it:1200, trainCI:0.8253178944377353, train_ranking:113.17422485351562, train_RAE:0.3271259069442749,  train_Gen:12.887173652648926, train_Disc:0.00609807251021266, train_reg:3.256211757659912, train_t_reg:7.1400957107543945, train_t_mse:292.3350524902344, train_layer_one_recon:0.0\n",
      "Iteration: 1209 epochs:13, Training: RAE:0.31884679198265076, Loss: 18.976383209228516, Ranking:108.2706069946289, Reg:3.2561824321746826, Gen:11.942056655883789, Disc:0.00817849487066269, Recon_One:0.0, T_Reg:7.02614688873291,T_MSE:236.7061309814453,  CI:0.8363471082502089 Validation RAE:0.2957017330865763 Loss:18.938389578148232, Ranking:103.44124656029459, Reg:3.2777627334007997, Gen:11.597408090991886, Disc:0.0074366170005066715, Recon_One:0.0, T_Reg:7.333544824993043, T_MSE:6821.4680486490415, CI:0.861994693372444, \n",
      "it:1300, trainCI:0.8395532620347518, train_ranking:106.67853546142578, train_RAE:0.3283747434616089,  train_Gen:12.833206176757812, train_Disc:0.005484415218234062, train_reg:3.255429267883301, train_t_reg:6.580982208251953, train_t_mse:207.2574462890625, train_layer_one_recon:0.0\n",
      "Iteration: 1302 epochs:14, Training: RAE:0.3230653405189514, Loss: 19.844650268554688, Ranking:114.79401397705078, Reg:3.255394697189331, Gen:13.325565338134766, Disc:0.005344695411622524, Recon_One:0.0, T_Reg:6.513739109039307,T_MSE:127.6109619140625,  CI:0.8009226734110815 Validation RAE:0.2716903175892674 Loss:18.998790112259655, Ranking:103.34620707718092, Reg:3.276969777713407, Gen:11.81297829945087, Disc:0.010063515111829164, Recon_One:0.0, T_Reg:7.1757482292802095, T_MSE:7356.935203297519, CI:0.8626553811293368, \n",
      "Iteration: 1395 epochs:15, Training: RAE:0.32495537400245667, Loss: 21.011362075805664, Ranking:103.94194030761719, Reg:3.2540550231933594, Gen:14.469366073608398, Disc:0.004096898250281811, Recon_One:0.0, T_Reg:6.537899494171143,T_MSE:188.1559600830078,  CI:0.8230698293969978 Validation RAE:0.2746508573574976 Loss:21.222553739730188, Ranking:103.31112469406862, Reg:3.275621225047711, Gen:12.993860990088537, Disc:0.0043357551761453225, Recon_One:0.0, T_Reg:8.224356995830986, T_MSE:21040.248134050635, CI:0.8672670042718219, \n",
      "it:1400, trainCI:0.8120515448821204, train_ranking:109.37017822265625, train_RAE:0.3194928467273712,  train_Gen:12.830130577087402, train_Disc:0.00550222210586071, train_reg:3.2538506984710693, train_t_reg:5.665472984313965, train_t_mse:159.46214294433594, train_layer_one_recon:0.0\n",
      "Iteration: 1488 epochs:16, Training: RAE:0.28159546852111816, Loss: 21.40180206298828, Ranking:106.40443420410156, Reg:3.25321888923645, Gen:13.369523048400879, Disc:0.0044958945363759995, Recon_One:0.0, T_Reg:8.027783393859863,T_MSE:1271.23583984375,  CI:0.8438801726073184 Validation RAE:0.265033335533383 Loss:20.46291613200165, Ranking:103.2862847039472, Reg:3.2747795496252876, Gen:12.956032270608253, Disc:0.004029771300202532, Recon_One:0.0, T_Reg:7.502854095483789, T_MSE:11345.609737805044, CI:0.871064165662837, \n",
      "it:1500, trainCI:0.8417711516173216, train_ranking:100.85433959960938, train_RAE:0.3004508316516876,  train_Gen:14.34280014038086, train_Disc:0.003648643149062991, train_reg:3.2529239654541016, train_t_reg:6.545641899108887, train_t_mse:343.3617858886719, train_layer_one_recon:0.0\n",
      "Iteration: 1581 epochs:17, Training: RAE:0.2990472614765167, Loss: 20.42193603515625, Ranking:107.94930267333984, Reg:3.253594398498535, Gen:14.88839340209961, Disc:0.003141888417303562, Recon_One:0.0, T_Reg:5.530401706695557,T_MSE:134.39149475097656,  CI:0.8204976303317536 Validation RAE:0.2552881454905674 Loss:20.65064561535481, Ranking:103.31850613566269, Reg:3.2751575475694894, Gen:13.282118288445385, Disc:0.003920488232869934, Recon_One:0.0, T_Reg:7.364606708232293, T_MSE:11824.650283609313, CI:0.8696624886812759, \n",
      "it:1600, trainCI:0.858795024229405, train_ranking:106.7313003540039, train_RAE:0.25794968008995056,  train_Gen:13.596365928649902, train_Disc:0.003803655505180359, train_reg:3.25338077545166, train_t_reg:4.695705413818359, train_t_mse:130.2733917236328, train_layer_one_recon:0.0\n",
      "Iteration: 1674 epochs:18, Training: RAE:0.3101886212825775, Loss: 20.831092834472656, Ranking:99.45311737060547, Reg:3.252772092819214, Gen:14.411089897155762, Disc:0.003429278964176774, Recon_One:0.0, T_Reg:6.416573524475098,T_MSE:144.05885314941406,  CI:0.8436611406191319 Validation RAE:0.25368485507986555 Loss:21.11598513671066, Ranking:103.29433730710845, Reg:3.2743297920713608, Gen:13.16562876546921, Disc:0.0036191574090664812, Recon_One:0.0, T_Reg:7.94673708657764, T_MSE:21059.71384333479, CI:0.8785288859535906, \n",
      "it:1700, trainCI:0.8559445764507171, train_ranking:92.30774688720703, train_RAE:0.26185011863708496,  train_Gen:14.729582786560059, train_Disc:0.0030847019515931606, train_reg:3.252394437789917, train_t_reg:7.018043041229248, train_t_mse:358.37530517578125, train_layer_one_recon:0.0\n",
      "Iteration: 1767 epochs:19, Training: RAE:0.267077773809433, Loss: 18.989290237426758, Ranking:102.85108947753906, Reg:3.252676486968994, Gen:13.948001861572266, Disc:0.003265351289883256, Recon_One:0.0, T_Reg:5.038022994995117,T_MSE:216.05519104003906,  CI:0.8400237345854187 Validation RAE:0.24562038176578777 Loss:21.237380817351795, Ranking:103.28605564084518, Reg:3.2742335525947732, Gen:13.387369236022483, Disc:0.003152951456145676, Recon_One:0.0, T_Reg:7.846858342826493, T_MSE:18934.136699768576, CI:0.8752216887951375, \n",
      "it:1800, trainCI:0.8577798396865899, train_ranking:103.1200180053711, train_RAE:0.2618153691291809,  train_Gen:14.947999954223633, train_Disc:0.002569983247667551, train_reg:3.2522480487823486, train_t_reg:4.861830711364746, train_t_mse:124.92267608642578, train_layer_one_recon:0.0\n",
      "Iteration: 1860 epochs:20, Training: RAE:0.2873303294181824, Loss: 19.98021697998047, Ranking:100.41329956054688, Reg:3.251725673675537, Gen:14.450002670288086, Disc:0.0027836584486067295, Recon_One:0.0, T_Reg:5.527431488037109,T_MSE:185.664794921875,  CI:0.8451845421303238 Validation RAE:0.25291186754474254 Loss:22.770027925301243, Ranking:103.26134787956983, Reg:3.273276437800184, Gen:12.990286762570983, Disc:0.003893536996975405, Recon_One:0.0, T_Reg:9.77584766390563, T_MSE:46380.43487570771, CI:0.8781962330087719, \n",
      "it:1900, trainCI:0.8442440907408386, train_ranking:100.05265808105469, train_RAE:0.2568674087524414,  train_Gen:14.721912384033203, train_Disc:0.0024629232939332724, train_reg:3.252054452896118, train_t_reg:5.100938320159912, train_t_mse:188.11550903320312, train_layer_one_recon:0.0\n",
      "Iteration: 1953 epochs:21, Training: RAE:0.24998563528060913, Loss: 19.331708908081055, Ranking:91.75926971435547, Reg:3.251415967941284, Gen:13.933635711669922, Disc:0.0033435216173529625, Recon_One:0.0, T_Reg:5.394730091094971,T_MSE:175.06668090820312,  CI:0.8507740825688074 Validation RAE:0.2536686166899196 Loss:21.5019447767423, Ranking:103.27392932070902, Reg:3.272964679495728, Gen:13.445122146272533, Disc:0.003133385428047555, Recon_One:0.0, T_Reg:8.053689207511349, T_MSE:25396.171231667548, CI:0.883793655540455, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:2000, trainCI:0.8593299134337928, train_ranking:98.90995788574219, train_RAE:0.2555827796459198,  train_Gen:14.147375106811523, train_Disc:0.0029252655804157257, train_reg:3.250941276550293, train_t_reg:4.680120468139648, train_t_mse:146.0420684814453, train_layer_one_recon:0.0\n",
      "Iteration: 2046 epochs:22, Training: RAE:0.29366475343704224, Loss: 20.972688674926758, Ranking:107.55656433105469, Reg:3.2507119178771973, Gen:15.348555564880371, Disc:0.0021804552525281906, Recon_One:0.0, T_Reg:5.621953010559082,T_MSE:108.82624816894531,  CI:0.8247645017352504 Validation RAE:0.2518792757693419 Loss:20.857932112344255, Ranking:103.28768546154637, Reg:3.272255963350186, Gen:13.564358464744638, Disc:0.0031792069587316626, Recon_One:0.0, T_Reg:7.29039435358633, T_MSE:16186.474591677705, CI:0.884234662653018, \n",
      "it:2100, trainCI:0.8402632886239444, train_ranking:107.20105743408203, train_RAE:0.2987845838069916,  train_Gen:14.857328414916992, train_Disc:0.00225302972830832, train_reg:3.250382661819458, train_t_reg:5.880956649780273, train_t_mse:198.32379150390625, train_layer_one_recon:0.0\n",
      "Iteration: 2139 epochs:23, Training: RAE:0.2815956473350525, Loss: 21.978267669677734, Ranking:99.36698150634766, Reg:3.249695062637329, Gen:16.196395874023438, Disc:0.0017838447820395231, Recon_One:0.0, T_Reg:5.780088901519775,T_MSE:218.0366668701172,  CI:0.8615908667535993 Validation RAE:0.2453346144069325 Loss:22.768517644819475, Ranking:103.19347841912274, Reg:3.2712323689171563, Gen:13.991818345158014, Disc:0.002350738719913946, Recon_One:0.0, T_Reg:8.774348314186774, T_MSE:37685.07630831304, CI:0.8862239233327079, \n",
      "it:2200, trainCI:0.8490395359974459, train_ranking:99.415771484375, train_RAE:0.2671971917152405,  train_Gen:15.030742645263672, train_Disc:0.0020763848442584276, train_reg:3.2499606609344482, train_t_reg:5.334942817687988, train_t_mse:131.52120971679688, train_layer_one_recon:0.0\n",
      "Iteration: 2232 epochs:24, Training: RAE:0.3126022219657898, Loss: 21.1949405670166, Ranking:100.16039276123047, Reg:3.2492759227752686, Gen:14.9189453125, Disc:0.0019308060873299837, Recon_One:0.0, T_Reg:6.274065017700195,T_MSE:211.958251953125,  CI:0.8345436234495919 Validation RAE:0.24237743853345073 Loss:23.776560579819634, Ranking:103.22847131402608, Reg:3.2708104512118186, Gen:14.460326892756903, Disc:0.002354576008826889, Recon_One:0.0, T_Reg:9.313879132911802, T_MSE:47817.60218850341, CI:0.8877090214781002, \n",
      "it:2300, trainCI:0.8732898938241479, train_ranking:102.2678451538086, train_RAE:0.26749637722969055,  train_Gen:15.166864395141602, train_Disc:0.0019768138881772757, train_reg:3.2498695850372314, train_t_reg:4.967658996582031, train_t_mse:131.05162048339844, train_layer_one_recon:0.0\n",
      "Iteration: 2325 epochs:25, Training: RAE:0.2854440212249756, Loss: 20.978763580322266, Ranking:106.71417999267578, Reg:3.2493860721588135, Gen:15.084070205688477, Disc:0.0021230257116258144, Recon_One:0.0, T_Reg:5.892571449279785,T_MSE:301.4037170410156,  CI:0.8411419476773102 Validation RAE:0.230899883781923 Loss:20.519918201117868, Ranking:103.16752717243637, Reg:3.2709213306087843, Gen:13.751564667465479, Disc:0.0025610387582311865, Recon_One:0.0, T_Reg:6.765792184104409, T_MSE:13139.442269641399, CI:0.8893706649428258, *\n",
      "it:2400, trainCI:0.8235579488672197, train_ranking:109.55060577392578, train_RAE:0.2766953706741333,  train_Gen:16.1024169921875, train_Disc:0.001640073605813086, train_reg:3.2487409114837646, train_t_reg:5.9896063804626465, train_t_mse:300.60125732421875, train_layer_one_recon:0.0\n",
      "Iteration: 2418 epochs:26, Training: RAE:0.2835380434989929, Loss: 21.194782257080078, Ranking:104.16869354248047, Reg:3.2484824657440186, Gen:15.64583683013916, Disc:0.001621860545128584, Recon_One:0.0, T_Reg:5.547323226928711,T_MSE:225.5107879638672,  CI:0.8648462597521799 Validation RAE:0.23826137418014728 Loss:24.203721693162247, Ranking:103.18657313493902, Reg:3.270011735555752, Gen:14.601214140433736, Disc:0.0017904339886817937, Recon_One:0.0, T_Reg:9.600717266813076, T_MSE:53868.05228767238, CI:0.8916427371240441, \n",
      "it:2500, trainCI:0.8503188824911565, train_ranking:103.02863311767578, train_RAE:0.27687492966651917,  train_Gen:15.747406959533691, train_Disc:0.0016295930836349726, train_reg:3.248955488204956, train_t_reg:4.774208068847656, train_t_mse:152.93980407714844, train_layer_one_recon:0.0\n",
      "Iteration: 2511 epochs:27, Training: RAE:0.2793578505516052, Loss: 21.75199317932129, Ranking:110.1216049194336, Reg:3.2489101886749268, Gen:16.260377883911133, Disc:0.0012209151173010468, Recon_One:0.0, T_Reg:5.490394115447998,T_MSE:277.6253356933594,  CI:0.8713253943331964 Validation RAE:0.22694866592338178 Loss:21.928866976601668, Ranking:103.15555378364238, Reg:3.2704422932141, Gen:14.104578079723545, Disc:0.0024267866147400937, Recon_One:0.0, T_Reg:7.821862030076998, T_MSE:26412.819584241523, CI:0.8920401421854292, \n",
      "it:2600, trainCI:0.8619201440372802, train_ranking:100.17491912841797, train_RAE:0.24930129945278168,  train_Gen:16.745464324951172, train_Disc:0.0011264875065535307, train_reg:3.2482802867889404, train_t_reg:4.8495893478393555, train_t_mse:162.40797424316406, train_layer_one_recon:0.0\n",
      "Iteration: 2604 epochs:28, Training: RAE:0.2707393169403076, Loss: 22.592403411865234, Ranking:108.43880462646484, Reg:3.2483315467834473, Gen:16.785568237304688, Disc:0.0011059932876378298, Recon_One:0.0, T_Reg:5.805728912353516,T_MSE:352.47991943359375,  CI:0.8284306515664068 Validation RAE:0.2287710268817664 Loss:25.38354417391385, Ranking:103.08252739819852, Reg:3.269859816381987, Gen:15.219632843158776, Disc:0.001343612449697389, Recon_One:0.0, T_Reg:10.16256814153847, T_MSE:69927.8663339027, CI:0.8925666584473729, \n",
      "Iteration: 2697 epochs:29, Training: RAE:0.2558714747428894, Loss: 20.665571212768555, Ranking:100.29937744140625, Reg:3.248030662536621, Gen:16.253108978271484, Disc:0.0013245941372588277, Recon_One:0.0, T_Reg:4.411137580871582,T_MSE:141.6591033935547,  CI:0.8733699382292381 Validation RAE:0.224615337309188 Loss:24.12147042571537, Ranking:103.10893510153282, Reg:3.2695569380292357, Gen:14.927150941691101, Disc:0.0014567001464738146, Recon_One:0.0, T_Reg:9.192862754555602, T_MSE:53060.76543792957, CI:0.8939369173875498, \n",
      "it:2700, trainCI:0.8840407382515638, train_ranking:102.61222839355469, train_RAE:0.25215867161750793,  train_Gen:17.017513275146484, train_Disc:0.000993348308838904, train_reg:3.248044490814209, train_t_reg:4.45611572265625, train_t_mse:181.35061645507812, train_layer_one_recon:0.0\n",
      "Iteration: 2790 epochs:30, Training: RAE:0.2435438334941864, Loss: 20.759201049804688, Ranking:100.2602310180664, Reg:3.2475757598876953, Gen:16.07124137878418, Disc:0.0011764612281695008, Recon_One:0.0, T_Reg:4.686784267425537,T_MSE:140.50975036621094,  CI:0.8666067225557348 Validation RAE:0.21915522222222575 Loss:21.932252586014616, Ranking:103.10707588381837, Reg:3.2690990205196884, Gen:14.501282231754342, Disc:0.00173662125655413, Recon_One:0.0, T_Reg:7.429233671531567, T_MSE:22678.322958507968, CI:0.8936091773499062, \n",
      "it:2800, trainCI:0.8842699032240817, train_ranking:99.01304626464844, train_RAE:0.24595068395137787,  train_Gen:16.887882232666016, train_Disc:0.0009634477319195867, train_reg:3.247554302215576, train_t_reg:4.583585262298584, train_t_mse:134.50135803222656, train_layer_one_recon:0.0\n",
      "Iteration: 2883 epochs:31, Training: RAE:0.24289996922016144, Loss: 19.918310165405273, Ranking:93.1126937866211, Reg:3.246800422668457, Gen:15.922243118286133, Disc:0.0012783946003764868, Recon_One:0.0, T_Reg:3.99478816986084,T_MSE:128.57901000976562,  CI:0.8965214186488248 Validation RAE:0.232587891775354 Loss:22.777488420378408, Ranking:103.06506818184872, Reg:3.2683185447644214, Gen:14.875299202824914, Disc:0.001452705704882961, Recon_One:0.0, T_Reg:7.9007364011070225, T_MSE:30299.796551275926, CI:0.8959733665406555, \n",
      "it:2900, trainCI:0.836904504244523, train_ranking:110.59390258789062, train_RAE:0.2631321847438812,  train_Gen:17.25037384033203, train_Disc:0.0009030853398144245, train_reg:3.2474491596221924, train_t_reg:4.690752983093262, train_t_mse:138.16220092773438, train_layer_one_recon:0.0\n",
      "Iteration: 2976 epochs:32, Training: RAE:0.2506828010082245, Loss: 20.0674991607666, Ranking:99.55391693115234, Reg:3.246734142303467, Gen:15.747552871704102, Disc:0.0014263092307373881, Recon_One:0.0, T_Reg:4.3185200691223145,T_MSE:204.54147338867188,  CI:0.890224444208063 Validation RAE:0.232971823165547 Loss:25.001908266888687, Ranking:103.03824843205973, Reg:3.2682518251272863, Gen:15.107999432902345, Disc:0.0013209128652720138, Recon_One:0.0, T_Reg:9.892587828996913, T_MSE:70910.34358249868, CI:0.896416265122481, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:3000, trainCI:0.879067755642063, train_ranking:104.0204849243164, train_RAE:0.24482356011867523,  train_Gen:16.465129852294922, train_Disc:0.0010475656017661095, train_reg:3.2467708587646484, train_t_reg:3.7869930267333984, train_t_mse:117.51766204833984, train_layer_one_recon:0.0\n",
      "Iteration: 3069 epochs:33, Training: RAE:0.28645262122154236, Loss: 22.356149673461914, Ranking:103.10993194580078, Reg:3.246397018432617, Gen:16.865039825439453, Disc:0.0008919195388443768, Recon_One:0.0, T_Reg:5.490218162536621,T_MSE:234.1758575439453,  CI:0.8682208525672435 Validation RAE:0.23263921040929347 Loss:23.178973012497025, Ranking:103.09874346304852, Reg:3.267912466972936, Gen:15.145630351719266, Disc:0.0012825913855097223, Recon_One:0.0, T_Reg:8.032060159926743, T_MSE:33368.36025194629, CI:0.8964619797237443, \n",
      "it:3100, trainCI:0.863011583011583, train_ranking:102.75753784179688, train_RAE:0.2577272653579712,  train_Gen:16.5709228515625, train_Disc:0.0010139639489352703, train_reg:3.2464423179626465, train_t_reg:6.597373008728027, train_t_mse:1135.13720703125, train_layer_one_recon:0.0\n",
      "Iteration: 3162 epochs:34, Training: RAE:0.27240246534347534, Loss: 21.793848037719727, Ranking:102.00811767578125, Reg:3.245569944381714, Gen:16.723955154418945, Disc:0.0010636122897267342, Recon_One:0.0, T_Reg:5.068829536437988,T_MSE:163.0757293701172,  CI:0.863852695638605 Validation RAE:0.22644615104768787 Loss:23.581869631718735, Ranking:103.11864524002715, Reg:3.2670799115009124, Gen:15.007935003204555, Disc:0.0013718675015308127, Recon_One:0.0, T_Reg:8.572562942061257, T_MSE:42489.65469919414, CI:0.8988534846783865, \n",
      "it:3200, trainCI:0.8579955103657732, train_ranking:100.5098648071289, train_RAE:0.2884659469127655,  train_Gen:18.023147583007812, train_Disc:0.0007409745012409985, train_reg:3.2449841499328613, train_t_reg:5.629021644592285, train_t_mse:147.8168182373047, train_layer_one_recon:0.0\n",
      "Iteration: 3255 epochs:35, Training: RAE:0.2563251554965973, Loss: 21.762460708618164, Ranking:99.34333038330078, Reg:3.2448348999023438, Gen:16.807392120361328, Disc:0.0009977895533666015, Recon_One:0.0, T_Reg:4.954070568084717,T_MSE:195.19766235351562,  CI:0.8450779130989736 Validation RAE:0.22791528870973854 Loss:25.05871407968097, Ranking:103.07240454542706, Reg:3.266339995525055, Gen:15.638752915016395, Disc:0.000992884099882595, Recon_One:0.0, T_Reg:9.418968206586667, T_MSE:63722.58702261799, CI:0.8982402556048041, \n",
      "it:3300, trainCI:0.8646031978876338, train_ranking:109.053466796875, train_RAE:0.2597971558570862,  train_Gen:17.660322189331055, train_Disc:0.000687410356476903, train_reg:3.2449920177459717, train_t_reg:5.728005886077881, train_t_mse:215.60836791992188, train_layer_one_recon:0.0\n",
      "Iteration: 3348 epochs:36, Training: RAE:0.2532617449760437, Loss: 32.00019073486328, Ranking:93.2076416015625, Reg:3.244459867477417, Gen:17.632509231567383, Disc:0.0007646844023838639, Recon_One:0.0, T_Reg:14.36691665649414,T_MSE:16879.775390625,  CI:0.8917315009370208 Validation RAE:0.22738043987946285 Loss:25.486323385368635, Ranking:103.13445059406737, Reg:3.2659624775782428, Gen:15.997872770585403, Disc:0.0008592929387001845, Recon_One:0.0, T_Reg:9.487591356847739, T_MSE:58932.87158711815, CI:0.9000129553362205, \n",
      "it:3400, trainCI:0.8413904007916873, train_ranking:107.37745666503906, train_RAE:0.26728513836860657,  train_Gen:17.95960235595703, train_Disc:0.0006901264423504472, train_reg:3.2444374561309814, train_t_reg:5.748538970947266, train_t_mse:464.6283264160156, train_layer_one_recon:0.0\n",
      "Iteration: 3441 epochs:37, Training: RAE:0.26220861077308655, Loss: 21.826892852783203, Ranking:107.53805541992188, Reg:3.2444517612457275, Gen:16.942230224609375, Disc:0.0007564958068542182, Recon_One:0.0, T_Reg:4.883906364440918,T_MSE:113.47769165039062,  CI:0.8542823156225218 Validation RAE:0.22035740307529106 Loss:26.56732725921207, Ranking:103.05832150549803, Reg:3.265954317622622, Gen:15.68407481113523, Disc:0.0009612889167584495, Recon_One:0.0, T_Reg:10.88229080461123, T_MSE:121841.84109429143, CI:0.8983989425065564, \n",
      "it:3500, trainCI:0.879847766074348, train_ranking:98.45328521728516, train_RAE:0.26039648056030273,  train_Gen:17.736989974975586, train_Disc:0.0006725870771333575, train_reg:3.244349479675293, train_t_reg:4.225727081298828, train_t_mse:139.0343780517578, train_layer_one_recon:0.0\n",
      "Iteration: 3534 epochs:38, Training: RAE:0.2359001785516739, Loss: 20.646461486816406, Ranking:98.14476013183594, Reg:3.244030237197876, Gen:16.857135772705078, Disc:0.0008132464718073606, Recon_One:0.0, T_Reg:3.7885117530822754,T_MSE:105.4654769897461,  CI:0.8787404971154364 Validation RAE:0.22790701266779248 Loss:26.71466266660224, Ranking:103.05791580335668, Reg:3.2655299999303367, Gen:15.246977084008279, Disc:0.0012199711333482812, Recon_One:0.0, T_Reg:11.46646572140227, T_MSE:125314.27696788998, CI:0.8976921471158262, \n",
      "it:3600, trainCI:0.8870273812576573, train_ranking:92.57347106933594, train_RAE:0.23876293003559113,  train_Gen:17.868423461914062, train_Disc:0.0006075907731428742, train_reg:3.2452197074890137, train_t_reg:4.12858247756958, train_t_mse:119.038330078125, train_layer_one_recon:0.0\n",
      "Iteration: 3627 epochs:39, Training: RAE:0.27939748764038086, Loss: 21.802001953125, Ranking:110.45606994628906, Reg:3.2450876235961914, Gen:17.237049102783203, Disc:0.0007984028197824955, Recon_One:0.0, T_Reg:4.564155101776123,T_MSE:96.38147735595703,  CI:0.8555193630112197 Validation RAE:0.22193648622634338 Loss:21.378522254950525, Ranking:103.09015292889985, Reg:3.2665943941414706, Gen:15.489578157629804, Disc:0.001214143221769787, Recon_One:0.0, T_Reg:5.8877297352295574, T_MSE:10479.68475109967, CI:0.8989042595740402, *\n",
      "it:3700, trainCI:0.8967496723460027, train_ranking:100.98907470703125, train_RAE:0.2091415822505951,  train_Gen:17.5999755859375, train_Disc:0.0006382600404322147, train_reg:3.244560718536377, train_t_reg:3.6106576919555664, train_t_mse:127.1100082397461, train_layer_one_recon:0.0\n",
      "Iteration: 3720 epochs:40, Training: RAE:0.2248687744140625, Loss: 21.562997817993164, Ranking:101.66363525390625, Reg:3.2446305751800537, Gen:18.05808448791504, Disc:0.0005363754462450743, Recon_One:0.0, T_Reg:3.5043764114379883,T_MSE:107.51705932617188,  CI:0.8746330692297701 Validation RAE:0.2167782125412501 Loss:24.201193651259207, Ranking:103.0790570588116, Reg:3.2661343166436705, Gen:15.499959040302269, Disc:0.0011597312826428475, Recon_One:0.0, T_Reg:8.70007485378619, T_MSE:50063.13548000422, CI:0.8986875512385513, \n",
      "it:3800, trainCI:0.8907952512603675, train_ranking:97.16291809082031, train_RAE:0.23328140377998352,  train_Gen:17.769556045532227, train_Disc:0.0006609670235775411, train_reg:3.2456777095794678, train_t_reg:4.44619083404541, train_t_mse:226.90789794921875, train_layer_one_recon:0.0\n",
      "Iteration: 3813 epochs:41, Training: RAE:0.2418053299188614, Loss: 23.000041961669922, Ranking:105.89370727539062, Reg:3.2457211017608643, Gen:18.862077713012695, Disc:0.00048482156125828624, Recon_One:0.0, T_Reg:4.13748025894165,T_MSE:134.4371795654297,  CI:0.8686628445534887 Validation RAE:0.22011500504578263 Loss:23.097387397082073, Ranking:103.08512689710423, Reg:3.267232070673372, Gen:16.223923283784824, Disc:0.0009680398302803759, Recon_One:0.0, T_Reg:6.872495739433457, T_MSE:21575.29597633613, CI:0.8979724284701596, \n",
      "it:3900, trainCI:0.864971337660042, train_ranking:105.3544921875, train_RAE:0.2689884901046753,  train_Gen:19.384801864624023, train_Disc:0.000480773946037516, train_reg:3.245603322982788, train_t_reg:4.838940620422363, train_t_mse:247.08822631835938, train_layer_one_recon:0.0\n",
      "Iteration: 3906 epochs:42, Training: RAE:0.2498726099729538, Loss: 21.931739807128906, Ranking:107.90959930419922, Reg:3.245817184448242, Gen:17.9392147064209, Disc:0.0005337570328265429, Recon_One:0.0, T_Reg:3.9919910430908203,T_MSE:155.8072967529297,  CI:0.888842312908396 Validation RAE:0.2327610208337659 Loss:22.85682639584357, Ranking:103.10996354811338, Reg:3.267328790147349, Gen:16.716273719107374, Disc:0.0005783730246090782, Recon_One:0.0, T_Reg:6.139974352924976, T_MSE:11140.155037406148, CI:0.9002934577713766, \n",
      "Iteration: 3999 epochs:43, Training: RAE:0.2468145489692688, Loss: 23.199249267578125, Ranking:99.08982849121094, Reg:3.245511293411255, Gen:18.02964210510254, Disc:0.0005061841802671552, Recon_One:0.0, T_Reg:5.169099807739258,T_MSE:205.16464233398438,  CI:0.8739437372981067 Validation RAE:0.22143281505259033 Loss:22.360718624434114, Ranking:103.06432756674621, Reg:3.2670208718220084, Gen:16.52045849190364, Disc:0.0006403639068918104, Recon_One:0.0, T_Reg:5.839619899490736, T_MSE:9573.998028811357, CI:0.901766740375013, *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:4000, trainCI:0.8657303218459369, train_ranking:97.79691314697266, train_RAE:0.26154085993766785,  train_Gen:18.006420135498047, train_Disc:0.0005042750271968544, train_reg:3.245508909225464, train_t_reg:4.719810485839844, train_t_mse:156.47869873046875, train_layer_one_recon:0.0\n",
      "Iteration: 4092 epochs:44, Training: RAE:0.25558024644851685, Loss: 22.784393310546875, Ranking:99.34451293945312, Reg:3.245208263397217, Gen:17.859426498413086, Disc:0.00049629807472229, Recon_One:0.0, T_Reg:4.924470901489258,T_MSE:173.4832000732422,  CI:0.8676525248234482 Validation RAE:0.2219024103247137 Loss:24.38728914002083, Ranking:103.06686128514013, Reg:3.2667158334810047, Gen:16.412682131974417, Disc:0.0006731019650842275, Recon_One:0.0, T_Reg:7.973934030836935, T_MSE:35941.30094417769, CI:0.9008044492466475, \n",
      "it:4100, trainCI:0.8721203324976909, train_ranking:100.11455535888672, train_RAE:0.24763815104961395,  train_Gen:18.43740463256836, train_Disc:0.00043777949758805335, train_reg:3.245208501815796, train_t_reg:3.8549344539642334, train_t_mse:148.1283721923828, train_layer_one_recon:0.0\n",
      "Iteration: 4185 epochs:45, Training: RAE:0.24960726499557495, Loss: 22.224742889404297, Ranking:109.09261322021484, Reg:3.24480938911438, Gen:18.2703914642334, Disc:0.0004767774953506887, Recon_One:0.0, T_Reg:3.9538755416870117,T_MSE:95.64390563964844,  CI:0.8767287299027513 Validation RAE:0.22735922282094193 Loss:25.298553916145984, Ranking:103.02235859847895, Reg:3.266314315664719, Gen:16.809980061645433, Disc:0.0005440276008379052, Recon_One:0.0, T_Reg:8.48802971485124, T_MSE:45609.420417937334, CI:0.9008901057832435, \n",
      "it:4200, trainCI:0.8835845022683423, train_ranking:101.47607421875, train_RAE:0.25616011023521423,  train_Gen:19.407581329345703, train_Disc:0.0003767251328099519, train_reg:3.244688034057617, train_t_reg:4.132862091064453, train_t_mse:204.0559539794922, train_layer_one_recon:0.0\n",
      "Iteration: 4278 epochs:46, Training: RAE:0.26526758074760437, Loss: 22.901262283325195, Ranking:109.3301010131836, Reg:3.2441694736480713, Gen:18.2280330657959, Disc:0.00047854194417595863, Recon_One:0.0, T_Reg:4.672749996185303,T_MSE:137.5940704345703,  CI:0.8403570383377231 Validation RAE:0.21218744185666405 Loss:27.438915868513252, Ranking:103.06068492916594, Reg:3.2656701591680597, Gen:16.24137164801974, Disc:0.0007506932185342536, Recon_One:0.0, T_Reg:11.196793525445845, T_MSE:113223.13913910839, CI:0.8996624439738207, \n",
      "it:4300, trainCI:0.8841762643965949, train_ranking:106.50325775146484, train_RAE:0.281434029340744,  train_Gen:19.340150833129883, train_Disc:0.00034904206404462457, train_reg:3.2439310550689697, train_t_reg:4.304055690765381, train_t_mse:126.8364486694336, train_layer_one_recon:0.0\n",
      "Iteration: 4371 epochs:47, Training: RAE:0.2542199492454529, Loss: 24.113670349121094, Ranking:102.78955841064453, Reg:3.2439606189727783, Gen:19.546527862548828, Disc:0.0003136747982352972, Recon_One:0.0, T_Reg:4.566827774047852,T_MSE:159.38722229003906,  CI:0.8763920109552231 Validation RAE:0.22777142775301726 Loss:27.288352791720722, Ranking:103.02732352451994, Reg:3.265459920311475, Gen:17.353018221295265, Disc:0.000568734472664964, Recon_One:0.0, T_Reg:9.934765450042443, T_MSE:83797.78899073198, CI:0.9008578279831038, \n",
      "it:4400, trainCI:0.9021947190392532, train_ranking:101.05269622802734, train_RAE:0.2132265567779541,  train_Gen:19.27553939819336, train_Disc:0.00031495935400016606, train_reg:3.24450421333313, train_t_reg:3.393160343170166, train_t_mse:122.26519012451172, train_layer_one_recon:0.0\n",
      "Iteration: 4464 epochs:48, Training: RAE:0.2354407012462616, Loss: 24.586700439453125, Ranking:105.63775634765625, Reg:3.2443082332611084, Gen:20.16884422302246, Disc:0.000338948069838807, Recon_One:0.0, T_Reg:4.417516708374023,T_MSE:156.19540405273438,  CI:0.8726143365225667 Validation RAE:0.22005091430739313 Loss:28.383998589410147, Ranking:102.91303506087732, Reg:3.2658098384083933, Gen:17.62378316261298, Disc:0.00037152256246966427, Recon_One:0.0, T_Reg:10.759843512298138, T_MSE:106991.2550727889, CI:0.9006727587698218, \n",
      "it:4500, trainCI:0.8389437183248867, train_ranking:99.13345336914062, train_RAE:0.2645997107028961,  train_Gen:19.727903366088867, train_Disc:0.0002895661455113441, train_reg:3.2442984580993652, train_t_reg:5.153965473175049, train_t_mse:203.03038024902344, train_layer_one_recon:0.0\n",
      "Iteration: 4557 epochs:49, Training: RAE:0.23596686124801636, Loss: 22.979299545288086, Ranking:101.06587219238281, Reg:3.2440648078918457, Gen:19.35896873474121, Disc:0.00032652594381943345, Recon_One:0.0, T_Reg:3.620004177093506,T_MSE:110.9139404296875,  CI:0.8782597300484865 Validation RAE:0.21907828046677186 Loss:28.9435583426831, Ranking:103.02872394820787, Reg:3.265564799741073, Gen:17.528589412512236, Disc:0.0003823890518590997, Recon_One:0.0, T_Reg:11.414587165290628, T_MSE:137442.03667248937, CI:0.9023819346114654, \n",
      "it:4600, trainCI:0.8732848283607598, train_ranking:108.73590087890625, train_RAE:0.2665492296218872,  train_Gen:18.75777244567871, train_Disc:0.00040599185740575194, train_reg:3.243997097015381, train_t_reg:4.091288089752197, train_t_mse:184.84429931640625, train_layer_one_recon:0.0\n",
      "Iteration: 4650 epochs:50, Training: RAE:0.23892273008823395, Loss: 24.588960647583008, Ranking:104.56756591796875, Reg:3.2438933849334717, Gen:20.155170440673828, Disc:0.00026196628459729254, Recon_One:0.0, T_Reg:4.433527946472168,T_MSE:167.6818389892578,  CI:0.8732683817932714 Validation RAE:0.22274528574061062 Loss:31.259920395239003, Ranking:103.01450734378786, Reg:3.2653922406795606, Gen:18.38795169526463, Disc:0.00028041217271190815, Recon_One:0.0, T_Reg:12.871688412803104, T_MSE:173553.34746463457, CI:0.9013197395156964, \n",
      "it:4700, trainCI:0.8574263893076073, train_ranking:106.22681427001953, train_RAE:0.25761523842811584,  train_Gen:20.46914291381836, train_Disc:0.0003016302362084389, train_reg:3.244428873062134, train_t_reg:4.973147392272949, train_t_mse:254.79336547851562, train_layer_one_recon:0.0\n",
      "Iteration: 4743 epochs:51, Training: RAE:0.24066472053527832, Loss: 24.027976989746094, Ranking:103.04782104492188, Reg:3.2446744441986084, Gen:19.924407958984375, Disc:0.0002384828112553805, Recon_One:0.0, T_Reg:4.103329658508301,T_MSE:169.0127410888672,  CI:0.8868589661117258 Validation RAE:0.22205861751863595 Loss:26.930056027327385, Ranking:102.97723817604697, Reg:3.266178476403501, Gen:17.99339734719398, Disc:0.000347342779318075, Recon_One:0.0, T_Reg:8.936311214197065, T_MSE:59311.97436343316, CI:0.9025057398722752, \n",
      "it:4800, trainCI:0.8759235223642172, train_ranking:106.68489837646484, train_RAE:0.2573268711566925,  train_Gen:18.81110382080078, train_Disc:0.0003523539053276181, train_reg:3.2446436882019043, train_t_reg:5.345661640167236, train_t_mse:202.97959899902344, train_layer_one_recon:0.0\n",
      "Iteration: 4836 epochs:52, Training: RAE:0.23897114396095276, Loss: 22.950286865234375, Ranking:106.36849212646484, Reg:3.2444992065429688, Gen:19.045568466186523, Disc:0.00032811512937769294, Recon_One:0.0, T_Reg:3.904390573501587,T_MSE:135.0722198486328,  CI:0.862184370239017 Validation RAE:0.22239829588863602 Loss:29.8184604365721, Ranking:103.06747100704146, Reg:3.2660020773628733, Gen:17.361751419015864, Disc:0.00042389583231736155, Recon_One:0.0, T_Reg:12.456284553777788, T_MSE:181344.4743055425, CI:0.9018558850757032, \n",
      "it:4900, trainCI:0.8678800307613432, train_ranking:103.55266571044922, train_RAE:0.2518356740474701,  train_Gen:19.590370178222656, train_Disc:0.0002973980735987425, train_reg:3.244379997253418, train_t_reg:4.21837854385376, train_t_mse:189.4921112060547, train_layer_one_recon:0.0\n",
      "Iteration: 4929 epochs:53, Training: RAE:0.23041826486587524, Loss: 23.554216384887695, Ranking:101.2872085571289, Reg:3.2441420555114746, Gen:19.38756561279297, Disc:0.0002561704604886472, Recon_One:0.0, T_Reg:4.1663947105407715,T_MSE:214.1003875732422,  CI:0.8800646186716694 Validation RAE:0.22748212693511835 Loss:31.151550925492018, Ranking:102.93083553450158, Reg:3.2656425593181657, Gen:17.57785367327689, Disc:0.00037427342599034223, Recon_One:0.0, T_Reg:13.57332286468607, T_MSE:192058.76603836683, CI:0.9021855657116811, \n",
      "it:5000, trainCI:0.8754379637262985, train_ranking:103.10203552246094, train_RAE:0.26137101650238037,  train_Gen:19.895296096801758, train_Disc:0.00020264438353478909, train_reg:3.2440648078918457, train_t_reg:4.386781692504883, train_t_mse:135.33709716796875, train_layer_one_recon:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5022 epochs:54, Training: RAE:0.24670320749282837, Loss: 24.600482940673828, Ranking:94.45259094238281, Reg:3.243790626525879, Gen:19.73611068725586, Disc:0.00024411105550825596, Recon_One:0.0, T_Reg:4.864129066467285,T_MSE:202.66648864746094,  CI:0.863020949409405 Validation RAE:0.22754074205111396 Loss:35.98681189081617, Ranking:102.9768358130179, Reg:3.2652888012421313, Gen:17.781667243067407, Disc:0.00034598529816493235, Recon_One:0.0, T_Reg:18.204797974851587, T_MSE:515933.3738725573, CI:0.9023646903072812, \n",
      "it:5100, trainCI:0.8878386731224889, train_ranking:103.25394439697266, train_RAE:0.25860267877578735,  train_Gen:20.925304412841797, train_Disc:0.00021150708198547363, train_reg:3.2438724040985107, train_t_reg:4.027962684631348, train_t_mse:145.8992156982422, train_layer_one_recon:0.0\n",
      "Iteration: 5115 epochs:55, Training: RAE:0.231073260307312, Loss: 24.112674713134766, Ranking:98.28901672363281, Reg:3.2437944412231445, Gen:19.991710662841797, Disc:0.00024201718042604625, Recon_One:0.0, T_Reg:4.120720863342285,T_MSE:172.130615234375,  CI:0.894750987717365 Validation RAE:0.23779669562041408 Loss:26.010077670766247, Ranking:103.02976374777016, Reg:3.2652926412212473, Gen:17.747426733637326, Disc:0.0003474273143542426, Recon_One:0.0, T_Reg:8.2623034398884, T_MSE:44030.97518099477, CI:0.903891941196235, \n",
      "it:5200, trainCI:0.8744973836408703, train_ranking:95.86582946777344, train_RAE:0.2419494241476059,  train_Gen:20.372488021850586, train_Disc:0.00019365247862879187, train_reg:3.2441985607147217, train_t_reg:4.431401252746582, train_t_mse:184.198974609375, train_layer_one_recon:0.0\n",
      "Iteration: 5208 epochs:56, Training: RAE:0.2720278203487396, Loss: 24.40582275390625, Ranking:105.27705383300781, Reg:3.2441444396972656, Gen:19.9511775970459, Disc:0.00023473988403566182, Recon_One:0.0, T_Reg:4.454411506652832,T_MSE:131.97201538085938,  CI:0.8751327299388179 Validation RAE:0.23166865047043647 Loss:28.90212605323615, Ranking:102.99145444655576, Reg:3.265644959305113, Gen:18.60398207513395, Disc:0.00022668048485305207, Recon_One:0.0, T_Reg:10.297917158466229, T_MSE:94854.26637039671, CI:0.9040725642285234, \n",
      "it:5300, trainCI:0.8837749546279492, train_ranking:109.71002197265625, train_RAE:0.2346610128879547,  train_Gen:21.398839950561523, train_Disc:0.00017017716891132295, train_reg:3.243877410888672, train_t_reg:4.112459182739258, train_t_mse:187.4193878173828, train_layer_one_recon:0.0\n",
      "Iteration: 5301 epochs:57, Training: RAE:0.2346610128879547, Loss: 25.5114688873291, Ranking:109.71002197265625, Reg:3.243877410888672, Gen:21.398839950561523, Disc:0.00017017716891132295, Recon_One:0.0, T_Reg:4.112459182739258,T_MSE:187.4193878173828,  CI:0.8837749546279492 Validation RAE:0.21557235344731987 Loss:31.39173160184245, Ranking:103.08118507506416, Reg:3.2653761607670138, Gen:19.362596328428154, Disc:0.00022611651543387042, Recon_One:0.0, T_Reg:12.02890899638407, T_MSE:140007.33403721882, CI:0.9034676379680668, \n",
      "Iteration: 5394 epochs:58, Training: RAE:0.2600522041320801, Loss: 25.41059684753418, Ranking:101.72370910644531, Reg:3.244387149810791, Gen:21.29100799560547, Disc:0.00015482459275517613, Recon_One:0.0, T_Reg:4.119434356689453,T_MSE:127.113525390625,  CI:0.8591699368486707 Validation RAE:0.21680876085336945 Loss:28.889953069005948, Ranking:102.95228265264086, Reg:3.2658892779763495, Gen:19.499044476769427, Disc:0.00018724097809556914, Recon_One:0.0, T_Reg:9.39072165412278, T_MSE:73963.22417703437, CI:0.9032510770197931, \n",
      "it:5400, trainCI:0.8957510279771023, train_ranking:98.36150360107422, train_RAE:0.26352763175964355,  train_Gen:20.75200080871582, train_Disc:0.00017883401596918702, train_reg:3.2443630695343018, train_t_reg:4.817602634429932, train_t_mse:211.20469665527344, train_layer_one_recon:0.0\n",
      "Iteration: 5487 epochs:59, Training: RAE:0.25305068492889404, Loss: 24.537416458129883, Ranking:105.17927551269531, Reg:3.2469699382781982, Gen:19.951318740844727, Disc:0.00022434344282373786, Recon_One:0.0, T_Reg:4.585873603820801,T_MSE:180.71583557128906,  CI:0.8529545339859717 Validation RAE:0.21394771423968312 Loss:24.30995304584205, Ranking:102.98274704347749, Reg:3.2684891838363757, Gen:19.035181844772602, Disc:0.00020072124594111776, Recon_One:0.0, T_Reg:5.274570509987026, T_MSE:7266.414456011519, CI:0.9025649404037339, *\n",
      "it:5500, trainCI:0.8684022918701978, train_ranking:99.84821319580078, train_RAE:0.2368217408657074,  train_Gen:21.499053955078125, train_Disc:0.0001463849621359259, train_reg:3.2470667362213135, train_t_reg:4.837053298950195, train_t_mse:240.70347595214844, train_layer_one_recon:0.0\n",
      "Iteration: 5580 epochs:60, Training: RAE:0.2368091493844986, Loss: 24.776283264160156, Ranking:100.89131164550781, Reg:3.2463715076446533, Gen:20.845291137695312, Disc:0.00018321060633752495, Recon_One:0.0, T_Reg:3.9308090209960938,T_MSE:162.71446228027344,  CI:0.9057292076739364 Validation RAE:0.2182641435902283 Loss:24.987436637768702, Ranking:103.11508374287513, Reg:3.2678867871125994, Gen:18.877659724088733, Disc:0.00020286085450664493, Recon_One:0.0, T_Reg:6.109574062609294, T_MSE:14346.613343775354, CI:0.9034205477527945, \n",
      "it:5600, trainCI:0.8827982725427962, train_ranking:102.07539367675781, train_RAE:0.2495390921831131,  train_Gen:20.81439971923828, train_Disc:0.00015341959078796208, train_reg:3.246372699737549, train_t_reg:4.674276828765869, train_t_mse:210.03985595703125, train_layer_one_recon:0.0\n",
      "Iteration: 5673 epochs:61, Training: RAE:0.2645292282104492, Loss: 26.259033203125, Ranking:111.83721160888672, Reg:3.246269941329956, Gen:21.600173950195312, Disc:0.00016235531074926257, Recon_One:0.0, T_Reg:4.658696174621582,T_MSE:217.4468231201172,  CI:0.8842571639551225 Validation RAE:0.22533232754016497 Loss:29.497888127878277, Ranking:103.06209870930297, Reg:3.267784547668644, Gen:19.212690472409058, Disc:0.00017322769663225018, Recon_One:0.0, T_Reg:10.285024887713192, T_MSE:94834.8412406762, CI:0.9042644378182427, \n",
      "it:5700, trainCI:0.892877218340962, train_ranking:98.94119262695312, train_RAE:0.24079486727714539,  train_Gen:20.350461959838867, train_Disc:0.00017358205514028668, train_reg:3.2461838722229004, train_t_reg:3.9891152381896973, train_t_mse:177.12074279785156, train_layer_one_recon:0.0\n",
      "Iteration: 5766 epochs:62, Training: RAE:0.25969719886779785, Loss: 26.571996688842773, Ranking:104.31449890136719, Reg:3.24672794342041, Gen:21.99093246459961, Disc:0.0001237889809999615, Recon_One:0.0, T_Reg:4.580939769744873,T_MSE:227.41368103027344,  CI:0.897507772284797 Validation RAE:0.2161952204945476 Loss:25.19081261689564, Ranking:103.0739034729271, Reg:3.2682455851612233, Gen:18.78006715572997, Disc:0.00022284104836347345, Recon_One:0.0, T_Reg:6.410522533682446, T_MSE:18529.04443558386, CI:0.9050381224401605, \n",
      "it:5800, trainCI:0.8924552329963354, train_ranking:102.19783020019531, train_RAE:0.2330985963344574,  train_Gen:20.401580810546875, train_Disc:0.000164042750839144, train_reg:3.2465333938598633, train_t_reg:3.686063766479492, train_t_mse:118.34229278564453, train_layer_one_recon:0.0\n",
      "Iteration: 5859 epochs:63, Training: RAE:0.2345816195011139, Loss: 24.867605209350586, Ranking:105.70130157470703, Reg:3.24676775932312, Gen:20.372528076171875, Disc:0.0001714394020382315, Recon_One:0.0, T_Reg:4.494905471801758,T_MSE:166.453857421875,  CI:0.8959937591222508 Validation RAE:0.22197069517072773 Loss:25.598211528152707, Ranking:103.07200785888885, Reg:3.2682856649432432, Gen:18.664812937339754, Disc:0.00022863150472787454, Recon_One:0.0, T_Reg:6.933170028577764, T_MSE:23935.557941342795, CI:0.9056460701385297, \n",
      "it:5900, trainCI:0.8724620124825534, train_ranking:100.88021850585938, train_RAE:0.24392251670360565,  train_Gen:21.441036224365234, train_Disc:0.0001148168375948444, train_reg:3.246952772140503, train_t_reg:4.560547828674316, train_t_mse:173.20460510253906, train_layer_one_recon:0.0\n",
      "Iteration: 5952 epochs:64, Training: RAE:0.2355651706457138, Loss: 24.886699676513672, Ranking:108.59378051757812, Reg:3.246861457824707, Gen:20.914533615112305, Disc:0.00015216904284898192, Recon_One:0.0, T_Reg:3.9720137119293213,T_MSE:202.35845947265625,  CI:0.8992542805278909 Validation RAE:0.22261024473592075 Loss:28.155762502010337, Ranking:103.00665308389571, Reg:3.2683799844302728, Gen:18.673594218277106, Disc:0.00022825641378212863, Recon_One:0.0, T_Reg:9.481939679759137, T_MSE:73296.02203235576, CI:0.9054845583151518, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:6000, trainCI:0.8708486138537687, train_ranking:103.6722412109375, train_RAE:0.24024246633052826,  train_Gen:21.492717742919922, train_Disc:0.00011837045894935727, train_reg:3.246936559677124, train_t_reg:4.1176371574401855, train_t_mse:118.13385772705078, train_layer_one_recon:0.0\n",
      "Iteration: 6045 epochs:65, Training: RAE:0.2230335772037506, Loss: 24.66299057006836, Ranking:99.35002899169922, Reg:3.2474000453948975, Gen:20.8028564453125, Disc:0.00014661331078968942, Recon_One:0.0, T_Reg:3.859987735748291,T_MSE:151.16542053222656,  CI:0.8850260278338468 Validation RAE:0.21933357973046283 Loss:28.317536397831162, Ranking:103.00539557421313, Reg:3.268922141481671, Gen:19.717128160254155, Disc:0.00013602089958136853, Recon_One:0.0, T_Reg:8.600272106143464, T_MSE:56852.31908354831, CI:0.9048124726136131, \n",
      "it:6100, trainCI:0.8986756416570897, train_ranking:96.78571319580078, train_RAE:0.2530302107334137,  train_Gen:21.4840145111084, train_Disc:0.00012070855882484466, train_reg:3.2473177909851074, train_t_reg:4.5403666496276855, train_t_mse:214.0843963623047, train_layer_one_recon:0.0\n",
      "Iteration: 6138 epochs:66, Training: RAE:0.24605116248130798, Loss: 26.22774314880371, Ranking:111.2868881225586, Reg:3.2474420070648193, Gen:21.221879959106445, Disc:0.0001508117711637169, Recon_One:0.0, T_Reg:5.005712985992432,T_MSE:403.53955078125,  CI:0.8678825303367559 Validation RAE:0.20260512875410502 Loss:28.41650417958019, Ranking:103.01611980110522, Reg:3.2689643812519438, Gen:19.536256629168218, Disc:0.00015111949188798728, Recon_One:0.0, T_Reg:8.880096632971888, T_MSE:54696.96859212105, CI:0.9016682365861545, \n",
      "it:6200, trainCI:0.8691658206848738, train_ranking:107.06692504882812, train_RAE:0.2543172538280487,  train_Gen:21.050926208496094, train_Disc:0.0001214667281601578, train_reg:3.2472290992736816, train_t_reg:3.90448260307312, train_t_mse:187.4695587158203, train_layer_one_recon:0.0\n",
      "Iteration: 6231 epochs:67, Training: RAE:0.2333071231842041, Loss: 26.31702423095703, Ranking:101.3580093383789, Reg:3.2478513717651367, Gen:21.539531707763672, Disc:0.00011310017725918442, Recon_One:0.0, T_Reg:4.777380466461182,T_MSE:154.3220672607422,  CI:0.8955755923673726 Validation RAE:0.21520408262412846 Loss:25.856773099318524, Ranking:103.13940416712664, Reg:3.2693764590107977, Gen:19.308457130698304, Disc:0.0001611847377580665, Recon_One:0.0, T_Reg:6.548154439063941, T_MSE:19060.929114105733, CI:0.9046358044716004, \n",
      "it:6300, trainCI:0.8948684579856593, train_ranking:103.83830261230469, train_RAE:0.2596248984336853,  train_Gen:21.06142807006836, train_Disc:0.00010899909102590755, train_reg:3.24765682220459, train_t_reg:4.141754627227783, train_t_mse:206.2394256591797, train_layer_one_recon:0.0\n",
      "Iteration: 6324 epochs:68, Training: RAE:0.2508755028247833, Loss: 25.591548919677734, Ranking:104.4664306640625, Reg:3.2479827404022217, Gen:21.161338806152344, Disc:0.00011036399519070983, Recon_One:0.0, T_Reg:4.430100917816162,T_MSE:100.32637023925781,  CI:0.8590978593272172 Validation RAE:0.2129343927942605 Loss:26.788791960830373, Ranking:103.11959121053452, Reg:3.269508698291595, Gen:18.628639749725178, Disc:0.0002502163433902128, Recon_One:0.0, T_Reg:8.159901892466591, T_MSE:48319.537403697715, CI:0.9031887076632067, \n",
      "it:6400, trainCI:0.8933499325656188, train_ranking:102.03408813476562, train_RAE:0.25494512915611267,  train_Gen:21.79439926147461, train_Disc:0.00012414866068866104, train_reg:3.2480342388153076, train_t_reg:3.889524221420288, train_t_mse:169.51846313476562, train_layer_one_recon:0.0\n",
      "Iteration: 6417 epochs:69, Training: RAE:0.23996493220329285, Loss: 26.004608154296875, Ranking:106.96819305419922, Reg:3.2480688095092773, Gen:21.705364227294922, Disc:0.00011573255324037746, Recon_One:0.0, T_Reg:4.29912805557251,T_MSE:194.1008758544922,  CI:0.8851654480109059 Validation RAE:0.21317859733568306 Loss:29.19118932981826, Ranking:102.96127955675334, Reg:3.269595337820393, Gen:20.037726764457144, Disc:0.00011302067831024303, Recon_One:0.0, T_Reg:9.153349609453707, T_MSE:65405.90436848846, CI:0.9053250116546441, \n",
      "it:6500, trainCI:0.860015085089332, train_ranking:113.32598876953125, train_RAE:0.2714797258377075,  train_Gen:21.816442489624023, train_Disc:8.647391223348677e-05, train_reg:3.248520851135254, train_t_reg:4.751659393310547, train_t_mse:116.21743774414062, train_layer_one_recon:0.0\n",
      "Iteration: 6510 epochs:70, Training: RAE:0.23289594054222107, Loss: 25.529712677001953, Ranking:100.52185821533203, Reg:3.2483959197998047, Gen:21.577146530151367, Disc:0.00010641651169862598, Recon_One:0.0, T_Reg:3.952460289001465,T_MSE:143.9842071533203,  CI:0.8691920920314845 Validation RAE:0.2173404068749771 Loss:28.24600919032076, Ranking:103.01808553650123, Reg:3.2699246160295647, Gen:20.180208104333833, Disc:0.0001176068307624732, Recon_One:0.0, T_Reg:8.065683350975071, T_MSE:46452.66628630298, CI:0.904668475304314, \n",
      "it:6600, trainCI:0.9033792378740114, train_ranking:95.6047592163086, train_RAE:0.23432746529579163,  train_Gen:21.695091247558594, train_Disc:9.783795394469053e-05, train_reg:3.248414993286133, train_t_reg:4.0160675048828125, train_t_mse:190.639404296875, train_layer_one_recon:0.0\n",
      "Iteration: 6603 epochs:71, Training: RAE:0.25315019488334656, Loss: 27.454011917114258, Ranking:105.3062973022461, Reg:3.2484350204467773, Gen:23.04766845703125, Disc:7.948929123813286e-05, Recon_One:0.0, T_Reg:4.40626335144043,T_MSE:163.25851440429688,  CI:0.8373287671232876 Validation RAE:0.2093578513762467 Loss:32.173114093047346, Ranking:103.04247975513401, Reg:3.2699639758155006, Gen:21.14885936487581, Disc:8.24235776060439e-05, Recon_One:0.0, T_Reg:11.024172126881641, T_MSE:123679.73981702031, CI:0.9039643328834585, \n",
      "Iteration: 6696 epochs:72, Training: RAE:0.23485691845417023, Loss: 26.454652786254883, Ranking:106.00862884521484, Reg:3.2479500770568848, Gen:21.72540855407715, Disc:0.00010822511103469878, Recon_One:0.0, T_Reg:4.729135513305664,T_MSE:279.0030517578125,  CI:0.8850790786274657 Validation RAE:0.20644433961311728 Loss:34.456507003171694, Ranking:103.07055835025048, Reg:3.2694758184704167, Gen:20.356438519791602, Disc:0.00014919280166175472, Recon_One:0.0, T_Reg:14.099919329408676, T_MSE:243070.85213703528, CI:0.903336561604638, \n",
      "it:6700, trainCI:0.8733521100092305, train_ranking:112.84931182861328, train_RAE:0.2581006586551666,  train_Gen:21.803407669067383, train_Disc:8.818897185847163e-05, train_reg:3.247823476791382, train_t_reg:4.255934238433838, train_t_mse:202.91050720214844, train_layer_one_recon:0.0\n",
      "Iteration: 6789 epochs:73, Training: RAE:0.26618215441703796, Loss: 26.697689056396484, Ranking:102.90994262695312, Reg:3.2480268478393555, Gen:22.387340545654297, Disc:7.874394941609353e-05, Recon_One:0.0, T_Reg:4.310270309448242,T_MSE:203.03549194335938,  CI:0.8664160465535443 Validation RAE:0.2135488253221432 Loss:36.39949786897091, Ranking:102.94056837895937, Reg:3.26955309805012, Gen:20.496511441462484, Disc:0.00011966230107126454, Recon_One:0.0, T_Reg:15.902866626420854, T_MSE:342219.8793292716, CI:0.9048159362131715, \n",
      "it:6800, trainCI:0.895618556701031, train_ranking:96.63007354736328, train_RAE:0.23026153445243835,  train_Gen:21.97610855102539, train_Disc:8.172039815690368e-05, train_reg:3.2479419708251953, train_t_reg:4.323205947875977, train_t_mse:174.2923126220703, train_layer_one_recon:0.0\n",
      "Iteration: 6882 epochs:74, Training: RAE:0.2444063276052475, Loss: 27.78641700744629, Ranking:102.8145751953125, Reg:3.248157501220703, Gen:23.681962966918945, Disc:5.9548754506977275e-05, Recon_One:0.0, T_Reg:4.104394435882568,T_MSE:142.4870147705078,  CI:0.8830913881283257 Validation RAE:0.21506331165060186 Loss:37.80142482525262, Ranking:103.092709019346, Reg:3.269684617334833, Gen:21.532869046816934, Disc:6.679137444263057e-05, Recon_One:0.0, T_Reg:16.26848867271369, T_MSE:349339.3261784387, CI:0.901824295082568, \n",
      "it:6900, trainCI:0.9058607473650591, train_ranking:99.77240753173828, train_RAE:0.2483251541852951,  train_Gen:22.52008056640625, train_Disc:6.0701921029249206e-05, train_reg:3.2482030391693115, train_t_reg:4.097623348236084, train_t_mse:169.9788055419922, train_layer_one_recon:0.0\n",
      "Iteration: 6975 epochs:75, Training: RAE:0.24874958395957947, Loss: 26.687227249145508, Ranking:105.44342041015625, Reg:3.2480833530426025, Gen:22.353260040283203, Disc:7.215645746327937e-05, Recon_One:0.0, T_Reg:4.333894729614258,T_MSE:190.89810180664062,  CI:0.8707876471179962 Validation RAE:0.21457790711663344 Loss:34.612958270549115, Ranking:103.10223083190803, Reg:3.2696099777407714, Gen:20.09238870021357, Disc:0.0001101558470627582, Recon_One:0.0, T_Reg:14.52046000571404, T_MSE:268760.54057528765, CI:0.9027218340943672, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:7000, trainCI:0.9013715812723256, train_ranking:97.11621856689453, train_RAE:0.23790346086025238,  train_Gen:21.773700714111328, train_Disc:9.077254799194634e-05, train_reg:3.2481467723846436, train_t_reg:5.762490272521973, train_t_mse:765.8975219726562, train_layer_one_recon:0.0\n",
      "Iteration: 7068 epochs:76, Training: RAE:0.22972385585308075, Loss: 26.477291107177734, Ranking:100.5341796875, Reg:3.248419761657715, Gen:22.14539337158203, Disc:8.539494592696428e-05, Recon_One:0.0, T_Reg:4.331812858581543,T_MSE:141.2552490234375,  CI:0.860163370367452 Validation RAE:0.2149137898551027 Loss:32.463952608670205, Ranking:102.97410876002328, Reg:3.269948615899038, Gen:20.210670575180902, Disc:0.00010265698361325412, Recon_One:0.0, T_Reg:12.253179147092345, T_MSE:183253.70439617906, CI:0.9034888617270628, \n",
      "it:7100, trainCI:0.8871330118900361, train_ranking:105.39442443847656, train_RAE:0.2467389851808548,  train_Gen:22.455089569091797, train_Disc:6.86925632180646e-05, train_reg:3.2483386993408203, train_t_reg:3.8259310722351074, train_t_mse:167.01930236816406, train_layer_one_recon:0.0\n",
      "Iteration: 7161 epochs:77, Training: RAE:0.23537369072437286, Loss: 26.015064239501953, Ranking:106.02275085449219, Reg:3.248595714569092, Gen:22.660293579101562, Disc:6.74994444125332e-05, Recon_One:0.0, T_Reg:3.354703903198242,T_MSE:128.4669952392578,  CI:0.8925367149516314 Validation RAE:0.213341535579954 Loss:29.603307158019494, Ranking:103.03987391191497, Reg:3.2701257349357493, Gen:19.79296081913133, Disc:0.0001403507197829986, Recon_One:0.0, T_Reg:9.810205804000306, T_MSE:92922.59364770296, CI:0.9044463627709327, \n",
      "it:7200, trainCI:0.8722379681351642, train_ranking:95.89274597167969, train_RAE:0.23506394028663635,  train_Gen:22.231199264526367, train_Disc:8.07191536296159e-05, train_reg:3.2487494945526123, train_t_reg:4.358661651611328, train_t_mse:187.61444091796875, train_layer_one_recon:0.0\n",
      "Iteration: 7254 epochs:78, Training: RAE:0.2509745657444, Loss: 27.207685470581055, Ranking:102.63702392578125, Reg:3.2489938735961914, Gen:22.915184020996094, Disc:5.6243436119984835e-05, Recon_One:0.0, T_Reg:4.292445182800293,T_MSE:160.45773315429688,  CI:0.865541432602449 Validation RAE:0.21818439293700753 Loss:32.9230067520361, Ranking:103.04404412923462, Reg:3.270526532755951, Gen:19.26436448702443, Disc:0.00020981122452701768, Recon_One:0.0, T_Reg:13.65843224528433, T_MSE:257784.3294252675, CI:0.9048808111524171, \n",
      "it:7300, trainCI:0.8904807641001014, train_ranking:99.31149291992188, train_RAE:0.25315219163894653,  train_Gen:22.60921859741211, train_Disc:6.344232679111883e-05, train_reg:3.2488276958465576, train_t_reg:4.034375190734863, train_t_mse:132.8433074951172, train_layer_one_recon:0.0\n",
      "Iteration: 7347 epochs:79, Training: RAE:0.23834137618541718, Loss: 27.869361877441406, Ranking:106.11711883544922, Reg:3.2490715980529785, Gen:23.807445526123047, Disc:4.873557190876454e-05, Recon_One:0.0, T_Reg:4.061867713928223,T_MSE:169.72341918945312,  CI:0.8741384477075217 Validation RAE:0.21271733549649796 Loss:34.59388284038063, Ranking:103.0044402541914, Reg:3.2706047723304335, Gen:20.677778239963917, Disc:8.741163195704973e-05, Recon_One:0.0, T_Reg:13.91601734523909, T_MSE:238370.89888131572, CI:0.9033795249778833, \n",
      "it:7400, trainCI:0.8685096272830768, train_ranking:107.08094024658203, train_RAE:0.23901616036891937,  train_Gen:23.25059700012207, train_Disc:5.5231234000530094e-05, train_reg:3.248959541320801, train_t_reg:3.732698917388916, train_t_mse:147.2735137939453, train_layer_one_recon:0.0\n",
      "Iteration: 7440 epochs:80, Training: RAE:0.2427748292684555, Loss: 26.711185455322266, Ranking:108.62229919433594, Reg:3.2490336894989014, Gen:22.951231002807617, Disc:5.213877375354059e-05, Recon_One:0.0, T_Reg:3.7599029541015625,T_MSE:130.18902587890625,  CI:0.8694200265186859 Validation RAE:0.21399230834497754 Loss:32.32028538306922, Ranking:103.04506790105799, Reg:3.270566612537971, Gen:20.419254150333384, Disc:9.947348848660788e-05, Recon_One:0.0, T_Reg:11.900931685451152, T_MSE:185813.5138454058, CI:0.9035901413084751, \n",
      "it:7500, trainCI:0.8853004477648864, train_ranking:107.3966064453125, train_RAE:0.24435043334960938,  train_Gen:23.365854263305664, train_Disc:6.0888793086633086e-05, train_reg:3.2501182556152344, train_t_reg:4.0611772537231445, train_t_mse:148.51393127441406, train_layer_one_recon:0.0\n",
      "Iteration: 7533 epochs:81, Training: RAE:0.25967279076576233, Loss: 28.251821517944336, Ranking:100.55152893066406, Reg:3.2498161792755127, Gen:23.295494079589844, Disc:4.9571368435863405e-05, Recon_One:0.0, T_Reg:4.956277847290039,T_MSE:310.76617431640625,  CI:0.8978679504814305 Validation RAE:0.2201608343617505 Loss:30.512209428732614, Ranking:103.00836137373537, Reg:3.27135428825408, Gen:20.382486016389294, Disc:0.00010129778064118097, Recon_One:0.0, T_Reg:10.129622007677431, T_MSE:93848.50611432099, CI:0.9065588391625714, \n",
      "it:7600, trainCI:0.9133045026710761, train_ranking:104.12297058105469, train_RAE:0.2464650422334671,  train_Gen:23.338821411132812, train_Disc:4.935805918648839e-05, train_reg:3.252106189727783, train_t_reg:3.5517501831054688, train_t_mse:188.4954376220703, train_layer_one_recon:0.0\n",
      "Iteration: 7626 epochs:82, Training: RAE:0.22739169001579285, Loss: 27.43889617919922, Ranking:99.65081024169922, Reg:3.251965045928955, Gen:23.524974822998047, Disc:5.8758170780492947e-05, Recon_One:0.0, T_Reg:3.913862466812134,T_MSE:168.44488525390625,  CI:0.8858493572718581 Validation RAE:0.21062030385803757 Loss:26.081813071926966, Ranking:103.08817884572315, Reg:3.273517396489695, Gen:21.315753872847427, Disc:7.160288535204479e-05, Recon_One:0.0, T_Reg:4.765987606425426, T_MSE:4477.970313858923, CI:0.9046664364478364, *\n",
      "it:7700, trainCI:0.8840330104054539, train_ranking:111.41433715820312, train_RAE:0.27435103058815,  train_Gen:23.06314468383789, train_Disc:4.842552880290896e-05, train_reg:3.2518200874328613, train_t_reg:4.6003618240356445, train_t_mse:264.81390380859375, train_layer_one_recon:0.0\n",
      "Iteration: 7719 epochs:83, Training: RAE:0.2400124967098236, Loss: 27.466676712036133, Ranking:100.8626708984375, Reg:3.25191068649292, Gen:23.118192672729492, Disc:4.653762880479917e-05, Recon_One:0.0, T_Reg:4.348437309265137,T_MSE:187.4920196533203,  CI:0.869876348329387 Validation RAE:0.20976610937326629 Loss:26.99657890727196, Ranking:103.0099891909692, Reg:3.273462676787296, Gen:20.00645756408455, Disc:0.0001440436230172103, Recon_One:0.0, T_Reg:6.989977270927491, T_MSE:27598.920349039046, CI:0.9060183456797147, \n",
      "it:7800, trainCI:0.903590232028997, train_ranking:100.13142395019531, train_RAE:0.24540606141090393,  train_Gen:23.43927764892578, train_Disc:4.5957902329973876e-05, train_reg:3.2517545223236084, train_t_reg:3.7212305068969727, train_t_mse:124.3105697631836, train_layer_one_recon:0.0\n",
      "Iteration: 7812 epochs:84, Training: RAE:0.2583789527416229, Loss: 28.222232818603516, Ranking:106.7450942993164, Reg:3.2519757747650146, Gen:23.741161346435547, Disc:4.0724673453951254e-05, Recon_One:0.0, T_Reg:4.481030464172363,T_MSE:213.87355041503906,  CI:0.8852516193323369 Validation RAE:0.21671337571996174 Loss:28.571626563750165, Ranking:102.979381552216, Reg:3.2735281964309575, Gen:20.05127780032304, Disc:0.00013792722540578798, Recon_One:0.0, T_Reg:8.520211118958333, T_MSE:56911.70416657024, CI:0.9072817243242997, \n",
      "it:7900, trainCI:0.8822815840683363, train_ranking:104.96174621582031, train_RAE:0.23785823583602905,  train_Gen:24.138813018798828, train_Disc:3.625976387411356e-05, train_reg:3.2522597312927246, train_t_reg:3.9409337043762207, train_t_mse:208.97061157226562, train_layer_one_recon:0.0\n",
      "Iteration: 7905 epochs:85, Training: RAE:0.2449723184108734, Loss: 27.845108032226562, Ranking:105.63140106201172, Reg:3.252351999282837, Gen:23.644241333007812, Disc:4.315506521379575e-05, Recon_One:0.0, T_Reg:4.200823783874512,T_MSE:185.9627227783203,  CI:0.8904395493864413 Validation RAE:0.21634704379926878 Loss:29.110618804534763, Ranking:103.00952772565286, Reg:3.273906914371244, Gen:20.83807507947011, Disc:7.884340558617241e-05, Recon_One:0.0, T_Reg:8.272465068101138, T_MSE:53869.16735750355, CI:0.9069905608805737, \n",
      "Iteration: 7998 epochs:86, Training: RAE:0.23135414719581604, Loss: 27.2718563079834, Ranking:102.48099517822266, Reg:3.252674102783203, Gen:23.11630630493164, Disc:5.833164323121309e-05, Recon_One:0.0, T_Reg:4.155491828918457,T_MSE:179.13023376464844,  CI:0.8970633934120572 Validation RAE:0.21184412828904323 Loss:30.499622811969406, Ranking:103.00902118232081, Reg:3.2742311526078263, Gen:21.60770932152612, Disc:5.072070341720161e-05, Recon_One:0.0, T_Reg:8.891862871588149, T_MSE:68984.35545235973, CI:0.9075413714684979, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:8000, trainCI:0.9038852527503133, train_ranking:94.90543365478516, train_RAE:0.20688454806804657,  train_Gen:23.73615264892578, train_Disc:4.355429337010719e-05, train_reg:3.2526166439056396, train_t_reg:3.918276071548462, train_t_mse:173.5854949951172, train_layer_one_recon:0.0\n",
      "Iteration: 8091 epochs:87, Training: RAE:0.24123388528823853, Loss: 27.526012420654297, Ranking:102.59442138671875, Reg:3.2529795169830322, Gen:23.493593215942383, Disc:3.996302984887734e-05, Recon_One:0.0, T_Reg:4.032380104064941,T_MSE:189.41534423828125,  CI:0.8683768632617738 Validation RAE:0.2023245768016676 Loss:32.59686288407881, Ranking:102.92198288003937, Reg:3.274538590935777, Gen:20.56912886913052, Disc:0.00010002045307300108, Recon_One:0.0, T_Reg:12.027634144187108, T_MSE:159920.8724411276, CI:0.9065257261682119, \n",
      "it:8100, trainCI:0.8770594043968609, train_ranking:102.02788543701172, train_RAE:0.22110721468925476,  train_Gen:23.425994873046875, train_Disc:4.302442539483309e-05, train_reg:3.2527661323547363, train_t_reg:3.9655613899230957, train_t_mse:113.65274047851562, train_layer_one_recon:0.0\n",
      "Iteration: 8184 epochs:88, Training: RAE:0.23266777396202087, Loss: 28.987302780151367, Ranking:100.42487335205078, Reg:3.252744436264038, Gen:23.959548950195312, Disc:4.0131282730726525e-05, Recon_One:0.0, T_Reg:5.027713775634766,T_MSE:380.4217529296875,  CI:0.8921501526797936 Validation RAE:0.20906435384204183 Loss:34.128488307150064, Ranking:102.86949236899507, Reg:3.2743019522227717, Gen:21.129825423177337, Disc:6.597285684730009e-05, Recon_One:0.0, T_Reg:12.998596832158045, T_MSE:230640.23726748215, CI:0.9077493839582871, \n",
      "it:8200, trainCI:0.896278912222975, train_ranking:109.81880187988281, train_RAE:0.24386920034885406,  train_Gen:24.11488151550293, train_Disc:3.57518729288131e-05, train_reg:3.252516984939575, train_t_reg:4.045915126800537, train_t_mse:144.56190490722656, train_layer_one_recon:0.0\n",
      "Iteration: 8277 epochs:89, Training: RAE:0.2393203228712082, Loss: 27.520801544189453, Ranking:104.43221282958984, Reg:3.253164768218994, Gen:23.547000885009766, Disc:3.7153324228711426e-05, Recon_One:0.0, T_Reg:3.973764181137085,T_MSE:137.1575469970703,  CI:0.8772403258655804 Validation RAE:0.20872027136555937 Loss:30.628841448325105, Ranking:102.98438654760427, Reg:3.2747250699215833, Gen:21.08226461053953, Disc:7.250558371619308e-05, Recon_One:0.0, T_Reg:9.546504097311619, T_MSE:101655.22087562736, CI:0.9070070928132176, \n",
      "it:8300, trainCI:0.9040352661919294, train_ranking:101.82555389404297, train_RAE:0.2271251082420349,  train_Gen:25.146499633789062, train_Disc:2.9252663807710633e-05, train_reg:3.253309965133667, train_t_reg:3.5805563926696777, train_t_mse:141.9416961669922, train_layer_one_recon:0.0\n",
      "Iteration: 8370 epochs:90, Training: RAE:0.25873082876205444, Loss: 28.5548095703125, Ranking:107.50210571289062, Reg:3.253526210784912, Gen:23.610116958618164, Disc:4.2376435885671526e-05, Recon_One:0.0, T_Reg:4.9446492195129395,T_MSE:224.9593505859375,  CI:0.8507166592272976 Validation RAE:0.20907409139777833 Loss:34.04942054934571, Ranking:103.01494176229477, Reg:3.2750889079427963, Gen:21.087367775828685, Disc:7.25533943639463e-05, Recon_One:0.0, T_Reg:12.96197993913054, T_MSE:247000.6485889774, CI:0.9058173586471858, \n",
      "it:8400, trainCI:0.8780125876272834, train_ranking:103.46133422851562, train_RAE:0.22926665842533112,  train_Gen:23.93316078186035, train_Disc:3.9100264984881505e-05, train_reg:3.253781318664551, train_t_reg:4.205761432647705, train_t_mse:148.3878936767578, train_layer_one_recon:0.0\n",
      "Iteration: 8463 epochs:91, Training: RAE:0.23686634004116058, Loss: 28.117786407470703, Ranking:100.17935180664062, Reg:3.2537195682525635, Gen:24.315582275390625, Disc:4.810863174498081e-05, Recon_One:0.0, T_Reg:3.802156686782837,T_MSE:156.4990234375,  CI:0.9076979975696096 Validation RAE:0.20794697078866065 Loss:32.005546420041185, Ranking:102.97354044311416, Reg:3.2752835468842236, Gen:21.55450607696682, Disc:5.359339255333601e-05, Recon_One:0.0, T_Reg:10.450986750322356, T_MSE:125883.43244263779, CI:0.9074419096627402, \n",
      "it:8500, trainCI:0.8884003967509315, train_ranking:98.96964263916016, train_RAE:0.24106565117835999,  train_Gen:25.599788665771484, train_Disc:2.727128958213143e-05, train_reg:3.253986358642578, train_t_reg:3.847109794616699, train_t_mse:156.87921142578125, train_layer_one_recon:0.0\n",
      "Iteration: 8556 epochs:92, Training: RAE:0.23256565630435944, Loss: 28.176376342773438, Ranking:97.62452697753906, Reg:3.254208564758301, Gen:24.494585037231445, Disc:2.5459827156737447e-05, Recon_One:0.0, T_Reg:3.681765556335449,T_MSE:201.78778076171875,  CI:0.9002060402320664 Validation RAE:0.2159534935483459 Loss:30.94729893025986, Ranking:103.01718564574325, Reg:3.275775784207118, Gen:21.957778596752835, Disc:4.240120457011126e-05, Recon_One:0.0, T_Reg:8.989477874607507, T_MSE:72103.39910209783, CI:0.9085388635767723, \n",
      "it:8600, trainCI:0.9077281893054104, train_ranking:109.50897979736328, train_RAE:0.23466968536376953,  train_Gen:24.540985107421875, train_Disc:2.4806886358419433e-05, train_reg:3.255267858505249, train_t_reg:3.4661407470703125, train_t_mse:136.17111206054688, train_layer_one_recon:0.0\n",
      "Iteration: 8649 epochs:93, Training: RAE:0.23240379989147186, Loss: 27.611541748046875, Ranking:99.428466796875, Reg:3.2554049491882324, Gen:24.301834106445312, Disc:3.018616007466335e-05, Recon_One:0.0, T_Reg:3.309678316116333,T_MSE:123.03917694091797,  CI:0.8916254848822998 Validation RAE:0.20812590416333102 Loss:28.2264998425122, Ranking:102.91717656183141, Reg:3.2769800976572805, Gen:21.677774164815894, Disc:5.163788852958262e-05, Recon_One:0.0, T_Reg:6.548674140585293, T_MSE:28048.554915112974, CI:0.9073311236059441, \n",
      "it:8700, trainCI:0.9107420141969832, train_ranking:95.16136932373047, train_RAE:0.208168163895607,  train_Gen:24.8277587890625, train_Disc:3.5099459637422115e-05, train_reg:3.2553746700286865, train_t_reg:3.1528537273406982, train_t_mse:108.04761505126953, train_layer_one_recon:0.0\n",
      "Iteration: 8742 epochs:94, Training: RAE:0.22736990451812744, Loss: 28.081031799316406, Ranking:98.52091217041016, Reg:3.2550101280212402, Gen:24.342021942138672, Disc:3.9522717997897416e-05, Recon_One:0.0, T_Reg:3.7389702796936035,T_MSE:104.94927978515625,  CI:0.8895431690462746 Validation RAE:0.2045159501009499 Loss:33.57422588854503, Ranking:103.03348685795625, Reg:3.276582659818805, Gen:22.739016420918436, Disc:2.9117760339909817e-05, Recon_One:0.0, T_Reg:10.835180073003851, T_MSE:152966.49356711816, CI:0.9075847524388531, \n",
      "it:8800, trainCI:0.8693940353894194, train_ranking:118.23529815673828, train_RAE:0.2433629035949707,  train_Gen:25.874910354614258, train_Disc:2.402874088147655e-05, train_reg:3.255148410797119, train_t_reg:4.098377704620361, train_t_mse:186.94107055664062, train_layer_one_recon:0.0\n",
      "Iteration: 8835 epochs:95, Training: RAE:0.24318963289260864, Loss: 27.945056915283203, Ranking:97.14327239990234, Reg:3.2549991607666016, Gen:24.60940933227539, Disc:2.7565369236981496e-05, Recon_One:0.0, T_Reg:3.3356199264526367,T_MSE:133.4075469970703,  CI:0.9110368619781055 Validation RAE:0.20739727790868176 Loss:36.98496303961428, Ranking:102.97229395250208, Reg:3.2765716198788475, Gen:21.974569823692125, Disc:4.4309780987334684e-05, Recon_One:0.0, T_Reg:15.010348179645355, T_MSE:361239.1963691134, CI:0.9073163357553474, \n",
      "it:8900, trainCI:0.881407187137745, train_ranking:105.49891662597656, train_RAE:0.24332445859909058,  train_Gen:26.1785831451416, train_Disc:2.095050149364397e-05, train_reg:3.2552502155303955, train_t_reg:4.264334201812744, train_t_mse:197.6800537109375, train_layer_one_recon:0.0\n",
      "Iteration: 8928 epochs:96, Training: RAE:0.24785937368869781, Loss: 28.888301849365234, Ranking:110.1137924194336, Reg:3.255540609359741, Gen:24.888225555419922, Disc:2.702723941183649e-05, Recon_One:0.0, T_Reg:4.000049114227295,T_MSE:154.1140899658203,  CI:0.8756466663443407 Validation RAE:0.2065465673180063 Loss:34.65281959114871, Ranking:103.00230555971429, Reg:3.2771166569145826, Gen:22.240468593214366, Disc:3.790922402206851e-05, Recon_One:0.0, T_Reg:12.412313078140578, T_MSE:223111.7329396053, CI:0.9079084638926133, \n",
      "it:9000, trainCI:0.8919812326722115, train_ranking:99.46543884277344, train_RAE:0.2536591589450836,  train_Gen:24.62591552734375, train_Disc:3.107784141320735e-05, train_reg:3.2561402320861816, train_t_reg:4.696982383728027, train_t_mse:166.681396484375, train_layer_one_recon:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9021 epochs:97, Training: RAE:0.2495632767677307, Loss: 29.284194946289062, Ranking:105.20092010498047, Reg:3.256786346435547, Gen:24.70645523071289, Disc:2.2231528419069946e-05, Recon_One:0.0, T_Reg:4.577718257904053,T_MSE:171.70465087890625,  CI:0.857857377421881 Validation RAE:0.2070220073409508 Loss:30.58508647237999, Ranking:103.0331529467288, Reg:3.2783706500945544, Gen:21.845830182992206, Disc:5.2165328287976325e-05, Recon_One:0.0, T_Reg:8.739203938368158, T_MSE:67478.27368957245, CI:0.907544491164554, \n",
      "it:9100, trainCI:0.8979140409989969, train_ranking:103.36186218261719, train_RAE:0.23099224269390106,  train_Gen:24.469871520996094, train_Disc:3.0302533559734002e-05, train_reg:3.2572240829467773, train_t_reg:3.6782002449035645, train_t_mse:131.38291931152344, train_layer_one_recon:0.0\n",
      "Iteration: 9114 epochs:98, Training: RAE:0.2370024472475052, Loss: 29.38186264038086, Ranking:100.9814453125, Reg:3.2573742866516113, Gen:25.1121826171875, Disc:2.1090145310154185e-05, Recon_One:0.0, T_Reg:4.269658088684082,T_MSE:150.2737579345703,  CI:0.8954220915581689 Validation RAE:0.20543943703495085 Loss:35.63894809924678, Ranking:103.03507427193156, Reg:3.2789624868757623, Gen:22.429282453278685, Disc:3.50959508544016e-05, Recon_One:0.0, T_Reg:13.209630679201748, T_MSE:262460.40861791145, CI:0.9075605318064804, \n",
      "it:9200, trainCI:0.8871883105210446, train_ranking:103.4177017211914, train_RAE:0.23911936581134796,  train_Gen:25.399444580078125, train_Disc:1.930791768245399e-05, train_reg:3.257852792739868, train_t_reg:3.81361985206604, train_t_mse:108.2986068725586, train_layer_one_recon:0.0\n",
      "Iteration: 9207 epochs:99, Training: RAE:0.23838584125041962, Loss: 28.47433853149414, Ranking:110.57459259033203, Reg:3.257963180541992, Gen:24.916336059570312, Disc:2.2001273464411497e-05, Recon_One:0.0, T_Reg:3.5579795837402344,T_MSE:131.08148193359375,  CI:0.878396886037627 Validation RAE:0.20378177387648497 Loss:31.282083669483356, Ranking:103.0824262230966, Reg:3.279555283651749, Gen:22.84806339654592, Disc:2.826453592862454e-05, Recon_One:0.0, T_Reg:8.433991681788584, T_MSE:66686.23245380455, CI:0.906688760992814, \n",
      "Iteration: 9300 epochs:100, Training: RAE:0.23892956972122192, Loss: 28.91966438293457, Ranking:102.58431243896484, Reg:3.2582855224609375, Gen:25.05415153503418, Disc:1.9885348592652008e-05, Recon_One:0.0, T_Reg:3.865492343902588,T_MSE:153.15023803710938,  CI:0.9051521340123568 Validation RAE:0.20683154424637784 Loss:32.33871085503347, Ranking:103.06902135687052, Reg:3.279879761887026, Gen:22.437557691750833, Disc:3.657102902422544e-05, Recon_One:0.0, T_Reg:9.901116516525423, T_MSE:104143.78840341647, CI:0.907962726952361, \n",
      "it:9300, trainCI:0.8952270680560916, train_ranking:103.34671783447266, train_RAE:0.23794126510620117,  train_Gen:25.643709182739258, train_Disc:1.769336813595146e-05, train_reg:3.2583019733428955, train_t_reg:3.8822994232177734, train_t_mse:113.54298400878906, train_layer_one_recon:0.0\n",
      "Iteration: 9393 epochs:101, Training: RAE:0.2193983495235443, Loss: 29.000873565673828, Ranking:100.03941345214844, Reg:3.2581989765167236, Gen:25.0330810546875, Disc:3.3989996154559776e-05, Recon_One:0.0, T_Reg:3.9677584171295166,T_MSE:144.8949737548828,  CI:0.8932805280528053 Validation RAE:0.20225448479156904 Loss:42.76449690763095, Ranking:102.98204048732019, Reg:3.2797926423608383, Gen:22.35967949269428, Disc:3.855293034412109e-05, Recon_One:0.0, T_Reg:20.404778827320445, T_MSE:713659.9569863761, CI:0.9077180150459747, \n",
      "it:9400, trainCI:0.9115788947986116, train_ranking:106.4363021850586, train_RAE:0.24017499387264252,  train_Gen:25.047725677490234, train_Disc:2.02182745852042e-05, train_reg:3.258167266845703, train_t_reg:3.8443996906280518, train_t_mse:176.74688720703125, train_layer_one_recon:0.0\n",
      "Iteration: 9486 epochs:102, Training: RAE:0.21524293720722198, Loss: 31.152265548706055, Ranking:100.52081298828125, Reg:3.2585721015930176, Gen:26.674571990966797, Disc:1.4907905097061303e-05, Recon_One:0.0, T_Reg:4.477678298950195,T_MSE:201.4728240966797,  CI:0.8818100546907867 Validation RAE:0.20722895469369776 Loss:40.651451314525694, Ranking:102.97728893055354, Reg:3.280168240318093, Gen:23.95493846185896, Disc:1.6514526776365243e-05, Recon_One:0.0, T_Reg:16.696496522320768, T_MSE:464885.97988249344, CI:0.9094961926688911, \n",
      "it:9500, trainCI:0.9012102078400421, train_ranking:100.54956817626953, train_RAE:0.22809988260269165,  train_Gen:26.51828384399414, train_Disc:1.43838333315216e-05, train_reg:3.2587411403656006, train_t_reg:4.128673076629639, train_t_mse:295.4269104003906, train_layer_one_recon:0.0\n",
      "Iteration: 9579 epochs:103, Training: RAE:0.24854792654514313, Loss: 29.915996551513672, Ranking:100.76863861083984, Reg:3.259058952331543, Gen:25.6528263092041, Disc:1.8521943275118247e-05, Recon_One:0.0, T_Reg:4.263152599334717,T_MSE:152.70152282714844,  CI:0.8865334034718569 Validation RAE:0.21045867191100995 Loss:38.01570342334491, Ranking:102.98559263495784, Reg:3.280658317652735, Gen:22.836511153287553, Disc:3.077001183071207e-05, Recon_One:0.0, T_Reg:15.179161383268816, T_MSE:308733.2982890797, CI:0.9094131891021696, \n",
      "it:9600, trainCI:0.887987464301059, train_ranking:105.1316146850586, train_RAE:0.2577579915523529,  train_Gen:25.840835571289062, train_Disc:1.784712731023319e-05, train_reg:3.2592532634735107, train_t_reg:4.212887763977051, train_t_mse:194.72384643554688, train_layer_one_recon:0.0\n",
      "Iteration: 9672 epochs:104, Training: RAE:0.21905508637428284, Loss: 29.11290168762207, Ranking:97.70891571044922, Reg:3.2593135833740234, Gen:25.524431228637695, Disc:1.9716853785212152e-05, Recon_One:0.0, T_Reg:3.5884499549865723,T_MSE:163.4138641357422,  CI:0.8982977690253311 Validation RAE:0.21139487290629838 Loss:33.99167970163875, Ranking:102.94926242558854, Reg:3.280914636258708, Gen:22.252472534885673, Disc:4.677153767131082e-05, Recon_One:0.0, T_Reg:11.739160617441032, T_MSE:174295.37447969572, CI:0.9091466393233905, \n",
      "it:9700, trainCI:0.8946676110492765, train_ranking:105.27839660644531, train_RAE:0.24879087507724762,  train_Gen:27.160743713378906, train_Disc:1.264842376258457e-05, train_reg:3.2593908309936523, train_t_reg:4.821351528167725, train_t_mse:231.4764404296875, train_layer_one_recon:0.0\n",
      "Iteration: 9765 epochs:105, Training: RAE:0.2276451140642166, Loss: 28.916427612304688, Ranking:103.93354797363281, Reg:3.259377956390381, Gen:25.19859504699707, Disc:2.0299885363783687e-05, Recon_One:0.0, T_Reg:3.7178115844726562,T_MSE:175.86512756347656,  CI:0.8987958569314761 Validation RAE:0.21006056233705753 Loss:37.900649301257744, Ranking:102.89016848611014, Reg:3.2809794359062856, Gen:22.520723458571897, Disc:3.916793734875374e-05, Recon_One:0.0, T_Reg:15.379886608116625, T_MSE:341035.7729103858, CI:0.9093178541384389, \n",
      "it:9800, trainCI:0.8427256697021667, train_ranking:110.21086883544922, train_RAE:0.25427043437957764,  train_Gen:26.110382080078125, train_Disc:1.3363982361624949e-05, train_reg:3.2593472003936768, train_t_reg:5.3737311363220215, train_t_mse:228.17916870117188, train_layer_one_recon:0.0\n",
      "Iteration: 9858 epochs:106, Training: RAE:0.25734731554985046, Loss: 31.017276763916016, Ranking:103.63522338867188, Reg:3.259356737136841, Gen:26.878829956054688, Disc:1.3447811397782061e-05, Recon_One:0.0, T_Reg:4.138434410095215,T_MSE:147.74838256835938,  CI:0.886051331917398 Validation RAE:0.20895907202332828 Loss:35.94544938881576, Ranking:103.06378796620265, Reg:3.2809580760224546, Gen:25.07531881719973, Disc:1.056722132923077e-05, Recon_One:0.0, T_Reg:10.870119896891476, T_MSE:146031.38008203538, CI:0.9087500448916893, \n",
      "it:9900, trainCI:0.8741362530413626, train_ranking:109.55860137939453, train_RAE:0.23784947395324707,  train_Gen:25.690675735473633, train_Disc:1.7277263395953923e-05, train_reg:3.2606587409973145, train_t_reg:3.063441276550293, train_t_mse:111.67664337158203, train_layer_one_recon:0.0\n",
      "Iteration: 9951 epochs:107, Training: RAE:0.23567067086696625, Loss: 31.365198135375977, Ranking:94.84514617919922, Reg:3.2608089447021484, Gen:27.14029312133789, Disc:1.6067684555309825e-05, Recon_One:0.0, T_Reg:4.224888324737549,T_MSE:197.55335998535156,  CI:0.8956550820768652 Validation RAE:0.20576815633411868 Loss:36.00444315492592, Ranking:102.97384430233114, Reg:3.2824199080720637, Gen:25.004585817543823, Disc:1.0684262703447298e-05, Recon_One:0.0, T_Reg:10.999847038307681, T_MSE:134884.43745513758, CI:0.9092758979111645, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:10000, trainCI:0.8962710941752858, train_ranking:96.68494415283203, train_RAE:0.22201856970787048,  train_Gen:25.60824203491211, train_Disc:1.918268208100926e-05, train_reg:3.260953426361084, train_t_reg:3.1763916015625, train_t_mse:131.49984741210938, train_layer_one_recon:0.0\n",
      "Iteration: 10044 epochs:108, Training: RAE:0.2447056621313095, Loss: 30.55759620666504, Ranking:99.35292053222656, Reg:3.2613844871520996, Gen:26.22403335571289, Disc:1.3716808098251931e-05, Recon_One:0.0, T_Reg:4.3335490226745605,T_MSE:205.98397827148438,  CI:0.8975165209976551 Validation RAE:0.20585262804861976 Loss:38.26135598645742, Ranking:102.96955187350224, Reg:3.2829992649211457, Gen:23.91167967799784, Disc:1.6119252941066325e-05, Recon_One:0.0, T_Reg:14.349660280720657, T_MSE:341447.8112249743, CI:0.9089733856518645, \n",
      "it:10100, trainCI:0.8708977880584068, train_ranking:110.50247192382812, train_RAE:0.25372105836868286,  train_Gen:26.207962036132812, train_Disc:1.4066499716136605e-05, train_reg:3.2615807056427, train_t_reg:4.188736915588379, train_t_mse:178.7497100830078, train_layer_one_recon:0.0\n",
      "Iteration: 10137 epochs:109, Training: RAE:0.24771490693092346, Loss: 30.216684341430664, Ranking:103.3465576171875, Reg:3.261276960372925, Gen:25.88744354248047, Disc:1.5055185940582305e-05, Recon_One:0.0, T_Reg:4.329225063323975,T_MSE:172.07144165039062,  CI:0.8586612161471641 Validation RAE:0.20141091416206183 Loss:36.81722215015888, Ranking:103.01739968284005, Reg:3.2828910255098216, Gen:23.42919899073633, Disc:2.193790110505484e-05, Recon_One:0.0, T_Reg:13.388001142150985, T_MSE:215528.71986475977, CI:0.9073949668346831, \n",
      "it:10200, trainCI:0.9113367809008334, train_ranking:98.9864730834961, train_RAE:0.21429292857646942,  train_Gen:26.13135528564453, train_Disc:1.3362394383875653e-05, train_reg:3.2612595558166504, train_t_reg:3.422031879425049, train_t_mse:138.57058715820312, train_layer_one_recon:0.0\n",
      "Iteration: 10230 epochs:110, Training: RAE:0.2043648660182953, Loss: 30.74199104309082, Ranking:101.98860168457031, Reg:3.261686325073242, Gen:26.820030212402344, Disc:1.0248588296235539e-05, Recon_One:0.0, T_Reg:3.9219512939453125,T_MSE:189.8939208984375,  CI:0.888254972640751 Validation RAE:0.2022021285545771 Loss:31.196211051296945, Ranking:102.9886305593052, Reg:3.2833031032686755, Gen:23.114989193526004, Disc:2.888084241774036e-05, Recon_One:0.0, T_Reg:8.081193318795723, T_MSE:50745.1388445529, CI:0.909450453503092, \n",
      "it:10300, trainCI:0.9032626904565639, train_ranking:99.34895324707031, train_RAE:0.19015581905841827,  train_Gen:26.802181243896484, train_Disc:1.012825669022277e-05, train_reg:3.261597156524658, train_t_reg:3.3366341590881348, train_t_mse:144.7261505126953, train_layer_one_recon:0.0\n",
      "Iteration: 10323 epochs:111, Training: RAE:0.22647283971309662, Loss: 30.433027267456055, Ranking:101.16031646728516, Reg:3.261690616607666, Gen:26.093265533447266, Disc:1.4112878488958813e-05, Recon_One:0.0, T_Reg:4.339747428894043,T_MSE:219.2588653564453,  CI:0.8707976613071623 Validation RAE:0.20257996932572092 Loss:37.92146065241995, Ranking:102.94602682579453, Reg:3.283307423245181, Gen:23.705126950930726, Disc:1.9413125593469485e-05, Recon_One:0.0, T_Reg:14.216314814635421, T_MSE:295795.55639262433, CI:0.9086298997467249, \n",
      "it:10400, trainCI:0.8859005075095963, train_ranking:102.6893310546875, train_RAE:0.2644931972026825,  train_Gen:26.565217971801758, train_Disc:1.1732078746717889e-05, train_reg:3.262225389480591, train_t_reg:3.811923027038574, train_t_mse:176.90773010253906, train_layer_one_recon:0.0\n",
      "Iteration: 10416 epochs:112, Training: RAE:0.2533056139945984, Loss: 31.273883819580078, Ranking:104.40145111083984, Reg:3.2625479698181152, Gen:26.797868728637695, Disc:9.599371878721286e-06, Recon_One:0.0, T_Reg:4.476006507873535,T_MSE:246.3163604736328,  CI:0.8954826607179072 Validation RAE:0.21565791689501385 Loss:33.498251233918374, Ranking:102.95907674438584, Reg:3.284170458551435, Gen:23.849294709461667, Disc:1.7692077134778127e-05, Recon_One:0.0, T_Reg:9.648938879335287, T_MSE:90107.15774974033, CI:0.9092099175678043, \n",
      "it:10500, trainCI:0.9074576271186441, train_ranking:101.64905548095703, train_RAE:0.21708473563194275,  train_Gen:27.537616729736328, train_Disc:1.0848062629520427e-05, train_reg:3.263246536254883, train_t_reg:3.825227975845337, train_t_mse:190.68124389648438, train_layer_one_recon:0.0\n",
      "Iteration: 10509 epochs:113, Training: RAE:0.23822256922721863, Loss: 31.761302947998047, Ranking:105.34419250488281, Reg:3.2632298469543457, Gen:27.569169998168945, Disc:8.161479854607023e-06, Recon_One:0.0, T_Reg:4.192123889923096,T_MSE:201.17030334472656,  CI:0.8740524340796333 Validation RAE:0.20456970263469573 Loss:37.74285729682429, Ranking:102.9741331355429, Reg:3.284856854818367, Gen:23.788694496436225, Disc:1.911073110400053e-05, Recon_One:0.0, T_Reg:13.95414387179536, T_MSE:313500.99382346845, CI:0.9084648260656452, \n",
      "it:10600, trainCI:0.9004263658271762, train_ranking:99.9115982055664, train_RAE:0.24864515662193298,  train_Gen:27.243724822998047, train_Disc:1.0129610018339008e-05, train_reg:3.2631099224090576, train_t_reg:3.837590217590332, train_t_mse:165.72181701660156, train_layer_one_recon:0.0\n",
      "Iteration: 10602 epochs:114, Training: RAE:0.23027537763118744, Loss: 30.279279708862305, Ranking:108.9288558959961, Reg:3.2631046772003174, Gen:26.77387237548828, Disc:9.78787738858955e-06, Recon_One:0.0, T_Reg:3.5053975582122803,T_MSE:116.25444030761719,  CI:0.8870746969104419 Validation RAE:0.20507699661483253 Loss:38.12877528143031, Ranking:103.03955669624888, Reg:3.2847308555036334, Gen:23.875632124482593, Disc:1.822643553548602e-05, Recon_One:0.0, T_Reg:14.253124927481636, T_MSE:340380.5740325439, CI:0.9092581623162628, \n",
      "Iteration: 10695 epochs:115, Training: RAE:0.23356127738952637, Loss: 30.32859992980957, Ranking:106.7520751953125, Reg:3.263505458831787, Gen:26.943119049072266, Disc:1.0225334335700609e-05, Recon_One:0.0, T_Reg:3.3854706287384033,T_MSE:157.76303100585938,  CI:0.8828286347010376 Validation RAE:0.203265074295328 Loss:35.698272020083095, Ranking:103.01889460340536, Reg:3.285134293309477, Gen:24.109613738895373, Disc:1.5815812089145333e-05, Recon_One:0.0, T_Reg:11.588643036053242, T_MSE:186970.8688374145, CI:0.908599243205953, \n",
      "it:10700, trainCI:0.893078055964654, train_ranking:108.45150756835938, train_RAE:0.25615769624710083,  train_Gen:28.193614959716797, train_Disc:8.406677807215601e-06, train_reg:3.2634775638580322, train_t_reg:4.371297836303711, train_t_mse:183.09808349609375, train_layer_one_recon:0.0\n",
      "Iteration: 10788 epochs:116, Training: RAE:0.23962454497814178, Loss: 32.48884201049805, Ranking:105.70157623291016, Reg:3.2634429931640625, Gen:28.260780334472656, Disc:7.672697392990813e-06, Recon_One:0.0, T_Reg:4.228055000305176,T_MSE:201.14369201660156,  CI:0.8626021149701799 Validation RAE:0.19601710588839316 Loss:42.53635790492052, Ranking:102.99756235072833, Reg:3.285071413651457, Gen:25.99927138519001, Disc:6.345095217501804e-06, Recon_One:0.0, T_Reg:16.537080624110402, T_MSE:458280.135482458, CI:0.9076912888309427, \n",
      "it:10800, trainCI:0.9028900763785916, train_ranking:94.42095947265625, train_RAE:0.23224031925201416,  train_Gen:27.48006820678711, train_Disc:7.511870535381604e-06, train_reg:3.263390064239502, train_t_reg:3.82792067527771, train_t_mse:187.72654724121094, train_layer_one_recon:0.0\n",
      "Iteration: 10881 epochs:117, Training: RAE:0.2509976923465729, Loss: 30.88374900817871, Ranking:106.25578308105469, Reg:3.2634265422821045, Gen:27.286296844482422, Disc:8.14232862467179e-06, Recon_One:0.0, T_Reg:3.5974442958831787,T_MSE:142.7407684326172,  CI:0.9056948406145223 Validation RAE:0.20778261863903597 Loss:40.254631712688, Ranking:103.00693590670537, Reg:3.2850548537415207, Gen:24.14583367408179, Disc:1.6360199639303626e-05, Recon_One:0.0, T_Reg:16.108781426522647, T_MSE:445611.726009087, CI:0.9102448214642164, \n",
      "it:10900, trainCI:0.8849402270619784, train_ranking:105.89384460449219, train_RAE:0.22138717770576477,  train_Gen:27.340496063232422, train_Disc:7.924305464257486e-06, train_reg:3.2633697986602783, train_t_reg:3.72892165184021, train_t_mse:143.32595825195312, train_layer_one_recon:0.0\n",
      "Iteration: 10974 epochs:118, Training: RAE:0.23006905615329742, Loss: 30.91071319580078, Ranking:112.8847427368164, Reg:3.2634730339050293, Gen:27.64638328552246, Disc:7.951946827233769e-06, Recon_One:0.0, T_Reg:3.264322280883789,T_MSE:125.84583282470703,  CI:0.8862064896755162 Validation RAE:0.20342775297579324 Loss:39.82191014263024, Ranking:102.9217010589634, Reg:3.2851016534869935, Gen:24.2361568280633, Disc:1.5824649228477093e-05, Recon_One:0.0, T_Reg:15.585737745955957, T_MSE:383038.8564149514, CI:0.9106999286203716, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:11000, trainCI:0.9217069299947099, train_ranking:105.58412170410156, train_RAE:0.20953984558582306,  train_Gen:26.859268188476562, train_Disc:1.0588895747787319e-05, train_reg:3.2633426189422607, train_t_reg:3.011533737182617, train_t_mse:178.178466796875, train_layer_one_recon:0.0\n",
      "Iteration: 11067 epochs:119, Training: RAE:0.22172005474567413, Loss: 31.291397094726562, Ranking:103.40496063232422, Reg:3.2638344764709473, Gen:27.633007049560547, Disc:6.161356395750772e-06, Recon_One:0.0, T_Reg:3.658384323120117,T_MSE:200.17941284179688,  CI:0.9064127896533142 Validation RAE:0.20166251692411763 Loss:38.62092612131664, Ranking:103.06562948662206, Reg:3.2854654915082064, Gen:24.69974632067607, Disc:1.1549062263014125e-05, Recon_One:0.0, T_Reg:13.921167936357271, T_MSE:335537.36150526203, CI:0.9100667285791229, \n",
      "it:11100, trainCI:0.8884513441878077, train_ranking:104.81214904785156, train_RAE:0.2243395298719406,  train_Gen:27.302719116210938, train_Disc:7.79490073909983e-06, train_reg:3.2639007568359375, train_t_reg:3.644148349761963, train_t_mse:169.73744201660156, train_layer_one_recon:0.0\n",
      "Iteration: 11160 epochs:120, Training: RAE:0.20803368091583252, Loss: 31.467500686645508, Ranking:103.49700164794922, Reg:3.2641899585723877, Gen:28.303895950317383, Disc:7.041580829536542e-06, Recon_One:0.0, T_Reg:3.1635982990264893,T_MSE:162.67356872558594,  CI:0.911892472019216 Validation RAE:0.2087395710432039 Loss:38.57740663626469, Ranking:102.91641557814405, Reg:3.2858233295620507, Gen:25.24244530999662, Disc:8.374628578678958e-06, Recon_One:0.0, T_Reg:13.334953061965193, T_MSE:254208.02655293373, CI:0.9106253261249194, \n",
      "it:11200, trainCI:0.9141837543899399, train_ranking:93.14913940429688, train_RAE:0.18925046920776367,  train_Gen:27.326656341552734, train_Disc:7.568746696051676e-06, train_reg:3.264153003692627, train_t_reg:3.305328369140625, train_t_mse:222.39109802246094, train_layer_one_recon:0.0\n",
      "Iteration: 11253 epochs:121, Training: RAE:0.23260582983493805, Loss: 31.48749351501465, Ranking:114.61126708984375, Reg:3.2649590969085693, Gen:27.869224548339844, Disc:6.48798140900908e-06, Recon_One:0.0, T_Reg:3.61826229095459,T_MSE:152.82086181640625,  CI:0.8834511299172324 Validation RAE:0.20354817232087477 Loss:35.70889081450512, Ranking:102.94021610261441, Reg:3.2865975653512547, Gen:23.90903560194326, Disc:2.183634786151125e-05, Recon_One:0.0, T_Reg:11.79983394659056, T_MSE:186268.61865005599, CI:0.9110330974204437, \n",
      "it:11300, trainCI:0.8963295484552416, train_ranking:100.36125946044922, train_RAE:0.2205839604139328,  train_Gen:27.64632797241211, train_Disc:9.349088031740393e-06, train_reg:3.2653143405914307, train_t_reg:4.542405605316162, train_t_mse:232.49960327148438, train_layer_one_recon:0.0\n",
      "Iteration: 11346 epochs:122, Training: RAE:0.24216313660144806, Loss: 32.524627685546875, Ranking:102.37178802490234, Reg:3.2657432556152344, Gen:28.50189781188965, Disc:4.670037924370263e-06, Recon_One:0.0, T_Reg:4.022726058959961,T_MSE:183.4822998046875,  CI:0.8808200408469274 Validation RAE:0.20717174696135227 Loss:37.0738643995893, Ranking:102.91240029563393, Reg:3.2873869210582263, Gen:23.928457381532418, Disc:2.1872889702727165e-05, Recon_One:0.0, T_Reg:13.145385679042501, T_MSE:245556.318477322, CI:0.9106646539468553, \n",
      "it:11400, trainCI:0.8691831839414216, train_ranking:105.2739028930664, train_RAE:0.2323913425207138,  train_Gen:28.259403228759766, train_Disc:5.604038051387761e-06, train_reg:3.265589475631714, train_t_reg:4.031524658203125, train_t_mse:135.35040283203125, train_layer_one_recon:0.0\n",
      "Iteration: 11439 epochs:123, Training: RAE:0.23524300754070282, Loss: 31.646530151367188, Ranking:104.3425064086914, Reg:3.2658886909484863, Gen:27.803749084472656, Disc:6.665934506600024e-06, Recon_One:0.0, T_Reg:3.8427743911743164,T_MSE:190.5022735595703,  CI:0.9026665649864003 Validation RAE:0.2098432813448594 Loss:36.814644438960755, Ranking:103.01564898627451, Reg:3.2875333202620127, Gen:24.86441313989374, Disc:1.1033214052693897e-05, Recon_One:0.0, T_Reg:11.950219810433845, T_MSE:192552.32419720796, CI:0.9093719698109715, \n",
      "it:11500, trainCI:0.9080374818529761, train_ranking:100.13542938232422, train_RAE:0.2459651380777359,  train_Gen:28.497610092163086, train_Disc:5.2764362408197485e-06, train_reg:3.2661333084106445, train_t_reg:3.646353006362915, train_t_mse:168.31996154785156, train_layer_one_recon:0.0\n",
      "Iteration: 11532 epochs:124, Training: RAE:0.23313164710998535, Loss: 32.63754653930664, Ranking:99.43070983886719, Reg:3.267258644104004, Gen:28.510028839111328, Disc:6.053353445167886e-06, Recon_One:0.0, T_Reg:4.127511501312256,T_MSE:211.10052490234375,  CI:0.8941398363617044 Validation RAE:0.20350601863709036 Loss:32.364585803004374, Ranking:102.96105950925444, Reg:3.2889123527619395, Gen:24.971310478516692, Disc:1.1248006665295336e-05, Recon_One:0.0, T_Reg:7.393264138461561, T_MSE:47916.24378205777, CI:0.9103832917529434, \n",
      "it:11600, trainCI:0.8907738954663587, train_ranking:110.7348403930664, train_RAE:0.2366807758808136,  train_Gen:27.58997344970703, train_Disc:6.144992312329123e-06, train_reg:3.2677359580993652, train_t_reg:3.3874175548553467, train_t_mse:121.49524688720703, train_layer_one_recon:0.0\n",
      "Iteration: 11625 epochs:125, Training: RAE:0.21244412660598755, Loss: 32.77583694458008, Ranking:102.43170166015625, Reg:3.268251895904541, Gen:29.35531234741211, Disc:5.354526365408674e-06, Recon_One:0.0, T_Reg:3.420518398284912,T_MSE:166.58474731445312,  CI:0.9118800639933943 Validation RAE:0.20050321170743202 Loss:34.1343636421527, Ranking:102.9170850701551, Reg:3.289912187324191, Gen:26.142073780711897, Disc:5.691823557589766e-06, Recon_One:0.0, T_Reg:7.99228398669015, T_MSE:60454.05019774681, CI:0.910062970205134, \n",
      "it:11700, trainCI:0.8894966015293118, train_ranking:99.52582550048828, train_RAE:0.22979995608329773,  train_Gen:28.008831024169922, train_Disc:5.425591552921105e-06, train_reg:3.2687315940856934, train_t_reg:4.0234198570251465, train_t_mse:199.1195831298828, train_layer_one_recon:0.0\n",
      "Iteration: 11718 epochs:126, Training: RAE:0.23950257897377014, Loss: 31.480667114257812, Ranking:107.25270080566406, Reg:3.2689507007598877, Gen:28.0487060546875, Disc:5.2420227802940644e-06, Recon_One:0.0, T_Reg:3.4319558143615723,T_MSE:128.2621307373047,  CI:0.885549935535059 Validation RAE:0.19751254427696863 Loss:32.49364917063931, Ranking:103.0783197828214, Reg:3.2906156234984487, Gen:25.01402047927549, Disc:1.081823089202467e-05, Recon_One:0.0, T_Reg:7.479618381854667, T_MSE:49610.899765169204, CI:0.9082768091080583, \n",
      "it:11800, trainCI:0.8987465285596337, train_ranking:106.2147216796875, train_RAE:0.21762484312057495,  train_Gen:29.294544219970703, train_Disc:4.04526053898735e-06, train_reg:3.2691197395324707, train_t_reg:3.56962251663208, train_t_mse:162.47740173339844, train_layer_one_recon:0.0\n",
      "Iteration: 11811 epochs:127, Training: RAE:0.21596010029315948, Loss: 32.87880325317383, Ranking:99.65880584716797, Reg:3.2692532539367676, Gen:28.670574188232422, Disc:6.408468834706582e-06, Recon_One:0.0, T_Reg:4.208221435546875,T_MSE:216.36251831054688,  CI:0.8846623163032522 Validation RAE:0.19711904750401577 Loss:38.98417437182407, Ranking:102.94337590455979, Reg:3.290920181842063, Gen:25.52416087001864, Disc:7.617802761290298e-06, Recon_One:0.0, T_Reg:13.460005874020823, T_MSE:284408.64610203326, CI:0.9103898996130938, \n",
      "it:11900, trainCI:0.8590471974224649, train_ranking:108.1116943359375, train_RAE:0.24617387354373932,  train_Gen:29.52893829345703, train_Disc:4.32852357334923e-06, train_reg:3.2695162296295166, train_t_reg:4.0255126953125, train_t_mse:151.2753448486328, train_layer_one_recon:0.0\n",
      "Iteration: 11904 epochs:128, Training: RAE:0.2494252622127533, Loss: 32.91960144042969, Ranking:113.43130493164062, Reg:3.269479513168335, Gen:28.874284744262695, Disc:4.959008492733119e-06, Recon_One:0.0, T_Reg:4.045313835144043,T_MSE:176.0611114501953,  CI:0.8792855966303503 Validation RAE:0.19408494291848744 Loss:37.94279749637636, Ranking:102.96730999352113, Reg:3.2911479406033632, Gen:25.900995384026455, Disc:6.262214629760268e-06, Recon_One:0.0, T_Reg:12.041795987165822, T_MSE:222325.65826305348, CI:0.9093478474367422, \n",
      "Iteration: 11997 epochs:129, Training: RAE:0.2294030785560608, Loss: 32.23847961425781, Ranking:103.06634521484375, Reg:3.2696313858032227, Gen:28.22242546081543, Disc:5.724727998313028e-06, Recon_One:0.0, T_Reg:4.016048431396484,T_MSE:246.31002807617188,  CI:0.8906863877975503 Validation RAE:0.19686938690531025 Loss:42.097463151045226, Ranking:102.9880625763073, Reg:3.2913008197719074, Gen:26.768889265595874, Disc:4.1647519372583535e-06, Recon_One:0.0, T_Reg:15.328569607211751, T_MSE:389421.78685149056, CI:0.9099181131370794, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:12000, trainCI:0.9032289532728515, train_ranking:109.22297668457031, train_RAE:0.22280724346637726,  train_Gen:28.54633331298828, train_Disc:5.091725597594632e-06, train_reg:3.2695975303649902, train_t_reg:3.5947439670562744, train_t_mse:141.218017578125, train_layer_one_recon:0.0\n",
      "Iteration: 12090 epochs:130, Training: RAE:0.23990032076835632, Loss: 32.868072509765625, Ranking:105.98748779296875, Reg:3.269906520843506, Gen:28.69440269470215, Disc:4.079576683579944e-06, Recon_One:0.0, T_Reg:4.173666000366211,T_MSE:169.67149353027344,  CI:0.8752532199574841 Validation RAE:0.198784661271415 Loss:42.14086117588819, Ranking:103.01750185967565, Reg:3.2915777782656273, Gen:25.58333845425952, Disc:8.014561175811307e-06, Recon_One:0.0, T_Reg:16.55751384167697, T_MSE:503104.99117074796, CI:0.9091821842068015, \n",
      "it:12100, trainCI:0.8993510724244872, train_ranking:101.81364440917969, train_RAE:0.23483215272426605,  train_Gen:28.567028045654297, train_Disc:4.688890840043314e-06, train_reg:3.2698042392730713, train_t_reg:3.8616201877593994, train_t_mse:154.7189483642578, train_layer_one_recon:0.0\n",
      "Iteration: 12183 epochs:131, Training: RAE:0.20538094639778137, Loss: 32.6910514831543, Ranking:109.65250396728516, Reg:3.2693183422088623, Gen:28.891494750976562, Disc:3.610261046560481e-06, Recon_One:0.0, T_Reg:3.7995543479919434,T_MSE:172.98550415039062,  CI:0.8825336914180639 Validation RAE:0.192224239559938 Loss:58.6235122351523, Ranking:102.9927563664316, Reg:3.2909857014857247, Gen:26.94281945008553, Disc:3.8152879253416e-06, Recon_One:0.0, T_Reg:31.680688663350054, T_MSE:2301954.839136339, CI:0.9084004669816528, \n",
      "it:12200, trainCI:0.9122246002769734, train_ranking:105.41706848144531, train_RAE:0.2272731214761734,  train_Gen:29.00933074951172, train_Disc:4.373464435047936e-06, train_reg:3.2693538665771484, train_t_reg:3.278287410736084, train_t_mse:156.6289825439453, train_layer_one_recon:0.0\n",
      "Iteration: 12276 epochs:132, Training: RAE:0.24708014726638794, Loss: 32.9913215637207, Ranking:103.02348327636719, Reg:3.2695865631103516, Gen:28.90392303466797, Disc:4.244584488333203e-06, Recon_One:0.0, T_Reg:4.087394714355469,T_MSE:149.0452880859375,  CI:0.8901576524445131 Validation RAE:0.19886100107362095 Loss:45.8249557550451, Ranking:103.05166031042158, Reg:3.2912557000172975, Gen:24.94485126937555, Disc:1.3868327165588324e-05, Recon_One:0.0, T_Reg:20.88009131704552, T_MSE:884220.3931796307, CI:0.9094619005768096, \n",
      "it:12300, trainCI:0.899123957404396, train_ranking:101.16117095947266, train_RAE:0.21686455607414246,  train_Gen:28.51687240600586, train_Disc:4.220276423438918e-06, train_reg:3.269641876220703, train_t_reg:5.507141590118408, train_t_mse:1035.2816162109375, train_layer_one_recon:0.0\n",
      "Iteration: 12369 epochs:133, Training: RAE:0.24785487353801727, Loss: 32.47190856933594, Ranking:108.05146789550781, Reg:3.2695422172546387, Gen:28.899181365966797, Disc:4.17442151956493e-06, Recon_One:0.0, T_Reg:3.572721004486084,T_MSE:153.66064453125,  CI:0.8834836610971463 Validation RAE:0.20338073149239613 Loss:44.673188829535285, Ranking:103.03741599136968, Reg:3.2912110602600775, Gen:24.88429638479924, Disc:1.547697379896411e-05, Recon_One:0.0, T_Reg:19.788876844820894, T_MSE:805100.7981495335, CI:0.9091635397240725, \n",
      "it:12400, trainCI:0.90252503153072, train_ranking:103.15898895263672, train_RAE:0.2229381501674652,  train_Gen:29.21239471435547, train_Disc:3.2493028356839204e-06, train_reg:3.269808769226074, train_t_reg:3.5372989177703857, train_t_mse:202.73500061035156, train_layer_one_recon:0.0\n",
      "Iteration: 12462 epochs:134, Training: RAE:0.2400316745042801, Loss: 32.66584777832031, Ranking:112.33126831054688, Reg:3.2698750495910645, Gen:28.9025821685791, Disc:3.967318662034813e-06, Recon_One:0.0, T_Reg:3.763262987136841,T_MSE:179.4884490966797,  CI:0.904690725318528 Validation RAE:0.20225263262772927 Loss:42.55001721150192, Ranking:103.028385028312, Reg:3.291546098437923, Gen:26.765938992945717, Disc:3.947989259847032e-06, Recon_One:0.0, T_Reg:15.784074263795102, T_MSE:426479.55696098326, CI:0.90986338335115, \n",
      "it:12500, trainCI:0.9095396046172217, train_ranking:99.6180419921875, train_RAE:0.21578042209148407,  train_Gen:29.40953826904297, train_Disc:2.4771748030616436e-06, train_reg:3.270143985748291, train_t_reg:3.0979409217834473, train_t_mse:118.75858306884766, train_layer_one_recon:0.0\n",
      "Iteration: 12555 epochs:135, Training: RAE:0.21815645694732666, Loss: 32.33608627319336, Ranking:101.73973846435547, Reg:3.2701706886291504, Gen:29.44532012939453, Disc:2.862954261217965e-06, Recon_One:0.0, T_Reg:2.8907630443573,T_MSE:125.85558319091797,  CI:0.9100587084148728 Validation RAE:0.20400573178889142 Loss:44.46081085414965, Ranking:103.03728008950011, Reg:3.29184369681939, Gen:25.33722250584112, Disc:1.1695725442946172e-05, Recon_One:0.0, T_Reg:19.123577151847687, T_MSE:785653.6887638865, CI:0.9106331130827918, \n",
      "it:12600, trainCI:0.9081831344550402, train_ranking:106.37908935546875, train_RAE:0.23416946828365326,  train_Gen:29.204654693603516, train_Disc:4.698811608250253e-06, train_reg:3.270355463027954, train_t_reg:3.5444278717041016, train_t_mse:159.439208984375, train_layer_one_recon:0.0\n",
      "Iteration: 12648 epochs:136, Training: RAE:0.22096578776836395, Loss: 33.20107650756836, Ranking:101.4190444946289, Reg:3.271057605743408, Gen:29.830995559692383, Disc:2.6589070785121294e-06, Recon_One:0.0, T_Reg:3.370077133178711,T_MSE:146.74351501464844,  CI:0.8948939594527945 Validation RAE:0.21016401677440522 Loss:42.486862577586464, Ranking:103.00296970914569, Reg:3.2927364919637907, Gen:26.18497653172674, Disc:6.002276972477957e-06, Recon_One:0.0, T_Reg:16.30187983719784, T_MSE:544200.3858476843, CI:0.9100638299638897, \n",
      "it:12700, trainCI:0.8877737226277372, train_ranking:95.27542877197266, train_RAE:0.2263372838497162,  train_Gen:28.93337059020996, train_Disc:4.07844072469743e-06, train_reg:3.2716429233551025, train_t_reg:3.8277816772460938, train_t_mse:144.18185424804688, train_layer_one_recon:0.0\n",
      "Iteration: 12741 epochs:137, Training: RAE:0.21425487101078033, Loss: 32.47706604003906, Ranking:107.11389923095703, Reg:3.2719945907592773, Gen:29.392730712890625, Disc:2.9145912776584737e-06, Recon_One:0.0, T_Reg:3.084331750869751,T_MSE:123.15412139892578,  CI:0.909808594622775 Validation RAE:0.19326203043749737 Loss:48.74683751960956, Ranking:102.96690395746855, Reg:3.2936796868340856, Gen:26.212129191009495, Disc:5.934789818552612e-06, Recon_One:0.0, T_Reg:22.534702839934262, T_MSE:1180610.6338322095, CI:0.9110800402485008, \n",
      "it:12800, trainCI:0.9065202150035055, train_ranking:102.15489196777344, train_RAE:0.21861742436885834,  train_Gen:29.8117618560791, train_Disc:2.320252406207146e-06, train_reg:3.2725274562835693, train_t_reg:4.121158599853516, train_t_mse:158.66983032226562, train_layer_one_recon:0.0\n",
      "Iteration: 12834 epochs:138, Training: RAE:0.21301105618476868, Loss: 32.55735397338867, Ranking:101.67646789550781, Reg:3.2723641395568848, Gen:29.526269912719727, Disc:3.4759711979859276e-06, Recon_One:0.0, T_Reg:3.031080722808838,T_MSE:138.4233856201172,  CI:0.8934974369649502 Validation RAE:0.20113730456317172 Loss:56.619725818259575, Ranking:102.98798010023413, Reg:3.294051684810919, Gen:27.53817665736437, Disc:2.8159413481337453e-06, Recon_One:0.0, T_Reg:29.08154708438476, T_MSE:2071699.3764936707, CI:0.9100773895876926, \n",
      "it:12900, trainCI:0.8909171532037543, train_ranking:107.64863586425781, train_RAE:0.22602911293506622,  train_Gen:30.740814208984375, train_Disc:2.279218278999906e-06, train_reg:3.2731385231018066, train_t_reg:3.4665727615356445, train_t_mse:172.71304321289062, train_layer_one_recon:0.0\n",
      "Iteration: 12927 epochs:139, Training: RAE:0.22319215536117554, Loss: 33.38978576660156, Ranking:97.17549133300781, Reg:3.273062229156494, Gen:29.560760498046875, Disc:2.7108324047731003e-06, Recon_One:0.0, T_Reg:3.8290224075317383,T_MSE:188.29290771484375,  CI:0.8970079769132341 Validation RAE:0.20165534761528292 Loss:42.67692125870673, Ranking:102.94285032828778, Reg:3.2947544009890932, Gen:25.75489761860084, Disc:9.258187629353848e-06, Recon_One:0.0, T_Reg:16.922013987984467, T_MSE:518429.6379322616, CI:0.910488084062986, \n",
      "it:13000, trainCI:0.872471416007036, train_ranking:108.75347137451172, train_RAE:0.2508469521999359,  train_Gen:30.07642364501953, train_Disc:2.3669583697483176e-06, train_reg:3.2732443809509277, train_t_reg:3.768646240234375, train_t_mse:132.38575744628906, train_layer_one_recon:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13020 epochs:140, Training: RAE:0.22981026768684387, Loss: 33.991432189941406, Ranking:100.280517578125, Reg:3.273334503173828, Gen:30.113502502441406, Disc:2.156350547011243e-06, Recon_One:0.0, T_Reg:3.877929210662842,T_MSE:188.017333984375,  CI:0.8596468107538219 Validation RAE:0.19591139233289726 Loss:50.985060922351494, Ranking:102.98362823520674, Reg:3.2950284794984763, Gen:25.664655521569795, Disc:1.0338710250881735e-05, Recon_One:0.0, T_Reg:25.32039572779083, T_MSE:1318866.4045671828, CI:0.9099010899137181, \n",
      "it:13100, trainCI:0.8893808668370315, train_ranking:104.93358612060547, train_RAE:0.24529553949832916,  train_Gen:29.705354690551758, train_Disc:2.71396652351541e-06, train_reg:3.2734084129333496, train_t_reg:3.870999813079834, train_t_mse:141.92872619628906, train_layer_one_recon:0.0\n",
      "Iteration: 13113 epochs:141, Training: RAE:0.21264898777008057, Loss: 33.0339241027832, Ranking:98.30445861816406, Reg:3.273545742034912, Gen:29.85329818725586, Disc:2.911281171691371e-06, Recon_One:0.0, T_Reg:3.180624008178711,T_MSE:177.35977172851562,  CI:0.9156516115123423 Validation RAE:0.19775212101747322 Loss:46.67510083575271, Ranking:102.94871280770816, Reg:3.2952411183420085, Gen:26.545584379203085, Disc:5.261752548259196e-06, Recon_One:0.0, T_Reg:20.129510988753275, T_MSE:752280.117995527, CI:0.9114038499604045, \n",
      "it:13200, trainCI:0.9020316987554515, train_ranking:99.4319076538086, train_RAE:0.20698261260986328,  train_Gen:29.850494384765625, train_Disc:2.620315171952825e-06, train_reg:3.2748751640319824, train_t_reg:3.226799488067627, train_t_mse:137.73440551757812, train_layer_one_recon:0.0\n",
      "Iteration: 13206 epochs:142, Training: RAE:0.23520991206169128, Loss: 33.844146728515625, Ranking:98.01200866699219, Reg:3.2749409675598145, Gen:30.03203773498535, Disc:2.4847049644449726e-06, Recon_One:0.0, T_Reg:3.8121066093444824,T_MSE:213.6250457763672,  CI:0.883785025264125 Validation RAE:0.19728898744933737 Loss:36.86985482390827, Ranking:103.03033239859052, Reg:3.296645590703577, Gen:27.154407308029207, Disc:3.4599138185085006e-06, Recon_One:0.0, T_Reg:9.715444354157128, T_MSE:130299.93465443142, CI:0.910885906721481, \n",
      "Iteration: 13299 epochs:143, Training: RAE:0.22727546095848083, Loss: 33.19733428955078, Ranking:100.64923858642578, Reg:3.2748947143554688, Gen:30.042377471923828, Disc:2.033633563769399e-06, Recon_One:0.0, T_Reg:3.1549551486968994,T_MSE:152.2546844482422,  CI:0.8989601932570108 Validation RAE:0.19253084897935369 Loss:43.23646381555147, Ranking:102.997789410363, Reg:3.2965990309567994, Gen:26.033767169753478, Disc:8.586054895156604e-06, Recon_One:0.0, T_Reg:17.202688454538194, T_MSE:697125.9513900774, CI:0.9090174544292241, \n",
      "it:13300, trainCI:0.8690548014077426, train_ranking:105.39324188232422, train_RAE:0.21731700003147125,  train_Gen:30.549270629882812, train_Disc:2.46659510594327e-06, train_reg:3.2748968601226807, train_t_reg:4.417007923126221, train_t_mse:206.74563598632812, train_layer_one_recon:0.0\n",
      "Iteration: 13392 epochs:144, Training: RAE:0.2125721126794815, Loss: 33.94552993774414, Ranking:102.19041442871094, Reg:3.2754735946655273, Gen:30.38896942138672, Disc:1.8787595763569698e-06, Recon_One:0.0, T_Reg:3.5565600395202637,T_MSE:189.24961853027344,  CI:0.8997815117307392 Validation RAE:0.19585876153218115 Loss:42.7842931686617, Ranking:102.95453655342617, Reg:3.297181747787607, Gen:26.223091663562375, Disc:7.364134541452416e-06, Recon_One:0.0, T_Reg:16.561193772967464, T_MSE:510032.5068414308, CI:0.9097115008258352, \n",
      "it:13400, trainCI:0.9020316987554515, train_ranking:99.60231018066406, train_RAE:0.21152912080287933,  train_Gen:30.436410903930664, train_Disc:2.1115247363923118e-06, train_reg:3.2755002975463867, train_t_reg:3.2753260135650635, train_t_mse:142.42027282714844, train_layer_one_recon:0.0\n",
      "Iteration: 13485 epochs:145, Training: RAE:0.21478252112865448, Loss: 35.09244918823242, Ranking:102.60961151123047, Reg:3.2757039070129395, Gen:31.63688087463379, Disc:1.8697580799198477e-06, Recon_One:0.0, T_Reg:3.4555656909942627,T_MSE:184.0899658203125,  CI:0.8883693293376046 Validation RAE:0.19949484740896226 Loss:53.70654402724859, Ranking:102.9346915413562, Reg:3.297413586526718, Gen:28.297057538773696, Disc:1.7971131217474716e-06, Recon_One:0.0, T_Reg:25.40948496500492, T_MSE:1504231.8434243144, CI:0.9117242452018215, \n",
      "it:13500, trainCI:0.8941955168425973, train_ranking:107.857421875, train_RAE:0.2203313708305359,  train_Gen:30.815937042236328, train_Disc:1.4916859072400257e-06, train_reg:3.275667667388916, train_t_reg:3.456861972808838, train_t_mse:173.35067749023438, train_layer_one_recon:0.0\n",
      "Iteration: 13578 epochs:146, Training: RAE:0.20850472152233124, Loss: 35.68226623535156, Ranking:100.03849792480469, Reg:3.2762410640716553, Gen:30.62505531311035, Disc:1.974283122763154e-06, Recon_One:0.0, T_Reg:5.05720853805542,T_MSE:603.8201293945312,  CI:0.8895615921824797 Validation RAE:0.19269928958500357 Loss:44.81230985377451, Ranking:103.00138530037142, Reg:3.2979543035859478, Gen:27.08519786182636, Disc:3.775852854329778e-06, Recon_One:0.0, T_Reg:17.727108308489925, T_MSE:619587.3574850186, CI:0.9114187360691447, \n",
      "it:13600, trainCI:0.8742868996992014, train_ranking:102.10582733154297, train_RAE:0.22917048633098602,  train_Gen:30.367996215820312, train_Disc:1.934076408360852e-06, train_reg:3.276447296142578, train_t_reg:3.7388906478881836, train_t_mse:159.79226684570312, train_layer_one_recon:0.0\n",
      "Iteration: 13671 epochs:147, Training: RAE:0.21758638322353363, Loss: 34.13178634643555, Ranking:105.98492431640625, Reg:3.276859998703003, Gen:30.999603271484375, Disc:1.691828174443799e-06, Recon_One:0.0, T_Reg:3.1321802139282227,T_MSE:154.9873046875,  CI:0.905199619295697 Validation RAE:0.19580741681142705 Loss:41.20448377112441, Ranking:103.0120010061146, Reg:3.2985773401974705, Gen:26.58198028560637, Disc:6.00405207082951e-06, Recon_One:0.0, T_Reg:14.622497725549364, T_MSE:381801.95312046836, CI:0.9111996695382117, \n",
      "it:13700, trainCI:0.871206684437154, train_ranking:103.43718719482422, train_RAE:0.2366403490304947,  train_Gen:30.727256774902344, train_Disc:1.8352882307226537e-06, train_reg:3.2771480083465576, train_t_reg:3.643364429473877, train_t_mse:174.3710174560547, train_layer_one_recon:0.0\n",
      "Iteration: 13764 epochs:148, Training: RAE:0.19578595459461212, Loss: 35.138694763183594, Ranking:100.38487243652344, Reg:3.2776410579681396, Gen:30.858596801757812, Disc:1.743659140629461e-06, Recon_One:0.0, T_Reg:4.280094146728516,T_MSE:458.6122741699219,  CI:0.8911076525363371 Validation RAE:0.1955849895428699 Loss:46.32792932563563, Ranking:102.94847573073667, Reg:3.299363575921411, Gen:26.843940238289584, Disc:5.19200345701362e-06, Recon_One:0.0, T_Reg:19.483983859548392, T_MSE:789527.7766991067, CI:0.9115780125197579, \n",
      "it:13800, trainCI:0.8919108968080565, train_ranking:105.39167022705078, train_RAE:0.22967121005058289,  train_Gen:31.090240478515625, train_Disc:1.4340710094984388e-06, train_reg:3.2777059078216553, train_t_reg:3.4282078742980957, train_t_mse:135.7902069091797, train_layer_one_recon:0.0\n",
      "Iteration: 13857 epochs:149, Training: RAE:0.19137266278266907, Loss: 34.29837417602539, Ranking:97.69877624511719, Reg:3.2779548168182373, Gen:31.089765548706055, Disc:1.4384709174919408e-06, Recon_One:0.0, T_Reg:3.208606719970703,T_MSE:152.17042541503906,  CI:0.9113657244771118 Validation RAE:0.19704103945001328 Loss:53.79896873501788, Ranking:103.01890194945236, Reg:3.299679414203678, Gen:26.77424528516202, Disc:5.4699178536849695e-06, Recon_One:0.0, T_Reg:27.024718169884576, T_MSE:1758988.816086012, CI:0.9124435685058478, \n",
      "it:13900, trainCI:0.9071204132715557, train_ranking:99.25125885009766, train_RAE:0.19690336287021637,  train_Gen:31.155237197875977, train_Disc:1.779932858880784e-06, train_reg:3.278174877166748, train_t_reg:3.3245790004730225, train_t_mse:185.2027130126953, train_layer_one_recon:0.0\n",
      "Iteration: 13950 epochs:150, Training: RAE:0.21052028238773346, Loss: 34.4801025390625, Ranking:91.10670471191406, Reg:3.278280258178711, Gen:30.945125579833984, Disc:1.3188565617383574e-06, Recon_One:0.0, T_Reg:3.534973621368408,T_MSE:153.2893524169922,  CI:0.8910215827338129 Validation RAE:0.20616046941785704 Loss:57.47002708594382, Ranking:102.9546026678492, Reg:3.300007012421986, Gen:27.497494666445505, Disc:3.238092452125872e-06, Recon_One:0.0, T_Reg:29.972528109956535, T_MSE:1948694.0275931398, CI:0.9125784769368731, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:14000, trainCI:0.89300121301024, train_ranking:92.745361328125, train_RAE:0.21059131622314453,  train_Gen:31.148151397705078, train_Disc:1.430120619261288e-06, train_reg:3.280116319656372, train_t_reg:3.433284282684326, train_t_mse:151.9996795654297, train_layer_one_recon:0.0\n",
      "Iteration: 14043 epochs:151, Training: RAE:0.22267396748065948, Loss: 34.678741455078125, Ranking:95.64431762695312, Reg:3.2802212238311768, Gen:31.169925689697266, Disc:1.4368770280270837e-06, Recon_One:0.0, T_Reg:3.5088140964508057,T_MSE:125.94746398925781,  CI:0.8724212961687828 Validation RAE:0.19492700703444893 Loss:33.998086957226725, Ranking:102.95985242016721, Reg:3.301960841795795, Gen:26.879456205130726, Disc:5.415711028347804e-06, Recon_One:0.0, T_Reg:7.11862533647328, T_MSE:41326.66475187022, CI:0.911518836552835, \n",
      "it:14100, trainCI:0.8706805421785807, train_ranking:113.52363586425781, train_RAE:0.2287215292453766,  train_Gen:32.03965759277344, train_Disc:1.4098507108428748e-06, train_reg:3.2801642417907715, train_t_reg:4.002973556518555, train_t_mse:169.188232421875, train_layer_one_recon:0.0\n",
      "Iteration: 14136 epochs:152, Training: RAE:0.23212191462516785, Loss: 34.61552810668945, Ranking:106.21463775634766, Reg:3.2805938720703125, Gen:31.335302352905273, Disc:1.893103103611793e-06, Recon_One:0.0, T_Reg:3.2802228927612305,T_MSE:131.21066284179688,  CI:0.8923793777243348 Validation RAE:0.19009812438400056 Loss:38.629929202786826, Ranking:102.94020575136635, Reg:3.30233595975566, Gen:27.592094555428226, Disc:3.2277491789911365e-06, Recon_One:0.0, T_Reg:11.037831944764607, T_MSE:176994.7217952494, CI:0.9122176975984776, \n",
      "it:14200, trainCI:0.8933247631290768, train_ranking:111.9547348022461, train_RAE:0.23850296437740326,  train_Gen:31.356630325317383, train_Disc:1.5642915514035849e-06, train_reg:3.2809951305389404, train_t_reg:3.097271680831909, train_t_mse:125.46009826660156, train_layer_one_recon:0.0\n",
      "Iteration: 14229 epochs:153, Training: RAE:0.214722141623497, Loss: 35.1493034362793, Ranking:103.9129867553711, Reg:3.2810659408569336, Gen:31.353864669799805, Disc:1.322282628279936e-06, Recon_One:0.0, T_Reg:3.795438528060913,T_MSE:183.24722290039062,  CI:0.8795759356239622 Validation RAE:0.189564351634829 Loss:43.784875859018115, Ranking:103.00470638143968, Reg:3.3028111571712286, Gen:28.780736466474917, Disc:1.4111099057172643e-06, Recon_One:0.0, T_Reg:15.004138453417873, T_MSE:424738.9226573578, CI:0.911358159923677, \n",
      "it:14300, trainCI:0.8879777347050345, train_ranking:106.48779296875, train_RAE:0.23882217705249786,  train_Gen:30.719738006591797, train_Disc:1.3311787370184902e-06, train_reg:3.2814464569091797, train_t_reg:3.715492010116577, train_t_mse:200.00579833984375, train_layer_one_recon:0.0\n",
      "Iteration: 14322 epochs:154, Training: RAE:0.2349395900964737, Loss: 35.20191192626953, Ranking:107.76935577392578, Reg:3.2814996242523193, Gen:31.543262481689453, Disc:1.4771080714126583e-06, Recon_One:0.0, T_Reg:3.658649444580078,T_MSE:119.66154479980469,  CI:0.8912534214484749 Validation RAE:0.19336837007654478 Loss:44.42439240873851, Ranking:103.01172152241722, Reg:3.303247714796945, Gen:28.92733701873127, Disc:1.3048371792140984e-06, Recon_One:0.0, T_Reg:15.497054836966775, T_MSE:498681.19118683215, CI:0.9109699174341733, \n",
      "it:14400, trainCI:0.8727329133256968, train_ranking:112.09720611572266, train_RAE:0.24056051671504974,  train_Gen:31.72252655029297, train_Disc:1.053839696396608e-06, train_reg:3.281752586364746, train_t_reg:3.670856237411499, train_t_mse:129.03347778320312, train_layer_one_recon:0.0\n",
      "Iteration: 14415 epochs:155, Training: RAE:0.21994978189468384, Loss: 34.78776931762695, Ranking:101.7433090209961, Reg:3.281820297241211, Gen:31.54721450805664, Disc:1.613305016689992e-06, Recon_One:0.0, T_Reg:3.240553379058838,T_MSE:119.32717895507812,  CI:0.8880899226416074 Validation RAE:0.18824094274542816 Loss:44.77719867081885, Ranking:103.00424892305806, Reg:3.303570513041359, Gen:27.747690926585566, Disc:3.0762887912361614e-06, Recon_One:0.0, T_Reg:17.029504989465654, T_MSE:497096.31160742283, CI:0.9099556477478963, \n",
      "it:14500, trainCI:0.8858583779333059, train_ranking:103.3377456665039, train_RAE:0.22394415736198425,  train_Gen:31.84923553466797, train_Disc:8.989879916043719e-07, train_reg:3.282160997390747, train_t_reg:3.3252151012420654, train_t_mse:122.10525512695312, train_layer_one_recon:0.0\n",
      "Iteration: 14508 epochs:156, Training: RAE:0.2080487459897995, Loss: 35.00557327270508, Ranking:98.47274017333984, Reg:3.2821543216705322, Gen:31.69414520263672, Disc:1.102651026485546e-06, Recon_One:0.0, T_Reg:3.311427116394043,T_MSE:171.7500457763672,  CI:0.8957757811452327 Validation RAE:0.19413753719766302 Loss:41.6630294511687, Ranking:103.02544493995428, Reg:3.303906751212678, Gen:27.843738821606138, Disc:3.269693020113734e-06, Recon_One:0.0, T_Reg:13.819287801751855, T_MSE:331674.6677223617, CI:0.9115201876023082, \n",
      "it:14600, trainCI:0.8920846930257267, train_ranking:105.12688446044922, train_RAE:0.2239629030227661,  train_Gen:31.56661605834961, train_Disc:1.550273850625672e-06, train_reg:3.283188581466675, train_t_reg:3.600584030151367, train_t_mse:154.9231414794922, train_layer_one_recon:0.0\n",
      "Iteration: 14601 epochs:157, Training: RAE:0.2239629030227661, Loss: 35.16720199584961, Ranking:105.12688446044922, Reg:3.283188581466675, Gen:31.56661605834961, Disc:1.550273850625672e-06, Recon_One:0.0, T_Reg:3.600584030151367,T_MSE:154.9231414794922,  CI:0.8920846930257267 Validation RAE:0.1936097481116006 Loss:49.12031480665876, Ranking:102.97132627776493, Reg:3.3049478655504227, Gen:27.56145794987604, Disc:3.95456054103452e-06, Recon_One:0.0, T_Reg:21.558853058542507, T_MSE:1037719.8340279647, CI:0.9124271102668117, \n",
      "Iteration: 14694 epochs:158, Training: RAE:0.24078549444675446, Loss: 35.470176696777344, Ranking:102.29413604736328, Reg:3.2838034629821777, Gen:32.161293029785156, Disc:9.165901246888097e-07, Recon_One:0.0, T_Reg:3.3088808059692383,T_MSE:128.0253143310547,  CI:0.9004342431761787 Validation RAE:0.19539432166680196 Loss:44.75271304398756, Ranking:103.02217628294876, Reg:3.3055668221841352, Gen:28.524291391624306, Disc:1.8646704225983343e-06, Recon_One:0.0, T_Reg:16.228420295848895, T_MSE:531700.4480745698, CI:0.9117293054962118, \n",
      "it:14700, trainCI:0.9132676351333208, train_ranking:98.13233184814453, train_RAE:0.21009667217731476,  train_Gen:32.14826202392578, train_Disc:1.3690621472051134e-06, train_reg:3.283824920654297, train_t_reg:2.8237717151641846, train_t_mse:155.94647216796875, train_layer_one_recon:0.0\n",
      "Iteration: 14787 epochs:159, Training: RAE:0.21484287083148956, Loss: 36.03356170654297, Ranking:103.48184967041016, Reg:3.284360885620117, Gen:32.66706085205078, Disc:6.68488098654052e-07, Recon_One:0.0, T_Reg:3.366502046585083,T_MSE:144.86541748046875,  CI:0.8877269240603426 Validation RAE:0.19280124468267956 Loss:60.04773552930249, Ranking:102.9635925598259, Reg:3.3061279391324176, Gen:28.180163315389848, Disc:2.5817904430411787e-06, Recon_One:0.0, T_Reg:31.867570450443974, T_MSE:2685515.8950511822, CI:0.9121752500804857, \n",
      "it:14800, trainCI:0.9193742478941035, train_ranking:96.6927261352539, train_RAE:0.21283645927906036,  train_Gen:32.33834457397461, train_Disc:6.510536536552536e-07, train_reg:3.2847118377685547, train_t_reg:3.311727523803711, train_t_mse:172.19544982910156, train_layer_one_recon:0.0\n",
      "Iteration: 14880 epochs:160, Training: RAE:0.23615816235542297, Loss: 36.98024368286133, Ranking:99.46796417236328, Reg:3.2849793434143066, Gen:32.85919952392578, Disc:7.137887223507278e-07, Recon_One:0.0, T_Reg:4.121044635772705,T_MSE:167.8650665283203,  CI:0.8860520722635494 Validation RAE:0.19766858777612897 Loss:40.68396825767747, Ranking:102.92320098819711, Reg:3.306750495746551, Gen:28.07078660242525, Disc:2.8535752284066233e-06, Recon_One:0.0, T_Reg:12.613178462226108, T_MSE:261708.02198865346, CI:0.9116338968388734, \n",
      "it:14900, trainCI:0.9017317452625673, train_ranking:103.95868682861328, train_RAE:0.2214994877576828,  train_Gen:33.294464111328125, train_Disc:8.359377829947334e-07, train_reg:3.285020589828491, train_t_reg:3.4079928398132324, train_t_mse:127.35589599609375, train_layer_one_recon:0.0\n",
      "Iteration: 14973 epochs:161, Training: RAE:0.212721586227417, Loss: 36.11214065551758, Ranking:94.9809341430664, Reg:3.2853598594665527, Gen:32.37204360961914, Disc:1.0227546454188996e-06, Recon_One:0.0, T_Reg:3.7400946617126465,T_MSE:157.50289916992188,  CI:0.8710957004160887 Validation RAE:0.1946884815925745 Loss:48.39143459686893, Ranking:102.9374356238234, Reg:3.3071335336633423, Gen:28.236340289863033, Disc:2.558069716336982e-06, Recon_One:0.0, T_Reg:20.15509205310512, T_MSE:817958.7057849445, CI:0.9114028428144335, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:15000, trainCI:0.9118140302897392, train_ranking:102.54013061523438, train_RAE:0.20326420664787292,  train_Gen:32.83633041381836, train_Disc:1.0454168659634888e-06, train_reg:3.2854652404785156, train_t_reg:3.4604439735412598, train_t_mse:198.45179748535156, train_layer_one_recon:0.0\n",
      "Iteration: 15066 epochs:162, Training: RAE:0.21028313040733337, Loss: 35.84884262084961, Ranking:108.40348052978516, Reg:3.2856686115264893, Gen:32.68573760986328, Disc:5.830265763506759e-07, Recon_One:0.0, T_Reg:3.1631038188934326,T_MSE:148.73435974121094,  CI:0.8952959028831563 Validation RAE:0.19854421975300493 Loss:77.96654954647681, Ranking:102.8818604408599, Reg:3.3074443319730196, Gen:29.131784185075873, Disc:1.3098234025999857e-06, Recon_One:0.0, T_Reg:48.834763577062816, T_MSE:6273564.6309498055, CI:0.9119257235250678, \n",
      "it:15100, trainCI:0.8997956659676292, train_ranking:98.21298217773438, train_RAE:0.21185189485549927,  train_Gen:32.29628372192383, train_Disc:9.426066753803752e-07, train_reg:3.2859866619110107, train_t_reg:3.1996266841888428, train_t_mse:124.7171401977539, train_layer_one_recon:0.0\n",
      "Iteration: 15159 epochs:163, Training: RAE:0.22571183741092682, Loss: 35.40509796142578, Ranking:107.17013549804688, Reg:3.286397695541382, Gen:32.327266693115234, Disc:8.40955749481509e-07, Recon_One:0.0, T_Reg:3.0778322219848633,T_MSE:145.0716094970703,  CI:0.9146416982487429 Validation RAE:0.1995148723000539 Loss:54.85783880228279, Ranking:102.90890992157337, Reg:3.3081782479815085, Gen:28.70945760543993, Disc:1.8414653240606472e-06, Recon_One:0.0, T_Reg:26.148379370765834, T_MSE:1683556.9712537846, CI:0.9125423916336728, \n",
      "it:15200, trainCI:0.9017442016716386, train_ranking:98.34654998779297, train_RAE:0.21787133812904358,  train_Gen:32.68509292602539, train_Disc:8.045292361202883e-07, train_reg:3.2867724895477295, train_t_reg:3.1987497806549072, train_t_mse:134.73460388183594, train_layer_one_recon:0.0\n",
      "Iteration: 15252 epochs:164, Training: RAE:0.1953449547290802, Loss: 35.76388931274414, Ranking:106.18550109863281, Reg:3.2875328063964844, Gen:32.27500915527344, Disc:8.262077813014912e-07, Recon_One:0.0, T_Reg:3.488879680633545,T_MSE:164.0041046142578,  CI:0.9014408645187112 Validation RAE:0.1912718751742957 Loss:43.574147655529515, Ranking:102.98221412115846, Reg:3.309320881767125, Gen:29.138131587076323, Disc:1.3827645421801902e-06, Recon_One:0.0, T_Reg:14.43601503541533, T_MSE:431856.44784707704, CI:0.9123247744103565, \n",
      "it:15300, trainCI:0.8918961488987478, train_ranking:100.99513244628906, train_RAE:0.20862309634685516,  train_Gen:32.878082275390625, train_Disc:6.068765969757806e-07, train_reg:3.2881829738616943, train_t_reg:3.3911781311035156, train_t_mse:171.2227020263672, train_layer_one_recon:0.0\n",
      "Iteration: 15345 epochs:165, Training: RAE:0.2231212705373764, Loss: 36.51969528198242, Ranking:106.01278686523438, Reg:3.2882468700408936, Gen:32.986942291259766, Disc:6.716308007526095e-07, Recon_One:0.0, T_Reg:3.5327529907226562,T_MSE:128.8956298828125,  CI:0.8964049669537353 Validation RAE:0.19671936880816246 Loss:57.21230488412601, Ranking:102.95959297114348, Reg:3.3100396778578456, Gen:28.978823542192426, Disc:1.6263407151860995e-06, Recon_One:0.0, T_Reg:28.233478910642457, T_MSE:2133435.1642614366, CI:0.9114272845276291, \n",
      "it:15400, trainCI:0.9013830185650848, train_ranking:98.61902618408203, train_RAE:0.1836024671792984,  train_Gen:33.3333854675293, train_Disc:5.724214133806527e-07, train_reg:3.288510799407959, train_t_reg:3.5712571144104004, train_t_mse:168.95994567871094, train_layer_one_recon:0.0\n",
      "Iteration: 15438 epochs:166, Training: RAE:0.20548991858959198, Loss: 36.296321868896484, Ranking:103.43341064453125, Reg:3.28873610496521, Gen:32.757171630859375, Disc:6.201834708008391e-07, Recon_One:0.0, T_Reg:3.539151191711426,T_MSE:142.95223999023438,  CI:0.8998363422492712 Validation RAE:0.19645771675293514 Loss:73.88656397260576, Ranking:102.90375166093169, Reg:3.310532155179435, Gen:28.887705429056158, Disc:1.8280245252997565e-06, Recon_One:0.0, T_Reg:44.998856070519565, T_MSE:5971375.486582834, CI:0.9127838855858595, \n",
      "it:15500, trainCI:0.9053859635355358, train_ranking:101.03081512451172, train_RAE:0.19784772396087646,  train_Gen:32.88396072387695, train_Disc:7.032432449705084e-07, train_reg:3.2889161109924316, train_t_reg:2.8542842864990234, train_t_mse:132.59378051757812, train_layer_one_recon:0.0\n",
      "Iteration: 15531 epochs:167, Training: RAE:0.20982983708381653, Loss: 36.388031005859375, Ranking:93.6983413696289, Reg:3.289214849472046, Gen:32.7476921081543, Disc:8.66207983563072e-07, Recon_One:0.0, T_Reg:3.6403398513793945,T_MSE:158.3170928955078,  CI:0.8982395155864543 Validation RAE:0.2031147542433007 Loss:41.67795144305671, Ranking:102.93119248560373, Reg:3.3110140725584554, Gen:29.712068639547507, Disc:9.691830608897676e-07, Recon_One:0.0, T_Reg:11.965881739167164, T_MSE:232650.0854183081, CI:0.9129377087095085, \n",
      "it:15600, trainCI:0.8767344875521295, train_ranking:105.04296875, train_RAE:0.2388998419046402,  train_Gen:33.168479919433594, train_Disc:1.0058873840534943e-06, train_reg:3.2897555828094482, train_t_reg:3.8647677898406982, train_t_mse:151.12274169921875, train_layer_one_recon:0.0\n",
      "Iteration: 15624 epochs:168, Training: RAE:0.21474865078926086, Loss: 37.45512771606445, Ranking:99.75054931640625, Reg:3.2904164791107178, Gen:33.433284759521484, Disc:7.648467317267205e-07, Recon_One:0.0, T_Reg:4.021842956542969,T_MSE:179.51251220703125,  CI:0.8948834248428423 Validation RAE:0.2006732233914939 Loss:41.5092471362323, Ranking:102.91563956845145, Reg:3.3122236659799023, Gen:29.404503425330063, Disc:1.2572507797078693e-06, Recon_One:0.0, T_Reg:12.104742615256024, T_MSE:247570.40857527574, CI:0.9122839727162683, \n",
      "it:15700, trainCI:0.8978702302030589, train_ranking:101.33378601074219, train_RAE:0.21082653105258942,  train_Gen:33.58293914794922, train_Disc:6.872928679513279e-07, train_reg:3.290802001953125, train_t_reg:3.6078438758850098, train_t_mse:181.29331970214844, train_layer_one_recon:0.0\n",
      "Iteration: 15717 epochs:169, Training: RAE:0.2251417487859726, Loss: 37.148780822753906, Ranking:104.33600616455078, Reg:3.2909576892852783, Gen:33.85258865356445, Disc:5.349920115804707e-07, Recon_One:0.0, T_Reg:3.2961931228637695,T_MSE:169.64146423339844,  CI:0.8965962947005601 Validation RAE:0.19304524248609606 Loss:64.01221795396327, Ranking:102.90465355515704, Reg:3.3127684630169427, Gen:30.0593437456229, Disc:8.175905692283868e-07, Recon_One:0.0, T_Reg:33.952873603126264, T_MSE:3272966.648034643, CI:0.912783197778855, \n",
      "it:15800, trainCI:0.9046278924327705, train_ranking:101.4974136352539, train_RAE:0.20928546786308289,  train_Gen:33.43690490722656, train_Disc:1.077760998668964e-06, train_reg:3.291411876678467, train_t_reg:2.804792881011963, train_t_mse:110.17070770263672, train_layer_one_recon:0.0\n",
      "Iteration: 15810 epochs:170, Training: RAE:0.22500835359096527, Loss: 36.952823638916016, Ranking:93.2489242553711, Reg:3.2921767234802246, Gen:33.20962142944336, Disc:6.057275641069282e-07, Recon_One:0.0, T_Reg:3.7432022094726562,T_MSE:177.38836669921875,  CI:0.8999489680199592 Validation RAE:0.19762229802788264 Loss:36.381800991304964, Ranking:102.9451359506397, Reg:3.3139955763431046, Gen:29.83438382823124, Disc:9.99856580385054e-07, Recon_One:0.0, T_Reg:6.547415733516283, T_MSE:39164.65147361169, CI:0.9126023536657436, \n",
      "it:15900, trainCI:0.8893427881403834, train_ranking:103.36296081542969, train_RAE:0.21235337853431702,  train_Gen:34.10100555419922, train_Disc:5.677655963154393e-07, train_reg:3.2927000522613525, train_t_reg:3.365722417831421, train_t_mse:138.4987335205078, train_layer_one_recon:0.0\n",
      "Iteration: 15903 epochs:171, Training: RAE:0.20966315269470215, Loss: 36.999473571777344, Ranking:103.87769317626953, Reg:3.2927427291870117, Gen:33.45703125, Disc:4.97271742005978e-07, Recon_One:0.0, T_Reg:3.542440176010132,T_MSE:132.34498596191406,  CI:0.9025631842832293 Validation RAE:0.19214715302266283 Loss:51.50866453030772, Ranking:103.01879042310239, Reg:3.314565333244397, Gen:29.65909128593358, Disc:1.1491293050594541e-06, Recon_One:0.0, T_Reg:21.849572023511215, T_MSE:1206716.7857045245, CI:0.912734093271641, \n",
      "Iteration: 15996 epochs:172, Training: RAE:0.19329947233200073, Loss: 37.63233184814453, Ranking:96.8756332397461, Reg:3.2933454513549805, Gen:34.13124084472656, Disc:4.555502357561636e-07, Recon_One:0.0, T_Reg:3.5010898113250732,T_MSE:156.3403778076172,  CI:0.8929010071235569 Validation RAE:0.19349737850534807 Loss:56.52094611200941, Ranking:103.05614173300522, Reg:3.315172049944678, Gen:30.997674698022898, Disc:4.7632236327569606e-07, Recon_One:0.0, T_Reg:25.523271434855964, T_MSE:1686601.4316326054, CI:0.912952128092067, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:16000, trainCI:0.9033044965295242, train_ranking:105.5367431640625, train_RAE:0.2262089103460312,  train_Gen:33.855567932128906, train_Disc:5.75511080569413e-07, train_reg:3.2934012413024902, train_t_reg:3.512627124786377, train_t_mse:183.46051025390625, train_layer_one_recon:0.0\n",
      "Iteration: 16089 epochs:173, Training: RAE:0.20789699256420135, Loss: 37.1537971496582, Ranking:110.03601837158203, Reg:3.293832540512085, Gen:33.842796325683594, Disc:5.834825174133584e-07, Recon_One:0.0, T_Reg:3.3110005855560303,T_MSE:143.19329833984375,  CI:0.9192686289210825 Validation RAE:0.19262040175317718 Loss:50.3768258442413, Ranking:102.94111766292853, Reg:3.3156623672780148, Gen:28.963172705572696, Disc:2.353404423056876e-06, Recon_One:0.0, T_Reg:21.413650175206463, T_MSE:1120302.6167132696, CI:0.9133740239957194, \n",
      "it:16100, trainCI:0.9024132033985721, train_ranking:97.3615951538086, train_RAE:0.24078033864498138,  train_Gen:33.784629821777344, train_Disc:4.770083137373149e-07, train_reg:3.2938714027404785, train_t_reg:3.773153781890869, train_t_mse:155.23216247558594, train_layer_one_recon:0.0\n",
      "Iteration: 16182 epochs:174, Training: RAE:0.22716088593006134, Loss: 36.88530349731445, Ranking:106.54888153076172, Reg:3.2943506240844727, Gen:33.56591033935547, Disc:4.5395259462566173e-07, Recon_One:0.0, T_Reg:3.319394111633301,T_MSE:149.9984588623047,  CI:0.9020719846777604 Validation RAE:0.18646558827052223 Loss:52.71648246228017, Ranking:103.02956039583265, Reg:3.3161838844416662, Gen:30.205006174285725, Disc:8.237895636952282e-07, Recon_One:0.0, T_Reg:22.511474921045355, T_MSE:1196885.2624474608, CI:0.9129836198270588, \n",
      "it:16200, trainCI:0.8922028838978359, train_ranking:110.63758850097656, train_RAE:0.2253410518169403,  train_Gen:34.24705505371094, train_Disc:4.7220012788784516e-07, train_reg:3.2944142818450928, train_t_reg:3.42933988571167, train_t_mse:204.25685119628906, train_layer_one_recon:0.0\n",
      "Iteration: 16275 epochs:175, Training: RAE:0.25577911734580994, Loss: 37.958152770996094, Ranking:118.46929168701172, Reg:3.2949275970458984, Gen:33.963722229003906, Disc:3.325433510781295e-07, Recon_One:0.0, T_Reg:3.994431257247925,T_MSE:147.09580993652344,  CI:0.8888235826024052 Validation RAE:0.19458131239282142 Loss:49.07466797317671, Ranking:103.04291751275319, Reg:3.3167646812829163, Gen:30.272153885614667, Disc:8.205912012416829e-07, Recon_One:0.0, T_Reg:18.8025129814811, T_MSE:779984.7334364824, CI:0.912970625187581, \n",
      "it:16300, trainCI:0.8833522643093487, train_ranking:107.40445709228516, train_RAE:0.2253287136554718,  train_Gen:34.24976348876953, train_Disc:3.717509855505341e-07, train_reg:3.2950191497802734, train_t_reg:3.1540393829345703, train_t_mse:104.04772186279297, train_layer_one_recon:0.0\n",
      "Iteration: 16368 epochs:176, Training: RAE:0.20971640944480896, Loss: 37.871273040771484, Ranking:103.41372680664062, Reg:3.2955892086029053, Gen:33.93915557861328, Disc:4.386121759125672e-07, Recon_One:0.0, T_Reg:3.9321155548095703,T_MSE:158.77980041503906,  CI:0.8933730148549144 Validation RAE:0.1898167761315461 Loss:51.93143011972996, Ranking:103.09609120616886, Reg:3.3174306776607962, Gen:30.25978122247045, Disc:8.721789808041294e-07, Recon_One:0.0, T_Reg:21.67164803117726, T_MSE:1063932.0279703715, CI:0.912973572931886, \n",
      "it:16400, trainCI:0.8703776916791861, train_ranking:100.20226287841797, train_RAE:0.22316785156726837,  train_Gen:33.826602935791016, train_Disc:4.340271857472544e-07, train_reg:3.295907735824585, train_t_reg:3.8403377532958984, train_t_mse:175.15032958984375, train_layer_one_recon:0.0\n",
      "Iteration: 16461 epochs:177, Training: RAE:0.2069028615951538, Loss: 37.641536712646484, Ranking:109.43254089355469, Reg:3.296200752258301, Gen:34.13292694091797, Disc:4.205380719213281e-07, Recon_One:0.0, T_Reg:3.508610725402832,T_MSE:178.75938415527344,  CI:0.8877627901324584 Validation RAE:0.19414914974320213 Loss:56.946383241684686, Ranking:102.90103162007286, Reg:3.3180462743127825, Gen:30.388583304569902, Disc:7.776645692512379e-07, Recon_One:0.0, T_Reg:26.557798392775357, T_MSE:1790355.563989019, CI:0.9133742942056141, \n",
      "it:16500, trainCI:0.8912833876221499, train_ranking:101.62516021728516, train_RAE:0.19870853424072266,  train_Gen:33.88848114013672, train_Disc:4.050854442994023e-07, train_reg:3.2962982654571533, train_t_reg:3.2429490089416504, train_t_mse:149.9252471923828, train_layer_one_recon:0.0\n",
      "Iteration: 16554 epochs:178, Training: RAE:0.21075594425201416, Loss: 36.892906188964844, Ranking:111.57311248779297, Reg:3.297168731689453, Gen:34.266319274902344, Disc:6.650757882198377e-07, Recon_One:0.0, T_Reg:2.6265876293182373,T_MSE:126.66831970214844,  CI:0.9182241317579664 Validation RAE:0.18672415295123637 Loss:93.12566657267288, Ranking:102.93215849078476, Reg:3.3190206690133923, Gen:31.37731849066508, Disc:4.0554175945035163e-07, Recon_One:0.0, T_Reg:61.748346798536524, T_MSE:11588465.7257248, CI:0.9135871950380423, \n",
      "it:16600, trainCI:0.9035673006199433, train_ranking:101.25945281982422, train_RAE:0.21144531667232513,  train_Gen:34.33231735229492, train_Disc:3.1219687457451073e-07, train_reg:3.2973904609680176, train_t_reg:3.411898612976074, train_t_mse:110.61213684082031, train_layer_one_recon:0.0\n",
      "Iteration: 16647 epochs:179, Training: RAE:0.21311800181865692, Loss: 38.06784439086914, Ranking:110.4227294921875, Reg:3.297685384750366, Gen:34.54289627075195, Disc:3.8867383977958525e-07, Recon_One:0.0, T_Reg:3.5249462127685547,T_MSE:212.56222534179688,  CI:0.8903991162768359 Validation RAE:0.18520229644547018 Loss:76.41998802376818, Ranking:102.910651936447, Reg:3.319540746184875, Gen:30.354733137364715, Disc:8.895464997675419e-07, Recon_One:0.0, T_Reg:46.06525328989042, T_MSE:6319210.187822451, CI:0.9127903214942588, \n",
      "it:16700, trainCI:0.8873422056952736, train_ranking:108.73018646240234, train_RAE:0.23997962474822998,  train_Gen:34.523040771484375, train_Disc:3.76734703877446e-07, train_reg:3.2981789112091064, train_t_reg:3.441795825958252, train_t_mse:136.99305725097656, train_layer_one_recon:0.0\n",
      "Iteration: 16740 epochs:180, Training: RAE:0.20715025067329407, Loss: 37.33867263793945, Ranking:103.23918914794922, Reg:3.29862380027771, Gen:34.211944580078125, Disc:4.22165300051347e-07, Recon_One:0.0, T_Reg:3.1267285346984863,T_MSE:177.49481201171875,  CI:0.9060961853978671 Validation RAE:0.18682361349726195 Loss:103.64221186731493, Ranking:102.90190012317547, Reg:3.3204853810473383, Gen:31.153117386537804, Disc:4.913547976673494e-07, Recon_One:0.0, T_Reg:72.48909366686851, T_MSE:16898525.185214214, CI:0.913654845769842, \n",
      "it:16800, trainCI:0.8941875201908501, train_ranking:106.72757720947266, train_RAE:0.2241414487361908,  train_Gen:34.465728759765625, train_Disc:4.253805343523709e-07, train_reg:3.3002209663391113, train_t_reg:2.950313091278076, train_t_mse:142.7648468017578, train_layer_one_recon:0.0\n",
      "Iteration: 16833 epochs:181, Training: RAE:0.24066025018692017, Loss: 38.434181213378906, Ranking:97.19132995605469, Reg:3.300675868988037, Gen:34.39826965332031, Disc:4.7636541466999915e-07, Recon_One:0.0, T_Reg:4.035911560058594,T_MSE:180.7726287841797,  CI:0.8766212622781787 Validation RAE:0.20077161372594868 Loss:41.9867116420794, Ranking:102.89610342426688, Reg:3.3225510498128923, Gen:30.735504130832968, Disc:7.048131185209363e-07, Recon_One:0.0, T_Reg:11.251206916467062, T_MSE:206213.44430925132, CI:0.9126582871139308, \n",
      "it:16900, trainCI:0.9004868108230499, train_ranking:93.03536224365234, train_RAE:0.2217394858598709,  train_Gen:34.97465515136719, train_Disc:2.798014122618042e-07, train_reg:3.3006937503814697, train_t_reg:3.6772408485412598, train_t_mse:190.91091918945312, train_layer_one_recon:0.0\n",
      "Iteration: 16926 epochs:182, Training: RAE:0.229333758354187, Loss: 38.190887451171875, Ranking:105.03743743896484, Reg:3.300926685333252, Gen:34.897186279296875, Disc:2.9560808911810454e-07, Recon_One:0.0, T_Reg:3.293701171875,T_MSE:113.39266967773438,  CI:0.8855720135483545 Validation RAE:0.1919438082590183 Loss:47.070822453996726, Ranking:102.99663174013742, Reg:3.3228035284397497, Gen:30.388334874616678, Disc:9.596673254349453e-07, Recon_One:0.0, T_Reg:16.682486828079785, T_MSE:551962.4478632917, CI:0.9123988856150913, \n",
      "it:17000, trainCI:0.8718653298675578, train_ranking:107.08938598632812, train_RAE:0.20814117789268494,  train_Gen:34.901145935058594, train_Disc:2.801404548335995e-07, train_reg:3.3013663291931152, train_t_reg:3.5534019470214844, train_t_mse:145.60696411132812, train_layer_one_recon:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17019 epochs:183, Training: RAE:0.22290486097335815, Loss: 38.453311920166016, Ranking:101.46199035644531, Reg:3.301474094390869, Gen:34.77877426147461, Disc:3.4614447486092104e-07, Recon_One:0.0, T_Reg:3.67453670501709,T_MSE:168.88107299804688,  CI:0.8932464146023468 Validation RAE:0.19157527461116935 Loss:69.14533775151186, Ranking:103.04321302418948, Reg:3.323354565442853, Gen:30.484116642388965, Disc:9.111252001907711e-07, Recon_One:0.0, T_Reg:38.66122045173516, T_MSE:4042064.483064721, CI:0.9133949775448208, \n",
      "it:17100, trainCI:0.892348124936824, train_ranking:105.02616882324219, train_RAE:0.21008747816085815,  train_Gen:34.810245513916016, train_Disc:3.918264042113151e-07, train_reg:3.3020262718200684, train_t_reg:2.9793734550476074, train_t_mse:121.33263397216797, train_layer_one_recon:0.0\n",
      "Iteration: 17112 epochs:184, Training: RAE:0.2195374220609665, Loss: 39.107460021972656, Ranking:102.87263488769531, Reg:3.301980972290039, Gen:35.49074935913086, Disc:3.336833742650924e-07, Recon_One:0.0, T_Reg:3.616710901260376,T_MSE:173.07369995117188,  CI:0.892922427331943 Validation RAE:0.18947744797703264 Loss:62.21814097740418, Ranking:102.9824795805843, Reg:3.323864802667852, Gen:33.03391519927883, Disc:2.0554743204212622e-07, Recon_One:0.0, T_Reg:29.184225099868172, T_MSE:1954838.299802871, CI:0.9127807658898034, \n",
      "it:17200, trainCI:0.9028633845673715, train_ranking:101.76410675048828, train_RAE:0.1900395005941391,  train_Gen:35.06551742553711, train_Disc:2.7596519203143544e-07, train_reg:3.303271532058716, train_t_reg:3.4490487575531006, train_t_mse:201.3261260986328, train_layer_one_recon:0.0\n",
      "Iteration: 17205 epochs:185, Training: RAE:0.22083842754364014, Loss: 39.635223388671875, Ranking:110.67107391357422, Reg:3.3033084869384766, Gen:35.85094451904297, Disc:2.397987088897935e-07, Recon_One:0.0, T_Reg:3.784278154373169,T_MSE:219.70501708984375,  CI:0.8885765090349865 Validation RAE:0.18778778607955318 Loss:72.045090801644, Ranking:102.96192333759987, Reg:3.325201115400117, Gen:31.531860286688318, Disc:4.169036146553738e-07, Recon_One:0.0, T_Reg:40.513230661041845, T_MSE:4809705.485846923, CI:0.9136642539870821, \n",
      "Iteration: 17298 epochs:186, Training: RAE:0.22491858899593353, Loss: 39.19533157348633, Ranking:105.29597473144531, Reg:3.304335355758667, Gen:35.63963317871094, Disc:2.401767744686367e-07, Recon_One:0.0, T_Reg:3.5556976795196533,T_MSE:181.40658569335938,  CI:0.9016509314888675 Validation RAE:0.18720708836819866 Loss:69.0792747508053, Ranking:103.0768008206477, Reg:3.3262347897783253, Gen:33.289477663277474, Disc:1.8145950188106395e-07, Recon_One:0.0, T_Reg:35.789798026653145, T_MSE:3806702.5994950896, CI:0.9127363286444056, \n",
      "it:17300, trainCI:0.8864224360964503, train_ranking:102.13121795654297, train_RAE:0.22159963846206665,  train_Gen:35.456214904785156, train_Disc:2.6817093612407916e-07, train_reg:3.3043713569641113, train_t_reg:3.5092403888702393, train_t_mse:185.40768432617188, train_layer_one_recon:0.0\n",
      "Iteration: 17391 epochs:187, Training: RAE:0.20141422748565674, Loss: 38.732643127441406, Ranking:104.35795593261719, Reg:3.3055474758148193, Gen:35.20148468017578, Disc:3.1948178502716473e-07, Recon_One:0.0, T_Reg:3.531158924102783,T_MSE:196.5506591796875,  CI:0.899634387853552 Validation RAE:0.19395710252680987 Loss:61.625739552310755, Ranking:102.96395919435365, Reg:3.32745494314234, Gen:31.805441602253982, Disc:3.631699651107277e-07, Recon_One:0.0, T_Reg:29.820298795269565, T_MSE:2560811.4614775525, CI:0.913957382593678, \n",
      "it:17400, trainCI:0.893538589340879, train_ranking:101.96510314941406, train_RAE:0.21959172189235687,  train_Gen:35.32158660888672, train_Disc:2.321833676433016e-07, train_reg:3.3056259155273438, train_t_reg:3.631859064102173, train_t_mse:205.3966522216797, train_layer_one_recon:0.0\n",
      "Iteration: 17484 epochs:188, Training: RAE:0.21572259068489075, Loss: 38.665348052978516, Ranking:103.26403045654297, Reg:3.3061742782592773, Gen:35.299835205078125, Disc:2.1568071417732426e-07, Recon_One:0.0, T_Reg:3.3655128479003906,T_MSE:208.43081665039062,  CI:0.9077604073382289 Validation RAE:0.187087634235259 Loss:62.99083810367181, Ranking:103.05103756598238, Reg:3.328085899710789, Gen:31.376181272502183, Disc:5.0702761756251e-07, Recon_One:0.0, T_Reg:31.61465606943464, T_MSE:2822589.402343542, CI:0.9127624407460406, \n",
      "it:17500, trainCI:0.8996673462817899, train_ranking:98.48179626464844, train_RAE:0.21405208110809326,  train_Gen:35.353271484375, train_Disc:1.8536820789449848e-07, train_reg:3.3063247203826904, train_t_reg:3.4523282051086426, train_t_mse:174.26499938964844, train_layer_one_recon:0.0\n",
      "Iteration: 17577 epochs:189, Training: RAE:0.21362566947937012, Loss: 38.27976989746094, Ranking:106.62005615234375, Reg:3.3067026138305664, Gen:35.19096374511719, Disc:2.910638841058244e-07, Recon_One:0.0, T_Reg:3.0888054370880127,T_MSE:176.6019744873047,  CI:0.9084708635152964 Validation RAE:0.1872174800508124 Loss:55.75437976283342, Ranking:103.01816801257442, Reg:3.3286177368183143, Gen:31.573528150506238, Disc:4.6182463250634127e-07, Recon_One:0.0, T_Reg:24.18085065233241, T_MSE:1516240.8404778761, CI:0.9123556765964873, \n",
      "it:17600, trainCI:0.8980865650260923, train_ranking:103.885986328125, train_RAE:0.20699606835842133,  train_Gen:35.09065246582031, train_Disc:1.8335825302528974e-07, train_reg:3.306736946105957, train_t_reg:3.2112131118774414, train_t_mse:156.21531677246094, train_layer_one_recon:0.0\n",
      "Iteration: 17670 epochs:190, Training: RAE:0.2026788741350174, Loss: 38.80446243286133, Ranking:104.24798583984375, Reg:3.307741403579712, Gen:35.688934326171875, Disc:3.5800906061922433e-07, Recon_One:0.0, T_Reg:3.115525960922241,T_MSE:152.8201446533203,  CI:0.9117609663064209 Validation RAE:0.19903326013676448 Loss:64.54634552555292, Ranking:103.04549931436385, Reg:3.3296634111312593, Gen:31.947915178694636, Disc:3.576291341985266e-07, Recon_One:0.0, T_Reg:32.5984299503387, T_MSE:2928092.233279945, CI:0.9126012482616292, \n",
      "No improvement found in a while, stopping optimization.\n",
      "Time usage: 4:20:56\n",
      "finished enqueueing\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\raibe\\Desktop\\Thesis Code\\DATE\\summaries\\mort_p\\DATE_AE_model\n",
      "Test observed_death:(5261,), percentage:0.526257877363209\n",
      "observed_samples:(5261, 200), empirical_observed:(5261,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\ipykernel_launcher.py:83: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n",
      "C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n",
      "C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed_samples:(4736, 200), empirical_observed:(4736,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\ipykernel_launcher.py:83: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n",
      "C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n",
      "C:\\Users\\raibe\\Anaconda3\\envs\\NewDate\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Setting whis to 'range' is deprecated since 3.2 and support for it will be removed two minor releases later; set it to [0, 100] to achieve the same effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":Test RAE:0.2082371774781784, Loss:25.19799843303821, Gen:21.509501012764154, Disc:8.022657475115799e-05, Reg:3.3017350421305283, Ranking101.50992513788071, Recon:0.0, T_Reg:3.6884172208144377,T_MSE:552.2015465418434, CI:0.9065759542086378, Observed: CI:0, Correlation:SpearmanrResult(correlation=0.802172671164176, pvalue=0.0)\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\raibe\\Desktop\\Thesis Code\\DATE\\summaries\\mort_p\\DATE_AE_model\n",
      "Valid observed_death:(4265,), percentage:0.5333249968738277\n",
      ":Valid RAE:0.21359430442262445, Loss:26.191246215889002, Gen:21.315678241954412, Disc:7.192693218426718e-05, Reg:3.273517396489695, Ranking103.09099471910426, Recon:0.0, T_Reg:4.875496057803979,T_MSE:5047.380039300474, CI:0.90464884824015, Observed: CI:0, Correlation:SpearmanrResult(correlation=0.7976431717508782, pvalue=0.0)\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\raibe\\Desktop\\Thesis Code\\DATE\\summaries\\mort_p\\DATE_AE_model\n",
      "Train observed_death:(17056,), percentage:0.533199949981243\n",
      ":Train RAE:0.2084275955660158, Loss:24.558090889351032, Gen:21.316033961758905, Disc:8.057208707325955e-05, Reg:3.273517396489695, Ranking103.08436224039338, Recon:0.0, T_Reg:3.2419763402043245,T_MSE:152.91529807266897, CI:0.9061116120893041, Observed: CI:0, Correlation:SpearmanrResult(correlation=0.7993329920234543, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    r_epochs = 600\n",
    "    \n",
    "    # Two date models to choose\n",
    "    simple = True\n",
    "    if simple:\n",
    "        model = DATE\n",
    "    else:\n",
    "        model = DATE_AE\n",
    "\n",
    "    \n",
    "    data_set = generate_data()\n",
    "    train_data, valid_data, test_data, end_t, covariates, one_hot_indices, imputation_values \\\n",
    "        = data_set['train'], \\\n",
    "          data_set['valid'], \\\n",
    "          data_set['test'], \\\n",
    "          data_set['end_t'], \\\n",
    "          data_set['covariates'], \\\n",
    "          data_set[\n",
    "              'one_hot_indices'], \\\n",
    "          data_set[\n",
    "              'imputation_values']\n",
    "\n",
    "    print(\"imputation_values:{}, one_hot_indices:{}\".format(imputation_values, one_hot_indices))\n",
    "    print(\"end_t:{}\".format(end_t))\n",
    "    train = {'x': train_data['x'], 'e': train_data['e'], 't': train_data['t']}\n",
    "    valid = {'x': valid_data['x'], 'e': valid_data['e'], 't': valid_data['t']}\n",
    "    test = {'x': test_data['x'], 'e': test_data['e'], 't': test_data['t']}\n",
    "\n",
    "    perfomance_record = []\n",
    "\n",
    "    date = model(batch_size=350,\n",
    "                 learning_rate=3e-4,\n",
    "                 beta1=0.9,\n",
    "                 beta2=0.999,\n",
    "                 require_improvement=10000,\n",
    "                 num_iterations=40000, seed=31415,\n",
    "                 l2_reg=0.001,\n",
    "                 hidden_dim=[50, 50],\n",
    "                 train_data=train, test_data=test, valid_data=valid,\n",
    "                 input_dim=train['x'].shape[1],\n",
    "                 num_examples=train['x'].shape[0], keep_prob=0.8,\n",
    "                 latent_dim=50, end_t=end_t,\n",
    "                 path_large_data='C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE',\n",
    "                 covariates=covariates,\n",
    "                 categorical_indices=one_hot_indices,\n",
    "                 disc_updates=1,\n",
    "                 sample_size=200, imputation_values=imputation_values,\n",
    "                 max_epochs=r_epochs,  gen_updates=2)\n",
    "\n",
    "    with date.session:\n",
    "        date.train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.load('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\Test_empirical_time.npy')\n",
    "ee = np.load('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\Test_data_e.npy')\n",
    "pp1 = np.load('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DATE\\\\matrix\\\\mort_p\\\\Test_predicted_time.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082440781409044\n"
     ]
    }
   ],
   "source": [
    "#print(concordance_index(tt, pp1, ee24))\n",
    "#print(concordance_index(tt, pp1, ee48))\n",
    "print(concordance_index(tt, pp1, ee))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NewDate)",
   "language": "python",
   "name": "newdate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
