{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z < b => e=1(event indicator), we calculate the probability of surviving, this mean in case of event happening, probability of surviving should be low => true y shold be zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow \n",
    "print(tensorflow.__version__)\n",
    "import pandas as pd\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.metrics import brier_score\n",
    "import numpy as np\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter\n",
    "from pycox.evaluation import EvalSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_data = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\Mortgage_p\\\\train_yzb.txt', header=None, sep=\" \")\n",
    "test_data = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\Mortgage_p\\\\test_yzb.txt', header=None, sep=\" \")\n",
    "#print(\"MAX_sequence_length: {}\".format(max(max(tr_data[2]),max(test_data[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "100\n",
      "99\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "#tr_data.head()\n",
    "print(max(tr_data[1]))\n",
    "print(max(tr_data[2]))\n",
    "print(max(test_data[1]))\n",
    "print(max(test_data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 5307\n"
     ]
    }
   ],
   "source": [
    "test_o = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\Mortgage_p\\\\testDRSA.txt', header=None)\n",
    "print('number of batches: {}'.format(test_o[18] [test_o[18] == 1].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_DEN: 2047343\n"
     ]
    }
   ],
   "source": [
    "fet_o = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\Mortgage_p\\\\feat_ind.txt', header=None, sep=\"\\t\")\n",
    "print('MAX_DEN: {}'.format(max(fet_o[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(test_o[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
    "from lifelines.utils import concordance_index\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pdb\n",
    "import signal\n",
    "import math\n",
    "TRAING_TIME = 15\n",
    "SHUFFLE = True\n",
    "LOAD_LITTLE_DATA = False\n",
    "\n",
    "class SparseData(): # it returns arrays of the data, shuffled\n",
    "\n",
    "    def shuffle(self):\n",
    "        if SHUFFLE:\n",
    "            np.random.shuffle(self.index)\n",
    "        return self.data[self.index], self.seqlen[self.index], self.labels[self.index], self.market_price[self.index]\n",
    "\n",
    "    def __init__(self, INPUT_FILE, win, all, discount): # INPUT_FILE: data file name, \n",
    "                                                        #all: true if the entire the dataset will be included, in case of doing lose, we need to get the entire data\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        self.market_price = []\n",
    "        fi = open(INPUT_FILE, 'r')\n",
    "        COUNT = 1\n",
    "        max_d = -1\n",
    "        self.finish_epoch = False\n",
    "        for line in fi:\n",
    "            if COUNT > 10000 and LOAD_LITTLE_DATA: # raiber: check if you are using the small version of the data\n",
    "                break\n",
    "            COUNT += 1\n",
    "            s = line.split(' ') # raiber: take each line (type str) and put it in S\n",
    "            #line = \"0 42 21 0:1 66:1 124:1 127:1 138:1 144:1 147:1 150:1 285:1 419:1 679:1 780:1 924:1 1011:1 1479:1\"\n",
    "            slen = len(s) # raiber: get the length of the features, here 18\n",
    "            t_indices = [] # [0, 66, 124, 127, 138, 144, 147, 150, 285, 419, 679, 780, 924, 1011, 1479]\n",
    "            for i in range(3, slen): # starting from the 3rd index, the 0, 1, 2 indexes has y, z, b respectively \n",
    "                w = s[i].split(':') #  x is the list of features (multi-hot encoded as feat_id:1).\n",
    "                td = int(w[0])\n",
    "                t_indices.append(td)\n",
    "                max_d = max(td, max_d)\n",
    "            market_price = int(s[1]) # z\n",
    "            bid_price = int(s[2]) # b\n",
    "            # notice that the market price is a varibale taking the \"z\" value and also defined at the beginning as array to contain the entire \"z\" values, please change the name of one of them \n",
    "            if all: # they go over the entire dataset, creating the feature array dataset (data variable) by appending each line t_indices to it \n",
    "                                                                    # observed time array (seqlen)  by appending each line \"b\" to it\n",
    "                                                                    # ture time array (market_price) by appending each line \"z\" true time to it\n",
    "                                                                    # if b <= z then the lable would be [1., 0.] otherwise would be [0., 1.]\n",
    "                if bid_price <= market_price: # if b <= z (survival zuncensored \n",
    "                    self.data.append(t_indices)  # data only conclude indices use this to get embedding\n",
    "                    self.seqlen.append(int(bid_price / discount))\n",
    "                    self.market_price.append(int(market_price / discount))\n",
    "                    self.labels.append([1., 0.])  # so far we always lose, it means we still survial\n",
    "                else: # if b > z\n",
    "                    self.data.append(t_indices)  # data only conclude indices use this to get embedding\n",
    "                    self.seqlen.append(int(bid_price / discount))\n",
    "                    self.market_price.append(int(market_price / discount))\n",
    "                    self.labels.append([0., 1.])  # we win means we dead\n",
    "\n",
    "            else: # if not all (all = False)\n",
    "                if bid_price <= market_price:  # if b <= z\n",
    "                    if not win:# if lose (survive)\n",
    "                        self.data.append(t_indices)  # data only conclude indices use this to get embedding\n",
    "                        #bid_price= 3\n",
    "                        #market_price = 2\n",
    "                        self.seqlen.append(int(bid_price / discount))\n",
    "                        self.market_price.append(int(market_price / discount))\n",
    "                        self.labels.append([1., 0.])  # so far we always lose, it means we still survial\n",
    "                else: # if b > z if our bid price (b) is bigger that the ture price we win\n",
    "                    if win: # dead \n",
    "                        #bid_price = 3\n",
    "                        #market_price = 2\n",
    "                        self.data.append(t_indices)  # data only conclude indices use this to get embedding\n",
    "                        self.seqlen.append(int(bid_price / discount))\n",
    "                        self.market_price.append(int(market_price / discount))\n",
    "                        self.labels.append([0., 1.])  # we win means we dead\n",
    "\n",
    "        self.max_d = max_d\n",
    "        fi.close()\n",
    "        self.size = len(self.data)\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.seqlen = np.array(self.seqlen)\n",
    "        self.market_price = np.array(self.market_price)\n",
    "        print(\"data size \", self.size, \"\\n\")\n",
    "        self.index = list(range(0, self.size)) # raiber modefied code to work with python 3.\n",
    "        self.data, self.seqlen, self.labels, self.market_price = self.shuffle() #  it shuffles the indxes \n",
    "        self.batch_id = 0\n",
    "\n",
    "\n",
    "    def next(self, batch_size): # returns a batch from data \n",
    "        if self.batch_id + batch_size > len(self.data):\n",
    "            self.data, self.seqlen, self.labels, self.market_price = self.shuffle()\n",
    "            self.batch_id = 0\n",
    "            self.finish_epoch = True\n",
    "        batch_data = self.data[self.batch_id:self.batch_id + batch_size]\n",
    "        batch_labels = self.labels[self.batch_id:self.batch_id + batch_size]\n",
    "        batch_seqlen = self.seqlen[self.batch_id:self.batch_id + batch_size]\n",
    "        batch_market_price = self.market_price[self.batch_id:self.batch_id + batch_size]\n",
    "        self.batch_id = self.batch_id + batch_size\n",
    "        return np.array(batch_data), np.array(batch_labels), np.array(batch_seqlen), np.array(batch_market_price)\n",
    "        \n",
    "\n",
    "class biSparseData():\n",
    "    def __init__(self, INPUT_FILE, discount):\n",
    "        random.seed(time.time())\n",
    "        #SparseData(INPUT_FILE, win, all, discount)\n",
    "        #self.winData.size = 12044, self.loseData.size = 39703, this is for the train data, loseData is the entier data \n",
    "        self.winData = SparseData(INPUT_FILE, True, False, discount) # only win data, uncensored \n",
    "        self.loseData = SparseData(INPUT_FILE, False, True, discount)#todo lose data get all data, even the win = False here, has no effect because the data\n",
    "        self.size = self.winData.size + self.loseData.size\n",
    "    def next(self, batch):\n",
    "        #win = int(random.random() * 100) % 11 == 1# todoe 1/10 get windata\n",
    "        win = int(random.random() * 100) % 11 <= 5 # int(random.random() * 100) % 11 gives values between 0 and 10,\n",
    "                                                      # the expression <= 5, will give us either true or false, \n",
    "                                                      # it is like take 50 % chance to get ture or false \n",
    "                                                      # which means randomly choose based on 50 probability from either win or lose data \n",
    "                                                      # again lose data contain the data\n",
    "        if win:\n",
    "            a, b, c, d = self.winData.next(batch)\n",
    "            return a, b, c, d, True\n",
    "        else:\n",
    "            a, b, c, d = self.loseData.next(batch)\n",
    "            return a, b, c, d, False\n",
    "\n",
    "\n",
    "class BASE_RNN():\n",
    "\n",
    "    train_data = None\n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random.normal(shape, stddev=0.1) # Outputs random values from a normal distribution.\n",
    "        # shape:\tA 1-D integer Tensor or Python array. The shape of the output tensor.\n",
    "        # stddev: standard deviation \n",
    "\n",
    "    def __init__(self,  EMB_DIM = 32,\n",
    "                        FEATURE_SIZE = 13,\n",
    "                        BATCH_SIZE = 128,\n",
    "                        MAX_DEN = 1580000,\n",
    "                        MAX_SEQ_LEN = 350,\n",
    "                        TRAING_STEPS = 100000,\n",
    "                        STATE_SIZE = 64,\n",
    "                        LR = 0.001,\n",
    "                        GRAD_CLIP = 5.0,\n",
    "                        L2_NORM = 0.001,\n",
    "                        INPUT_FILE = \"2259\",\n",
    "                        ALPHA = 1.0,\n",
    "                        BETA = 0.2,\n",
    "                        ADD_TIME_FEATURE=False,\n",
    "                        MIDDLE_FEATURE_SIZE = 30,\n",
    "                        LOG_FILE_NAME=None,\n",
    "                        FIND_PARAMETER = False,\n",
    "                        SAVE_LOG=True,\n",
    "                        OPEN_TEST=True,\n",
    "                        ONLY_TRAIN_ANLP=False,\n",
    "                        LOG_PREFIX=\"\",\n",
    "                        TEST_FREQUENT=True, # changed by raibrt to true \n",
    "                        ANLP_LR = 0.001,\n",
    "                        DNN_MODEL = False,\n",
    "                        QRNN_MODEL = False,\n",
    "                        GLOAL_STEP = 0,\n",
    "                        COV_SIZE = 1,\n",
    "                        DOUBLE_QRNN = False,\n",
    "                        ANLP_ROUND_ROBIN_RATE = 0.2,\n",
    "                        DISCOUNT = 1\n",
    "):\n",
    "        self.DISCOUNT = DISCOUNT\n",
    "        self.DOUBLE_QRNN = DOUBLE_QRNN\n",
    "        self.ANLP_ROUND_ROBIN_RATE = ANLP_ROUND_ROBIN_RATE\n",
    "        self.QRNN_MODEL = QRNN_MODEL\n",
    "        self.global_step = GLOAL_STEP\n",
    "        self.DNN_MODEL = DNN_MODEL\n",
    "        self.ANLP_LR = ANLP_LR\n",
    "        self.TEST_FREQUENT = TEST_FREQUENT\n",
    "        self.ONLY_TRAIN_ANLP = ONLY_TRAIN_ANLP\n",
    "        self.FIND_PARAMETER = FIND_PARAMETER\n",
    "        self.add_time_feature = ADD_TIME_FEATURE\n",
    "        self.MIDDLE_FEATURE_SIZE = MIDDLE_FEATURE_SIZE\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.TRAING_STEPS = TRAING_STEPS\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.STATE_SIZE = STATE_SIZE\n",
    "        self.EMB_DIM = EMB_DIM\n",
    "        self.FEATURE_SIZE = FEATURE_SIZE\n",
    "        self.MAX_DEN = MAX_DEN\n",
    "        self.MAX_SEQ_LEN = int(MAX_SEQ_LEN / self.DISCOUNT) + 10 # was changed by raiber to int to work with python 3\n",
    "        self.LR = LR\n",
    "        self.GRAD_CLIP = GRAD_CLIP\n",
    "        self.L2_NORM = L2_NORM\n",
    "        self.ALPHA = ALPHA\n",
    "        self.BETA = BETA\n",
    "        self.INPUT_FILE = INPUT_FILE # 2997\n",
    "        self.SAVE_LOG = SAVE_LOG\n",
    "        self.TRAIN_FILE = \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\Mortgage_p\\\\train_yzb.txt\"\n",
    "        self.TEST_FILE = \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\Mortgage_p\\\\test_yzb.txt\"\n",
    "        self.OPEN_TEST = OPEN_TEST # true\n",
    "        self.COV_SIZE = COV_SIZE # 1\n",
    "\n",
    "  \n",
    "\n",
    "        para = None\n",
    "        if LOG_FILE_NAME != None: # LOG_FILE_NAME is none \n",
    "            para = LOG_FILE_NAME\n",
    "        else:\n",
    "            para = LOG_PREFIX + str(self.EMB_DIM) + \"_\" + \\\n",
    "                str(BATCH_SIZE) + \"_\" + \\\n",
    "                str(self.STATE_SIZE) + \"_\" + \\\n",
    "                \"{:.6f}\".format(self.LR) + \"_\" + \"{:.6f}\".format(self.ANLP_LR) + \"_\" + \\\n",
    "                \"{:.6f}\".format(self.L2_NORM) + \"_\" + \\\n",
    "                INPUT_FILE + \"_\" + \\\n",
    "                \"{:.2f}\".format(self.ALPHA) + \"_\" \\\n",
    "                \"{:.2f}\".format(self.BETA) + \"_\" + str(ADD_TIME_FEATURE) + \\\n",
    "                   \"_\" + str(self.QRNN_MODEL) + \"_\" + str(self.COV_SIZE) + \"_\" + str(DISCOUNT)\n",
    "        print (para, '\\n')\n",
    "        self.filename = para\n",
    "        self.train_log_txt_filename = \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\\" + para + '.train.log.txt'\n",
    "        if os.path.exists(self.train_log_txt_filename):\n",
    "            self.exist = True\n",
    "        else:\n",
    "            if self.SAVE_LOG:\n",
    "                self.exist = False\n",
    "                self.train_log_txt = open(self.train_log_txt_filename, 'w')\n",
    "                self.train_log_txt.close()\n",
    "\n",
    "    def get_survival_data(self, model, sess):\n",
    "        alltestdata = SparseData(self.TEST_FILE, True, True) #all test data\n",
    "        ret = []\n",
    "        while alltestdata.finish_epoch == False:\n",
    "            test_batch_x, test_batch_y, test_batch_len, test_batch_market_price = alltestdata.next(self.BATCH_SIZE)\n",
    "            bid_loss, bid_test_prob, anlp, preds = sess.run(\n",
    "                [self.cost, self.predict, self.anlp_node, self.preds],\n",
    "                feed_dict={self.tf_x: test_batch_x,\n",
    "                           self.tf_y: test_batch_y,\n",
    "                           self.tf_bid_len: test_batch_len,\n",
    "                           self.tf_market_price: test_batch_market_price\n",
    "                           })\n",
    "            ret.append(preds)\n",
    "        return ret # return will have the preds \n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        self.train_data = biSparseData(self.TRAIN_FILE, self.DISCOUNT)\n",
    "        self.test_data_win = SparseData(self.TEST_FILE, True, False, self.DISCOUNT) # win, only win test data \n",
    "        self.test_data_lose = SparseData(self.TEST_FILE, False, False, self.DISCOUNT) #lose, only lose test data\n",
    "\n",
    "    def is_exist(self):\n",
    "        if self.SAVE_LOG == False:\n",
    "            return False\n",
    "        return self.exist\n",
    "\n",
    "\n",
    "    def create_graph(self):\n",
    "        BATCH_SIZE = self.BATCH_SIZE\n",
    "        self.tf_x = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE, self.FEATURE_SIZE], name=\"tf_x\")\n",
    "        self.tf_y = tf.compat.v1.placeholder(tf.float32, [BATCH_SIZE, 2], name=\"tf_y\")\n",
    "        self.tf_bid_len = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE], name=\"tf_len\")\n",
    "        self.tf_market_price = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE], name=\"tf_market_price\")\n",
    "        self.tf_control_parameter = tf.compat.v1.placeholder(tf.float32, [2], name=\"tf_control_parameter\")\n",
    "        alpha = self.tf_control_parameter[0]\n",
    "        beta = self.tf_control_parameter[1]\n",
    "        self.tf_rnn_len = tf.maximum(self.tf_bid_len, self.tf_market_price) + 2 # the longest time between b and z plus 2 \n",
    "        embeddings = tf.Variable(self.init_matrix([self.MAX_DEN, self.EMB_DIM]))\n",
    "        x_emds = tf.nn.embedding_lookup(params=embeddings, ids=self.tf_x)\n",
    "        input = tf.reshape(x_emds, [BATCH_SIZE, self.FEATURE_SIZE * self.EMB_DIM])\n",
    "        input_x = None\n",
    "        if self.add_time_feature: # which is true \n",
    "            middle_layer = tf.compat.v1.layers.dense(input, self.MIDDLE_FEATURE_SIZE, tf.nn.relu)  # hidden layer\n",
    "\n",
    "            def add_time(x):\n",
    "                y = tf.reshape(tf.tile(x, [self.MAX_SEQ_LEN]), [self.MAX_SEQ_LEN, self.MIDDLE_FEATURE_SIZE])\n",
    "                t = tf.reshape(tf.range(self.MAX_SEQ_LEN), [self.MAX_SEQ_LEN, 1])\n",
    "                z = tf.concat([y, tf.cast(t, dtype=tf.float32)], 1)\n",
    "                return z\n",
    "\n",
    "            input_x = tf.map_fn(add_time, middle_layer)\n",
    "\n",
    "        preds = None\n",
    "\n",
    "        if self.DNN_MODEL:\n",
    "            outlist = []\n",
    "            for i in range(0, self.BATCH_SIZE):\n",
    "                sigleout = tf.compat.v1.layers.dense(input_x[i], 1, tf.nn.sigmoid)\n",
    "                outlist.append(sigleout)\n",
    "            preds = tf.reshape(tf.stack(outlist, axis=0), [self.BATCH_SIZE, self.MAX_SEQ_LEN], name=\"preds\")\n",
    "        else:\n",
    "            # input_x = tf.reshape(tf.tile(input, [1, self.MAX_SEQ_LEN]), [BATCH_SIZE, self.MAX_SEQ_LEN, self.FEATURE_SIZE * self.EMB_DIM])\n",
    "            rnn_cell = None\n",
    "            rnn_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units=self.STATE_SIZE)\n",
    "\n",
    "\n",
    "            outputs, (h_c, h_n) = tf.compat.v1.nn.dynamic_rnn(\n",
    "                rnn_cell,                   # cell you have chosen\n",
    "                input_x,                    # input\n",
    "                initial_state=None,         # the initial hidden state\n",
    "                dtype=tf.float32,           # must given if set initial_state = None\n",
    "                time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)\n",
    "                sequence_length=self.tf_rnn_len\n",
    "            )\n",
    "\n",
    "            new_output = tf.reshape(outputs, [self.MAX_SEQ_LEN * BATCH_SIZE, self.STATE_SIZE])\n",
    "\n",
    "            with tf.compat.v1.variable_scope('softmax'):\n",
    "                W = tf.compat.v1.get_variable('W', [self.STATE_SIZE, 1])\n",
    "                b = tf.compat.v1.get_variable('b', [1], initializer=tf.compat.v1.constant_initializer(0))\n",
    "\n",
    "            logits = tf.matmul(new_output, W) + b\n",
    "            preds = tf.transpose(a=tf.nn.sigmoid(logits, name=\"preds\"), name=\"preds\")[0]\n",
    "\n",
    "\n",
    "        self.preds = preds\n",
    "        survival_rate = preds \n",
    "        death_rate = tf.subtract(tf.constant(1.0, dtype=tf.float32), survival_rate)\n",
    "        batch_rnn_survival_rate = tf.reshape(survival_rate, [BATCH_SIZE, self.MAX_SEQ_LEN])\n",
    "        batch_rnn_death_rate = tf.reshape(death_rate, [BATCH_SIZE, self.MAX_SEQ_LEN])\n",
    "        \n",
    "        # this line added by raiber, because I want the survival rate at every single time, from 1 to self.MAX_SEQ_LEN\n",
    "        # the rate _result below is a multipication of different survival rate from time 1 to the last time, so the survival time at last tiem only \n",
    "        self.survival_rate_different_times = batch_rnn_survival_rate\n",
    "        self.death_rate_different_times = batch_rnn_death_rate\n",
    "\n",
    "        map_parameter = tf.concat([batch_rnn_survival_rate,\n",
    "                                   tf.cast(tf.reshape(self.tf_bid_len, [BATCH_SIZE, 1]), tf.float32)],\n",
    "                                  1)\n",
    "        map_parameter = tf.concat([map_parameter,\n",
    "                                   tf.cast(tf.reshape(self.tf_market_price, [BATCH_SIZE, 1]), tf.float32)],\n",
    "                                  1)\n",
    "        def reduce_mul(x):\n",
    "            bid_len = tf.cast(x[self.MAX_SEQ_LEN], dtype=tf.int32)\n",
    "            market_len = tf.cast(x[self.MAX_SEQ_LEN + 1], dtype=tf.int32)\n",
    "            # survival_rate_last_one is the survival rate at the obsevation time, which is bid_length\n",
    "            # we are muliplying the survival rates at each single time from 1 to observation time \n",
    "            survival_rate_last_one = tf.reduce_prod(input_tensor=x[0:bid_len])\n",
    "            anlp_rate_last_one = tf.reduce_prod(input_tensor=x[0:market_len + 1])\n",
    "            anlp_rate_last_two = tf.reduce_prod(input_tensor=x[0:market_len])\n",
    "            ret = tf.stack([survival_rate_last_one, anlp_rate_last_one, anlp_rate_last_two])\n",
    "            return ret\n",
    "\n",
    "        self.mp_para = map_parameter\n",
    "        rate_result = tf.map_fn(reduce_mul, elems=map_parameter ,name=\"rate_result\")\n",
    "        self.rate_result = rate_result\n",
    "        log_minus = tf.math.log(tf.add(tf.transpose(a=rate_result)[2] - tf.transpose(a=rate_result)[1], 1e-20))#todo debug\n",
    "\n",
    "\n",
    "        self.anlp_node = -tf.reduce_sum(input_tensor=log_minus) / self.BATCH_SIZE #todo load name\n",
    "        self.anlp_node = tf.add(self.anlp_node, 0, name=\"anlp_node\")\n",
    "        self.final_survival_rate = tf.transpose(a=rate_result)[0]\n",
    "        final_dead_rate = tf.subtract(tf.constant(1.0, dtype=tf.float32), self.final_survival_rate)\n",
    "\n",
    "        self.predict = tf.transpose(a=tf.stack([self.final_survival_rate, final_dead_rate]), name=\"predict\")\n",
    "        cross_entropy = -tf.reduce_sum(input_tensor=self.tf_y*tf.math.log(tf.clip_by_value(self.predict,1e-10,1.0))) # tf.clip_by_value\n",
    "                                                                                                    #this operation returns a tensor of the same type and shape as t \n",
    "                                                                                                    #with its values clipped to clip_value_min and clip_value_max. \n",
    "                                                                                                    #Any values less than clip_value_min are set to clip_value_min. \n",
    "                                                                                                    #Any values greater than clip_value_max are set to clip_value_max.\n",
    "\n",
    "        tvars = tf.compat.v1.trainable_variables()\n",
    "        lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in tvars ]) * self.L2_NORM\n",
    "        cost = tf.add(cross_entropy, lossL2, name = \"cost\")  / self.BATCH_SIZE\n",
    "        self.cost = tf.add(cost, 0, name=\"cost\")\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.LR, beta2=0.99)#.minimize(cost)\n",
    "        optimizer_anlp = tf.compat.v1.train.AdamOptimizer(learning_rate=self.ANLP_LR, beta2=0.99)#.minimize(cost)\n",
    "\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(ys=self.cost, xs=tvars),\n",
    "                                          self.GRAD_CLIP,\n",
    "                                          )\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train_op\")\n",
    "        tf.compat.v1.add_to_collection('train_op', self.train_op)\n",
    "\n",
    "        anlp_grads, _ = tf.clip_by_global_norm(tf.gradients(ys=self.anlp_node, xs=tvars),\n",
    "                                          self.GRAD_CLIP,\n",
    "                                          )\n",
    "        self.anlp_train_op = optimizer_anlp.apply_gradients(zip(anlp_grads, tvars), name=\"anlp_train_op\")\n",
    "        tf.compat.v1.add_to_collection('anlp_train_op', self.anlp_train_op)\n",
    "\n",
    "\n",
    "        self.com_cost = tf.add(alpha * self.cost, beta * self.anlp_node)\n",
    "        com_grads, _ = tf.clip_by_global_norm(tf.gradients(ys=self.com_cost, xs=tvars),\n",
    "                                          self.GRAD_CLIP,\n",
    "                                          )\n",
    "\n",
    "        self.com_train_op = optimizer.apply_gradients(zip(com_grads, tvars), name=\"train_op\")\n",
    "        tf.compat.v1.add_to_collection('com_train_op', self.com_train_op)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(input=self.predict, axis=1), tf.argmax(input=self.tf_y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_pred, tf.float32), name=\"accuracy\")\n",
    "\n",
    "\n",
    "    def train_test(self,sess):\n",
    "        self.load_data()\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        self.sess = sess\n",
    "        sess.run(init)\n",
    "        saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
    "        self.saver = saver\n",
    "        TRAIN_LOG_STEP = int((self.train_data.size * 0.1) / self.BATCH_SIZE)\n",
    "        train_auc_arr = []\n",
    "        train_anlp_arr = []\n",
    "        train_loss_arr = []\n",
    "        train_auc_label = []\n",
    "        train_death_label = []#added by raiber\n",
    "        train_auc_prob = []\n",
    "        train_time = [] #added by raiber \n",
    "        train_m_time = [] #added by raiber \n",
    "        total_train_duration = 0\n",
    "        total_test_duration = 0\n",
    "        TEST_COUNT = 0\n",
    "        max_auc = -1\n",
    "        min_anlp = 200\n",
    "        enough_test = 0\n",
    "        last_loss = [9999.0, 9999.0]\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #added by raiber\n",
    "        self.max_valid = -99\n",
    "        self.stop_flag = 0\n",
    "        \n",
    "        for step in range(1, self.TRAING_STEPS + 1):\n",
    "            if self.stop_flag > 5: #for faster early stopping\n",
    "                break\n",
    "            self.global_step = step\n",
    "            batch_x, batch_y, batch_len, batch_market_price, win = self.train_data.next(self.BATCH_SIZE)\n",
    "            if self.ONLY_TRAIN_ANLP:\n",
    "                if win: #if win\n",
    "                    _, train_anlp, train_loss, train_outputs = sess.run([self.com_train_op, self.anlp_node, self.cost, self.predict],\n",
    "                                                                    feed_dict={self.tf_x: batch_x,\n",
    "                                                                             self.tf_y: batch_y,\n",
    "                                                                             self.tf_bid_len: batch_len,\n",
    "                                                                             self.tf_market_price:batch_market_price,\n",
    "                                                                             self.tf_control_parameter:[self.ALPHA, self.BETA]\n",
    "                                                       })\n",
    "                    train_anlp_arr.append(train_anlp)\n",
    "                    train_loss_arr.append(train_loss)\n",
    "                    train_auc_label.append(batch_y.T[0])\n",
    "                    train_auc_prob.append(np.array(train_outputs).T[0])\n",
    "                else:\n",
    "                    train_loss, train_outputs = sess.run([self.cost, self.predict], feed_dict={self.tf_x: batch_x,\n",
    "                                                       self.tf_y: batch_y,\n",
    "                                                       self.tf_bid_len: batch_len,\n",
    "                                                       self.tf_market_price:batch_market_price,\n",
    "                                                       self.tf_control_parameter:[self.ALPHA, self.BETA]\n",
    "                                                       })\n",
    "                    #print train_outputs\n",
    "                    train_loss_arr.append(train_loss)\n",
    "                    train_auc_label.append(batch_y.T[0])\n",
    "                    train_auc_prob.append(np.array(train_outputs).T[0])\n",
    "            else:\n",
    "                if win: #if win\n",
    "                    _, train_anlp, train_loss, train_outputs, preds= sess.run([self.com_train_op, self.anlp_node, self.cost, self.predict, self.preds],\n",
    "                                                                    feed_dict={self.tf_x: batch_x,\n",
    "                                                                             self.tf_y: batch_y,\n",
    "                                                                             self.tf_bid_len: batch_len,\n",
    "                                                                             self.tf_market_price:batch_market_price,\n",
    "                                                                             self.tf_control_parameter:[self.ALPHA, self.BETA]\n",
    "                                                       })\n",
    "                    train_anlp_arr.append(train_anlp)\n",
    "                    train_loss_arr.append(train_loss)\n",
    "                    train_auc_label.append(batch_y.T[0])\n",
    "                    train_death_label.append(batch_y.T[1])\n",
    "                    train_auc_prob.append(np.array(train_outputs).T[0])\n",
    "                    train_time.append(batch_market_price)\n",
    "                    train_m_time.append(batch_market_price)\n",
    "                else:\n",
    "                    _, train_loss, train_outputs = sess.run([self.train_op, self.cost, self.predict], feed_dict={self.tf_x: batch_x,\n",
    "                                                       self.tf_y: batch_y,\n",
    "                                                       self.tf_bid_len: batch_len,\n",
    "                                                       self.tf_market_price:batch_market_price,\n",
    "                                                       self.tf_control_parameter:[self.ALPHA, self.BETA]\n",
    "                                                       })\n",
    "                    #print train_outputs\n",
    "                    train_loss_arr.append(train_loss) \n",
    "                    train_auc_label.append(batch_y.T[0]) # in the first loop ww will have 100 list (number of steps), each one with 128 elements (batch size)\n",
    "                    train_death_label.append(batch_y.T[1])\n",
    "                    train_auc_prob.append(np.array(train_outputs).T[0])\n",
    "                    train_time.append(batch_len)\n",
    "                    train_m_time.append(batch_market_price)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                mean_anlp = np.array(train_anlp_arr[-99:]).mean()\n",
    "                mean_loss = np.array(train_loss_arr[-99:]).mean()\n",
    "                mean_auc = 0.0001\n",
    "                if not self.ONLY_TRAIN_ANLP:\n",
    "                    try:\n",
    "                        mean_auc = roc_auc_score(np.reshape(train_auc_label, [1, -1])[0], np.reshape(train_auc_prob, [1, -1])[0])\n",
    "                    except Exception:\n",
    "                        print(\"AUC ERROE\")\n",
    "                        continue\n",
    "\n",
    "                log = self.getStatStr(\"TRAIN\", self.global_step, mean_auc, mean_loss, mean_anlp)\n",
    "                print(log)\n",
    "                self.force_write(log)\n",
    "                train_loss_arr = []\n",
    "                train_anlp_arr = []\n",
    "                train_auc_label = []\n",
    "                train_auc_prob = []\n",
    "                if self.TEST_FREQUENT:\n",
    "                    self.run_test(sess)\n",
    "                    #self.save_model()\n",
    "\n",
    "            #if self.global_step < 1000: # raiber changed from 300 to 1000, just for experimenting \n",
    "            #    if self.global_step % 100 == 0:\n",
    "            #        self.run_test(sess, tr_time = train_time, tr_label = train_auc_label)\n",
    "                    #self.save_model()\n",
    "            #elif self.global_step < 2000:\n",
    "            #    if self.global_step % 500 == 0:\n",
    "            #        self.run_test(sess, tr_time = train_time, tr_label = train_auc_label)\n",
    "            #        #self.save_model()\n",
    "            #elif self.global_step < 10000:\n",
    "            #    if self.global_step % 2000 == 0:\n",
    "            #        self.run_test(sess, tr_time = train_time, tr_label = train_auc_label)\n",
    "                    #self.save_model()\n",
    "            #elif self.global_step <= 50000:\n",
    "            #    if self.global_step % 2000 == 0:\n",
    "            #        self.run_test(sess, tr_time = train_time, tr_label = train_auc_label)\n",
    "                    #self.save_model()\n",
    "            #else:\n",
    "            #    break\n",
    "\n",
    "    def run_model(self):\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.compat.v1.Session(config=config) as sess:\n",
    "            self.train_test(sess)\n",
    "\n",
    "    def save_model(self):\n",
    "        print(\"model name: \", self.filename, \" \", self.global_step, \"\\n\")\n",
    "        self.saver.save(self.sess, \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\saved_model\\\\model\" + self.filename, global_step=self.global_step)\n",
    "\n",
    "    def getStatStr(self, category ,step, mean_auc, mean_loss, mean_anlp):\n",
    "        statistics_log = str(self.INPUT_FILE) + \"\\t\" + category + \"\\t\" + str(step) + \"\\t\" \\\n",
    "                         \"{:.6f}\".format(mean_loss) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_auc) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_anlp) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(self.ALPHA * mean_loss + self.BETA * mean_anlp) + \\\n",
    "                         str(self.EMB_DIM) + \"\\t\" + str(self.BATCH_SIZE) + \"\\t\" + \\\n",
    "                         str(self.STATE_SIZE) + \"\\t\" + \\\n",
    "                         \"{:.6f}\".format(self.LR) + \"\\t\" + \\\n",
    "                         \"{:.6f}\".format(self.ANLP_LR) + \"\\t\" + \\\n",
    "                         \"{:.6}\".format(self.L2_NORM) + \"\\t\" +\\\n",
    "                         str(self.ALPHA) + '\\t' + \\\n",
    "                         str(self.BETA) + \"\\n\"\n",
    "        return statistics_log\n",
    "\n",
    "    def getStatStr_test(self, category ,step, mean_auc, mean_lc, mean_br, mean_loss, mean_anlp):\n",
    "        statistics_log = str(self.INPUT_FILE) + \"\\t\" + category + \"\\t\" + str(step) + \"\\t\" \\\n",
    "                         \"{:.6f}\".format(mean_loss) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_auc) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_lc) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_br) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_anlp) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(self.ALPHA * mean_loss + self.BETA * mean_anlp) + \\\n",
    "                         str(self.EMB_DIM) + \"\\t\" + str(self.BATCH_SIZE) + \"\\t\" + \\\n",
    "                         str(self.STATE_SIZE) + \"\\t\" + \\\n",
    "                         \"{:.6f}\".format(self.LR) + \"\\t\" + \\\n",
    "                         \"{:.6f}\".format(self.ANLP_LR) + \"\\t\" + \\\n",
    "                         \"{:.6}\".format(self.L2_NORM) + \"\\t\" +\\\n",
    "                         str(self.ALPHA) + '\\t' + \\\n",
    "                         str(self.BETA) + \"\\n\"\n",
    "        return statistics_log\n",
    "\n",
    "    def load(self, meta, ckpt, step): # this returns sess (session)\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        saver = tf.compat.v1.train.import_meta_graph(meta)\n",
    "        #self.load_data()\n",
    "        self.global_step = step\n",
    "        #with tf.Session(config=config) as sess:\n",
    "        sess = tf.compat.v1.Session(config=config)\n",
    "        saver.restore(sess, ckpt)\n",
    "        graph = tf.compat.v1.get_default_graph()\n",
    "        self.tf_x = graph.get_tensor_by_name(\"tf_x:0\")\n",
    "        self.tf_y = graph.get_tensor_by_name(\"tf_y:0\")\n",
    "        self.tf_bid_len = graph.get_tensor_by_name(\"tf_len:0\")\n",
    "        self.tf_market_price = graph.get_tensor_by_name(\"tf_market_price:0\")\n",
    "        self.accuracy = graph.get_tensor_by_name(\"accuracy:0\")\n",
    "        self.cost = graph.get_tensor_by_name(\"cost:0\")\n",
    "        self.predict = graph.get_tensor_by_name(\"predict:0\")\n",
    "        self.anlp_node = graph.get_tensor_by_name(\"anlp_node:0\")\n",
    "        self.train_op = tf.compat.v1.get_collection('train_op')[0]\n",
    "\n",
    "        #self.anlp_train_op = graph.get_collection(\"anlp_train_op\")[0]\n",
    "        #self.train _op = graph.get_tensor_by_name(\"train_op:0\")\n",
    "        self.preds = graph.get_tensor_by_name(\"preds:0\")\n",
    "        #self.com_train_op = tf.get_collection(\"com_train_op\")[0]\n",
    "        #self.tf_control_parameter = graph.get_tensor_by_name(\"tf_control_parameter:0\")\n",
    "        # self.train_log_txt.write(statistics_log)\n",
    "        return sess\n",
    "\n",
    "    def run_test(self, sess):\n",
    "        auc_arr = []\n",
    "        s_t_sr = []\n",
    "        s_t_dr = []\n",
    "        cind_arr = []#added by raiber\n",
    "        m_br_arr = []#added by raiber\n",
    "        br_arr = []#added by raiber\n",
    "        lc_arr = []#added by raiber\n",
    "        surv_cind_arr = []#added by raiber\n",
    "        surv_br_arr = []#added by raiber\n",
    "        loss_arr = []\n",
    "        anlp_arr = []\n",
    "        auc_prob = []\n",
    "        death_prob = []#added by raiber\n",
    "        auc_label = []\n",
    "        cindex_label = []#added by raiber\n",
    "        auc_time = [] # added by raiber, to get the batch time \n",
    "        m_time = [] # added by raiber\n",
    "        b_time = [] # added by raiber\n",
    "        #print self.test_data_win.size + self.test_data_lose.size, \"total size\"\n",
    "        total_time = 0\n",
    "        for i in range(0, int(self.test_data_win.size / self.BATCH_SIZE)):\n",
    "            test_batch_x, test_batch_y, test_batch_len, test_batch_market_price = self.test_data_win.next(\n",
    "                self.BATCH_SIZE)\n",
    "            start_time = time.time()\n",
    "            bid_loss, bid_test_prob, anlp, single_time_survival, single_time_death = sess.run(\n",
    "                [self.cost, self.predict, self.anlp_node, self.survival_rate_different_times, self.death_rate_different_times],\n",
    "                feed_dict={self.tf_x: test_batch_x,\n",
    "                           self.tf_y: test_batch_y,\n",
    "                           self.tf_bid_len: test_batch_len,\n",
    "                           self.tf_market_price:test_batch_market_price\n",
    "                           })\n",
    "            total_time += time.time() - start_time\n",
    "            s_t_sr.append(np.array(single_time_survival))\n",
    "            s_t_dr.append(np.array(single_time_death))\n",
    "            auc_prob.append(np.array(bid_test_prob).T[0]) # bid_test_prob is the result from self.predict, and self.predict is stacked and tranposed\n",
    "                                                          #array fron survival rates and deas rates, so when we do np.array(bid_test_prob).T[0]\n",
    "                                                          # we get back only the survival rates \n",
    "            death_prob.append(np.array(bid_test_prob).T[1])\n",
    "            auc_label.append(test_batch_y.T[0]) # because our label is either [0,1] or [1,0], we only care about the first index [0]\n",
    "            cindex_label.append(test_batch_y.T[1])\n",
    "            # changed by raiber from auc_label.append(test_batch_y.T[0]) to auc_label.append(test_batch_y.T[1])\n",
    "            anlp_arr.append(anlp)\n",
    "            loss_arr.append(bid_loss)\n",
    "            auc_time.append(test_batch_market_price) # added by raiber\n",
    "            m_time.append(test_batch_market_price)# added by raiber\n",
    "            b_time.append(test_batch_len)# added by raiber\n",
    "        mean_loss = np.array(loss_arr).mean()\n",
    "        mean_anlp = np.array(anlp_arr).mean()\n",
    "        log = self.getStatStr(\"TEST_WIN_DATA\", self.global_step, 0.000001, mean_loss, mean_anlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #result1, result2 = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "        #result1a, result2a = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "        #for t, t_time in enumerate(EVAL_TIMES):\n",
    "        #eval_horizon = int(t_time)\n",
    "\n",
    "        #if eval_horizon >= num_Category:\n",
    "        #    print( 'ERROR: evaluation horizon is out of range')\n",
    "        #    result1[:, t] = result2[:, t] = -1\n",
    "        #else:\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "        #    risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n",
    "        #    for k in range(num_Event):\n",
    "        #        result1a[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "        #        result2a[k, t] = brier_score(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "        #        result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "        #        result2[k, t] = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "        #FINAL1[:, :, out_itr] = result1\n",
    "        #FINAL2[:, :, out_itr] = result2\n",
    "\n",
    "\n",
    "        \n",
    "        print(log)\n",
    "        for i in range(0, int(self.test_data_lose.size / self.BATCH_SIZE)):\n",
    "            test_batch_x, test_batch_y, test_batch_len, test_batch_market_price = self.test_data_lose.next(\n",
    "                self.BATCH_SIZE)\n",
    "            bid_loss, bid_test_prob, anlp, single_time_survival, single_time_death= sess.run(\n",
    "                                   [self.cost, self.predict,self.anlp_node, self.survival_rate_different_times, self.death_rate_different_times],\n",
    "                                   feed_dict={self.tf_x: test_batch_x,\n",
    "                                              self.tf_y: test_batch_y,\n",
    "                                              self.tf_bid_len: test_batch_len,\n",
    "                                              self.tf_market_price: test_batch_market_price\n",
    "                                              })\n",
    "            s_t_sr.append(np.array(single_time_survival))\n",
    "            s_t_dr.append(np.array(single_time_death))\n",
    "            auc_prob.append(np.array(bid_test_prob).T[0])\n",
    "            death_prob.append(np.array(bid_test_prob).T[1])\n",
    "            auc_label.append(test_batch_y.T[0])\n",
    "            cindex_label.append(test_batch_y.T[1])\n",
    "            # changed by raiber from auc_label.append(test_batch_y.T[0]) to auc_label.append(test_batch_y.T[1])\n",
    "            anlp_arr.append(anlp)\n",
    "            loss_arr.append(bid_loss)\n",
    "            auc_time.append(test_batch_len) # added by raiber \n",
    "            m_time.append(test_batch_market_price)# added by raiber \n",
    "            b_time.append(test_batch_len)# added by raiber\n",
    "        if len(auc_prob) > 0:\n",
    "            #pdb.set_trace()\n",
    "            try:  \n",
    "                ll = np.array(s_t_sr).transpose(2,0,1).reshape(self.MAX_SEQ_LEN,-1)\n",
    "                l_time = np.reshape(np.array(auc_time), [1, -1])[0]\n",
    "                l_label = np.reshape(np.array(cindex_label), [1, -1])[0]\n",
    "                df = pd.DataFrame(ll[:l_time.max() + 1,:])\n",
    "                ev = EvalSurv(df, l_time, l_label, censor_surv='km')\n",
    "                pyind = ev.concordance_td()\n",
    "                time_grid = np.linspace(l_time.min(), l_time.max(), l_time.max())\n",
    "                pybr= ev.integrated_brier_score(time_grid) \n",
    "                #y_true = ((Time_survival <= Time) * Death).astype(float)\n",
    "                #arr[:, :2]\n",
    "                auc = roc_auc_score(np.reshape(np.array(auc_label), [1, -1])[0],\n",
    "                                np.reshape(np.array(auc_prob), [1, -1])[0])\n",
    "                #cind_m_time_e = concordance_index(np.reshape(np.array(m_time), [1, -1])[0], \n",
    "                #                                np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0])\n",
    "                #cind_b_time_e = concordance_index(np.reshape(np.array(b_time), [1, -1])[0], \n",
    "                #                                np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0])\n",
    "                #cind_b_time_cen = concordance_index(np.reshape(np.array(b_time), [1, -1])[0], \n",
    "                #                                np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0])\n",
    "                #surv_label = np.reshape(np.array(cindex_label), [1, -1])[0] > 0\n",
    "                #surv_c_ind1 = concordance_index_censored(surv_label, np.reshape(np.array(auc_time), [1, -1])[0], \n",
    "                #                np.reshape(np.array(death_prob), [1, -1])[0])\n",
    "                #cind_auc_time_e = surv_c_ind1[0]\n",
    "                \n",
    "                #print(\"m_time_e:{0}, b_time_e:{1}, b_time_cen:{2}, auctime_e:{3}, \".format(cind_m_time_e, cind_b_time_e, cind_b_time_cen, cind_auc_time_e))\n",
    "                \n",
    "                #### using t\n",
    "                #i_time = max(np.reshape(np.array(auc_time), [1, -1])[0]) - 1\n",
    "                #r_test = np.core.records.fromarrays([surv_label, np.reshape(np.array(auc_time), [1, -1])[0]], names='cens,time')\n",
    "                #bla, surv_brscore1 = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], i_time)\n",
    "                \n",
    "                #surv_brscore = np.zeros([3])\n",
    "                #bla, surv_brscore[0] = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], 18)\n",
    "                #bla, surv_brscore[1] = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], 36)\n",
    "                #bla, surv_brscore[2] = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], i_time)\n",
    "                \n",
    "                #surv = np.mean(surv_brscore)\n",
    "                #print(\"surv_brscore_auc_time:{0}\".format(surv_brscore))\n",
    "                \n",
    "                #### using z\n",
    "                #m_i_time = max(np.reshape(np.array(m_time), [1, -1])[0]) - 1\n",
    "                #m_r_test = np.core.records.fromarrays([surv_label, np.reshape(np.array(m_time), [1, -1])[0]], names='cens,time')\n",
    "                #m_bla, m_surv_brscore1 = brier_score(m_r_train,m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], m_i_time)\n",
    "                \n",
    "                #m_surv_brscore = np.zeros([3])\n",
    "                #m_bla, m_surv_brscore[0] = brier_score(m_r_train, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], 18)\n",
    "                #m_bla, m_surv_brscore[1] = brier_score(m_r_train, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], 36)\n",
    "                #m_bla, m_surv_brscore[2] = brier_score(m_r_train, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], 60)\n",
    "                \n",
    "                #m_surv = np.mean(m_surv_brscore)\n",
    "                #print(\"surv_brscore_m_time:{0}\".format(m_surv_brscore))\n",
    "                \n",
    "                \n",
    "                ####### train \n",
    "                #tr_tt = np.reshape(np.array(tr_time), [1, -1])[0]\n",
    "                #tr_m_tt = np.reshape(np.array(tr_m_time), [1, -1])[0]\n",
    "                #tr_surv_label = np.reshape(np.array(tr_label), [1, -1])[0] #> 0\n",
    "                \n",
    "                #surv_brscore1 = weighted_brier_score(tr_tt, tr_surv_label, np.reshape(np.array(death_prob), [1, -1])[0], \n",
    "                #                     np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0], 61)\n",
    "                #m_surv_brscore1 = weighted_brier_score(tr_m_tt, tr_surv_label, np.reshape(np.array(death_prob), [1, -1])[0], \n",
    "                #                     np.reshape(np.array(m_time), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0], 61)\n",
    "                ##\n",
    "                #idx_t = tr_tt < 52\n",
    "                #tr_tt_a = tr_tt[idx]\n",
    "                #tr_surv_label_a = tr_surv_label[idx]\n",
    "                ##\n",
    "                #r_train_n_a = np.core.records.fromarrays([tr_surv_label_a, tr_tt_a], names='cens,time')\n",
    "                #r_train_n = np.core.records.fromarrays([tr_surv_label, tr_tt ], names='cens,time')\n",
    "                ############\n",
    "                #tt = np.reshape(np.array(auc_time), [1, -1])[0]\n",
    "                #m_tt = np.reshape(np.array(m_time), [1, -1])[0]\n",
    "                #i_time = max(tt) \n",
    "                #lm_time = max(m_tt) - 1 \n",
    "                #m_i_time = min(max(m_tt),max(tr_tt)) + 1\n",
    "                #idx = m_tt < m_i_time\n",
    "                #m_tt_a = m_tt[idx]\n",
    "                #surv_label_a = surv_label[idx]\n",
    "                #r_test = np.core.records.fromarrays([surv_label, tt], names='cens,time')\n",
    "                #m_r_test = np.core.records.fromarrays([surv_label_a, m_tt_a], names='cens,time')\n",
    "                #bla, surv_brscore = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], i_time)\n",
    "                #m_bla, m_surv_brscore = brier_score(r_train, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], m_i_time)\n",
    "                \n",
    "                ####train\n",
    "                #bla1, surv_brscore1 = brier_score(r_train_n, r_test, np.reshape(np.array(death_prob), [1, -1])[0], i_time)\n",
    "                #m_bla1, m_surv_brscore1 = brier_score(r_train_n, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], m_i_time)\n",
    "                ####\n",
    "                \n",
    "                #if eval_time is None:\n",
    "                #    eval_time = [int(np.percentile(tr_time, 25)), int(np.percentile(tr_time, 50)), int(np.percentile(tr_time, 75))]\n",
    "                ### VALIDATION  (based on average C-index of our interest)\n",
    "                #eval_time = [18,36,60]\n",
    "\n",
    "                ### EVALUATION\n",
    "                #va_result = np.zeros([len(eval_time)])\n",
    "                #va_result_m = np.zeros([len(eval_time)])\n",
    "                #br_result = np.zeros([len(eval_time)])\n",
    "                #br_result_m = np.zeros([len(eval_time)])\n",
    "                #s_t_sr_array = np.array(s_t_sr)\n",
    "                #s_t_dr_array = np.array(s_t_dr)\n",
    "                #for t, t_time in enumerate(eval_time):\n",
    "                #    eval_horizon = int(t_time)\n",
    "                #    \n",
    "                #    idx_observed = np.reshape(np.array(auc_time), [1, -1])[0] <= eval_horizon\n",
    "                #    labels = np.reshape(np.array(cindex_label), [1, -1])[0] * idx_observed\n",
    "                    #auc_ll = (labels < 1).astype(int)\n",
    "                    #idx_observed_m = np.reshape(np.array(m_time), [1, -1])[0] <= eval_horizon\n",
    "                    #labels_m = np.reshape(np.array(cindex_label), [1, -1])[0] * idx_observed_m \n",
    "                    \n",
    "                #    surv_prob = np.prod(s_t_sr_array[:,:,:(eval_horizon+1)], axis=2) #survival score until eval_time\n",
    "                #    risk_prob = np.prod(s_t_dr_array[:,:,:(eval_horizon+1)], axis=2)\n",
    "                    #auc_result[t] = roc_auc_score(auc_ll,\n",
    "                    #             np.reshape(np.array(surv_prob), [1, -1])[0])\n",
    "                    #va_result[t] = concordance_index(np.reshape(np.array(auc_time), [1, -1])[0], \n",
    "                    #                            np.reshape(np.array(surv_prob), [1, -1])[0], labels)\n",
    "                #    va_result[t] = c_index(np.reshape(np.array(risk_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0],\n",
    "                #            np.reshape(np.array(cindex_label), [1, -1])[0], eval_horizon)\n",
    "                #    br_result[t] = weighted_brier_score(train_o[17].values, train_o[18].values, np.reshape(np.array(risk_prob), [1, -1])[0], \n",
    "                #                     np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0], eval_horizon)\n",
    "                \n",
    "                    \n",
    "                    #va_result1[0, t] = brier_score(risk[:,k], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon)\n",
    "                    #va_result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon)\n",
    "                \n",
    "                #c1 = np.mean(va_result)\n",
    "                #print(\"hit_c_index:{0}\".format(va_result))\n",
    "                #print(va_result)\n",
    "                #c2 = np.mean(br_result)\n",
    "                #print(\"hit_br:{0}\".format(br_result))\n",
    "                #print(br_result)\n",
    "                \n",
    "\n",
    "                if auc >  self.max_valid:\n",
    "                    self.stop_flag = 0\n",
    "                    self.max_valid = auc\n",
    "                    name = 'test'\n",
    "                    print( 'updated.... average c-index = ' + str('%.4f' %(auc)))\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_s_t_sr'.format(name), np.array(s_t_sr))\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_s_t_dr'.format(name), np.array(s_t_dr))\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_cindex_label'.format(name), np.reshape(np.array(cindex_label), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_auc_label'.format(name), np.reshape(np.array(auc_label), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_auc_time'.format(name), np.reshape(np.array(auc_time), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_auc_prob'.format(name), np.reshape(np.array(auc_prob), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_death_prob'.format(name), np.reshape(np.array(death_prob), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_m_time'.format(name), np.reshape(np.array(m_time), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\Mortgage_p\\\\{}_b_time'.format(name), np.reshape(np.array(b_time), [1, -1])[0])\n",
    "                    #self.save_model()\n",
    "                else:\n",
    "                    self.stop_flag += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "                ############## final time specific evaluation \n",
    "                #EVAL_TIMES = [18,36,60]\n",
    "                #for t, t_time in enumerate(EVAL_TIMES):\n",
    "                #    eval_horizon = int(t_time)\n",
    "                #    risk = np.prod(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n",
    "                #    result1a[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "                #    result2a[k, t] = brier_score(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "                #    result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "                #    result2[k, t] = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "                #FINAL1[:, :, out_itr] = result1\n",
    "                #FINAL2[:, :, out_itr] = result2\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #surv_brscore = brier_score(survival_train, survival_test, estimate, times)\n",
    "                #prob = np.reshape(np.array(auc_prob), [1, -1])[0]\n",
    "                #risk = 1 - prob\n",
    "                #eval_time = [20,40,60]\n",
    "                #va_result1 = np.zeros([1, len(eval_time)]) \n",
    "                #for ind, t_time in enumerate(eval_time):\n",
    "                #    eval_horizon = int(t_time)\n",
    "                #    y_true = ((np.reshape(np.array(auc_time), [1, -1])[0] <= t_time) * np.reshape(np.array(auc_label), [1, -1])[0]).astype(float)\n",
    "                #    va_result1[0, ind] = roc_auc_score(y_true, np.reshape(np.array(auc_prob), [1, -1])[0]) \n",
    "                #mean_C = (va_result1)/3\n",
    "                #c1 = c_index(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 20)\n",
    "                #c2 = c_index(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 40)\n",
    "                #c3 = c_index(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 60)\n",
    "                #mean_C = (c1+c2+c3)/3\n",
    "                \n",
    "                #b1 = brier_score(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 20)\n",
    "                #b2 = brier_score(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 40)\n",
    "                #b3 = brier_score(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 60)\n",
    "                #mean_B = (b1+b2+b3)/3\n",
    "\n",
    "                #eval_time = [20,40,60]\n",
    "                #va_result1 = np.zeros([1, len(eval_time)]) \n",
    "                #for ind, t_time in enumerate(eval_time):\n",
    "                #    eval_horizon = int(t_time)\n",
    "                #    idx_observed = np.reshape(np.array(auc_time), [1, -1])[0] <= eval_horizon\n",
    "                #    va_result1[0, ind] = roc_auc_score(np.reshape(np.array(auc_label), [1, -1])[0][idx_observed],\n",
    "                #                  np.reshape(np.array(auc_prob), [1, -1])[0][idx_observed]) \n",
    "                \n",
    "                \n",
    "                #raiber return\n",
    "                #eval_time = [20,40,60]\n",
    "                #va_result1 = np.zeros([1, len(eval_time)]) \n",
    "                #for ind, t_time in enumerate(eval_time): #raiber: enumerate give us the index and the value , so t will be the index\n",
    "                #     eval_horizon = int(t_time)\n",
    "                #     idx_observed = np.reshape(np.array(auc_time), [1, -1])[0] <= eval_horizon\n",
    "                     #print(\"t: {0}, p: {1}, e:{2}\".format(t[idx_observed].shape,predicted_event_times[idx_observed].shape, e[idx_observed].shape))\n",
    "                #     va_result1[0, ind] = concordance_index(event_times=(np.reshape(np.array(auc_time), [1, -1])[0][idx_observed]).tolist(), predicted_scores=(np.reshape(np.array(auc_prob), [1, -1])[0][idx_observed]).tolist(),\n",
    "                #                            event_observed=(np.reshape(np.array(auc_label), [1, -1])[0][idx_observed]).tolist())\n",
    "                #ci_index = np.mean(va_result1)\n",
    "                # end raiber \n",
    "            except Exception:\n",
    "                print(\"AUC ERROR\")\n",
    "                return\n",
    "            #auc = np.mean(va_result1)\n",
    "            auc_arr.append(auc)\n",
    "            lc_arr.append(pyind)\n",
    "            #surv_cind_arr.append(cind_auc_time_e)#added by raiber\n",
    "            br_arr.append(pybr)#added by raiber\n",
    "            #m_br_arr.append(surv_brscore1)#added by raiber\n",
    "            ############\n",
    "            #br_arr1.append(surv_brscore1)#added by raiber\n",
    "            #m_br_arr1.append(m_surv_brscore1)#added by raiber\n",
    "            ###########\n",
    "        mean_loss = np.array(loss_arr).mean()\n",
    "        mean_auc = np.array(auc_arr).mean()\n",
    "        #mean_c_surv = np.array(surv_cind_arr).mean()\n",
    "        mean_lc = np.array(lc_arr).mean()\n",
    "        mean_br = np.array(br_arr).mean() #added by raiber\n",
    "        #mean_br_m = np.array(m_br_arr).mean() #added by raiber\n",
    "        ###########\n",
    "        #mean_br1 = np.array(br_arr1).mean() #added by raiber\n",
    "        #mean_br_m1 = np.array(m_br_arr1).mean() #added by raiber\n",
    "        ############\n",
    "        mean_anlp = np.array(anlp_arr).mean() \n",
    "        #mean_br, mean_br_m\n",
    "        log = self.getStatStr_test(\"TEST\", self.global_step, mean_auc, mean_lc, mean_br, mean_loss, mean_anlp)\n",
    "        self.force_write(log)\n",
    "        print(log)\n",
    "        return mean_auc, mean_loss, mean_anlp\n",
    "\n",
    "    def force_write(self, log):\n",
    "        if not self.SAVE_LOG:\n",
    "            return\n",
    "        self.train_log_txt = open(self.train_log_txt_filename, 'a')\n",
    "        self.train_log_txt.write(log)\n",
    "        self.train_log_txt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drsa32_128_128_0.000100_0.000100_0.001000_2259_1.20_0.20_True_False_1_1 \n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-7-86c2a93c5bdf>:272: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-86c2a93c5bdf>:293: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-86c2a93c5bdf>:302: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:281: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
      "data size  21050 \n",
      "\n",
      "data size  39703 \n",
      "\n",
      "data size  5307 \n",
      "\n",
      "data size  4619 \n",
      "\n",
      "2259\tTRAIN\t100\t3.544304\t0.4570\t4.7908\t5.211332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t100\t2.333102\t0.0000\t4.0626\t3.612232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.4564\n",
      "2259\tTEST\t100\t3.457714\t0.4564\t0.6139\t0.2305\t5.0688\t5.163032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t200\t2.561175\t0.5599\t4.0815\t3.889732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t200\t2.108673\t0.0000\t4.1397\t3.358432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.6395\n",
      "2259\tTEST\t200\t2.690072\t0.6395\t0.7366\t0.2365\t4.7361\t4.175332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t300\t2.187909\t0.7040\t4.0668\t3.438832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t300\t1.787632\t0.0000\t3.9606\t2.937332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.7703\n",
      "2259\tTEST\t300\t2.373526\t0.7703\t0.7679\t0.2348\t4.5473\t3.757732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t400\t1.936433\t0.8176\t3.9244\t3.108632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t400\t1.573359\t0.0000\t3.8436\t2.656832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8583\n",
      "2259\tTEST\t400\t1.983728\t0.8583\t0.7788\t0.2340\t4.3380\t3.248132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t500\t1.649826\t0.8620\t3.7597\t2.731732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t500\t1.348022\t0.0000\t3.6905\t2.355732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8801\n",
      "2259\tTEST\t500\t1.716890\t0.8801\t0.7874\t0.2286\t4.1523\t2.890732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t600\t1.411446\t0.8808\t3.6230\t2.418332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t600\t1.157453\t0.0000\t3.5618\t2.101332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8874\n",
      "2259\tTEST\t600\t1.508243\t0.8874\t0.7987\t0.2218\t4.0065\t2.611232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t700\t1.230544\t0.8915\t3.5254\t2.181732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t700\t1.013670\t0.0000\t3.4844\t1.913332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8971\n",
      "2259\tTEST\t700\t1.306442\t0.8971\t0.8100\t0.2184\t3.9295\t2.353632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t800\t1.063851\t0.8985\t3.4581\t1.968232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t800\t0.868485\t0.0000\t3.4010\t1.722432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9028\n",
      "2259\tTEST\t800\t1.158848\t0.9028\t0.8220\t0.2155\t3.8543\t2.161532\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t900\t0.934294\t0.9055\t3.3922\t1.799632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t900\t0.800830\t0.0000\t3.3430\t1.629632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9116\n",
      "2259\tTEST\t900\t0.994678\t0.9116\t0.8303\t0.2180\t3.8161\t1.956832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1000\t0.808355\t0.9182\t3.2838\t1.626832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1000\t0.619665\t0.0000\t3.3123\t1.406132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t1000\t0.964146\t0.9083\t0.8379\t0.2061\t3.8622\t1.929432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1100\t0.711271\t0.9209\t3.2530\t1.504132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1100\t0.563573\t0.0000\t3.2093\t1.318132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9193\n",
      "2259\tTEST\t1100\t0.824872\t0.9193\t0.8440\t0.2065\t3.7576\t1.741432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1200\t0.645352\t0.9225\t3.2101\t1.416432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1200\t0.484561\t0.0000\t3.1715\t1.215832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9223\n",
      "2259\tTEST\t1200\t0.755231\t0.9223\t0.8492\t0.2021\t3.7805\t1.662432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1300\t0.563455\t0.9307\t3.1418\t1.304532\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1300\t0.423526\t0.0000\t3.1193\t1.132132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9266\n",
      "2259\tTEST\t1300\t0.689348\t0.9266\t0.8551\t0.2006\t3.8017\t1.587632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1400\t0.511261\t0.9320\t3.0751\t1.228532\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1400\t0.390163\t0.0000\t3.0535\t1.078932\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9329\n",
      "2259\tTEST\t1400\t0.612185\t0.9329\t0.8591\t0.2000\t3.7706\t1.488732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1500\t0.465445\t0.9434\t3.0248\t1.163532\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1500\t0.354318\t0.0000\t3.0344\t1.032132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9351\n",
      "2259\tTEST\t1500\t0.560732\t0.9351\t0.8622\t0.1979\t3.7741\t1.427732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1600\t0.410498\t0.9428\t2.9724\t1.087132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1600\t0.345362\t0.0000\t3.0170\t1.017832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9385\n",
      "2259\tTEST\t1600\t0.504573\t0.9385\t0.8648\t0.1993\t3.7368\t1.352832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1700\t0.380253\t0.9481\t2.9439\t1.045132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1700\t0.266868\t0.0000\t2.9625\t0.912732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t1700\t0.534756\t0.9330\t0.8682\t0.1927\t3.9968\t1.441132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1800\t0.346326\t0.9471\t2.9168\t0.999032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1800\t0.258228\t0.0000\t2.9327\t0.896432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated.... average c-index = 0.9395\n",
      "2259\tTEST\t1800\t0.478602\t0.9395\t0.8699\t0.1908\t4.0036\t1.375132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1900\t0.323476\t0.9518\t2.8732\t0.962832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1900\t0.278851\t0.0000\t2.9399\t0.922632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9442\n",
      "2259\tTEST\t1900\t0.420719\t0.9442\t0.8716\t0.1935\t3.7860\t1.262132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2000\t0.293016\t0.9549\t2.8378\t0.919232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2000\t0.245344\t0.0000\t2.8745\t0.869332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9450\n",
      "2259\tTEST\t2000\t0.414843\t0.9450\t0.8739\t0.1893\t3.9737\t1.292632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2100\t0.302291\t0.9512\t2.7714\t0.917032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2100\t0.222148\t0.0000\t2.8349\t0.833632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2100\t0.414414\t0.9446\t0.8763\t0.1875\t4.1087\t1.319032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2200\t0.281978\t0.9574\t2.7553\t0.889432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2200\t0.213924\t0.0000\t2.8100\t0.818732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9464\n",
      "2259\tTEST\t2200\t0.399099\t0.9464\t0.8773\t0.1876\t4.1192\t1.302832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2300\t0.265277\t0.9630\t2.7553\t0.869432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2300\t0.213566\t0.0000\t2.8116\t0.818632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9473\n",
      "2259\tTEST\t2300\t0.381316\t0.9473\t0.8783\t0.1867\t4.1779\t1.293232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2400\t0.267919\t0.9616\t2.7166\t0.864832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2400\t0.192500\t0.0000\t2.8131\t0.793632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2400\t0.408318\t0.9420\t0.8790\t0.1855\t4.4485\t1.379732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2500\t0.255297\t0.9607\t2.6833\t0.843032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2500\t0.243102\t0.0000\t2.8052\t0.852832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9512\n",
      "2259\tTEST\t2500\t0.339001\t0.9512\t0.8802\t0.1870\t4.0729\t1.221432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2600\t0.247110\t0.9623\t2.6403\t0.824632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2600\t0.239911\t0.0000\t2.8126\t0.850432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9512\n",
      "2259\tTEST\t2600\t0.333852\t0.9512\t0.8820\t0.1871\t4.1527\t1.231232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2700\t0.247837\t0.9652\t2.5905\t0.815532\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2700\t0.233828\t0.0000\t2.7583\t0.832332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9517\n",
      "2259\tTEST\t2700\t0.330741\t0.9517\t0.8819\t0.1827\t4.4589\t1.288732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2800\t0.220559\t0.9662\t2.5631\t0.777332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2800\t0.177336\t0.0000\t2.7263\t0.758132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2800\t0.394991\t0.9437\t0.8839\t0.1806\t4.8390\t1.441832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2900\t0.212037\t0.9689\t2.5580\t0.766032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2900\t0.195862\t0.0000\t2.6885\t0.772732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2900\t0.353578\t0.9496\t0.8852\t0.1785\t4.8031\t1.384932\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3000\t0.214401\t0.9718\t2.4841\t0.754132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3000\t0.196927\t0.0000\t2.6956\t0.775432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t3000\t0.353265\t0.9489\t0.8853\t0.1780\t5.0753\t1.439032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3100\t0.226158\t0.9680\t2.5083\t0.773032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3100\t0.207323\t0.0000\t2.6744\t0.783732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t3100\t0.335938\t0.9510\t0.8859\t0.1795\t4.9753\t1.398232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3200\t0.215280\t0.9719\t2.4739\t0.753132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3200\t0.231330\t0.0000\t2.7021\t0.818032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9536\n",
      "2259\tTEST\t3200\t0.313753\t0.9536\t0.8869\t0.1787\t4.7527\t1.327132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3300\t0.194700\t0.9714\t2.4134\t0.716332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3300\t0.219667\t0.0000\t2.6560\t0.794832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t3300\t0.321075\t0.9528\t0.8880\t0.1787\t5.1254\t1.410432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3400\t0.198876\t0.9745\t2.3741\t0.713532\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3400\t0.199090\t0.0000\t2.6355\t0.766032\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t3400\t0.333667\t0.9520\t0.8879\t0.1764\t5.1511\t1.430632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3500\t0.210492\t0.9732\t2.3749\t0.727632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3500\t0.219299\t0.0000\t2.6621\t0.795632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9536\n",
      "2259\tTEST\t3500\t0.315827\t0.9536\t0.8881\t0.1766\t4.9211\t1.363232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3600\t0.190429\t0.9776\t2.3852\t0.705532\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3600\t0.203782\t0.0000\t2.6288\t0.770332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t3600\t0.339996\t0.9498\t0.8885\t0.1792\t5.3734\t1.482732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3700\t0.184532\t0.9765\t2.2867\t0.678832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3700\t0.228624\t0.0000\t2.6450\t0.803332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9538\n",
      "2259\tTEST\t3700\t0.312081\t0.9538\t0.8891\t0.1771\t5.1734\t1.409232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3800\t0.185333\t0.9776\t2.3135\t0.685132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3800\t0.201584\t0.0000\t2.6347\t0.768832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t3800\t0.346885\t0.9484\t0.8896\t0.1783\t5.6757\t1.551432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t3900\t0.194339\t0.9764\t2.2836\t0.689932\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t3900\t0.255914\t0.0000\t2.7278\t0.852632\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9539\n",
      "2259\tTEST\t3900\t0.303041\t0.9539\t0.8896\t0.1799\t5.1830\t1.400232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t4000\t0.174681\t0.9783\t2.2289\t0.655432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t4000\t0.221159\t0.0000\t2.6417\t0.793732\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t4000\t0.320750\t0.9524\t0.8902\t0.1748\t5.5701\t1.498932\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t4100\t0.180062\t0.9803\t2.2455\t0.665232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t4100\t0.287923\t0.0000\t2.7770\t0.900932\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t4100\t0.299374\t0.9537\t0.8902\t0.1767\t5.1849\t1.396232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t4200\t0.179570\t0.9791\t2.2097\t0.657432\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t4200\t0.242601\t0.0000\t2.6511\t0.821332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t4200\t0.308107\t0.9536\t0.8903\t0.1772\t5.5456\t1.478932\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t4300\t0.165948\t0.9827\t2.1451\t0.628132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t4300\t0.225202\t0.0000\t2.6445\t0.799132\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t4300\t0.320191\t0.9523\t0.8907\t0.1790\t5.8400\t1.552232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t4400\t0.162612\t0.9825\t2.1459\t0.624332\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t4400\t0.255878\t0.0000\t2.6506\t0.837232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t4400\t0.305221\t0.9535\t0.8902\t0.1757\t5.5659\t1.479532\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t4500\t0.157745\t0.9857\t2.1247\t0.614232\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t4500\t0.266632\t0.0000\t2.7148\t0.862932\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2259\tTEST\t4500\t0.305263\t0.9537\t0.8902\t0.1751\t5.7224\t1.510832\t128\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from BASE_MODEL import BASE_RNN\n",
    "import sys\n",
    "\n",
    "#default parameter\n",
    "FEATURE_SIZE = 18 # dataset input fields countnn\n",
    "\n",
    "MAX_DEN = 2047343 # max input data demension 97419, 2047294, 2047341, here is the max the embedding can get \n",
    "EMB_DIM = 32\n",
    "BATCH_SIZE = 128\n",
    "# max time for mortgage data 93\n",
    "MAX_SEQ_LEN = int(118) # Dimension of time horizon of interest (equivalent to the output dimension per survival)\n",
    "TRAING_STEPS = 10000000 # changed by raiber from 10000000\n",
    "\n",
    "STATE_SIZE = 128\n",
    "GRAD_CLIP = 5.0\n",
    "L2_NORM = 0.001\n",
    "ADD_TIME = True\n",
    "ALPHA = 1.2 # coefficient for cross entropy\n",
    "BETA = 0.2 # coefficient for anlp\n",
    "input_file=\"2259\" #toy dataset\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    print (\"Please input learning rate. ex. 0.0001\")\n",
    "    sys.exit(0)\n",
    "\n",
    "LR = float(0.0001)\n",
    "LR_ANLP = LR\n",
    "RUNNING_MODEL = BASE_RNN(EMB_DIM=EMB_DIM,\n",
    "                         FEATURE_SIZE=FEATURE_SIZE,\n",
    "                         BATCH_SIZE=BATCH_SIZE,\n",
    "                         MAX_DEN=MAX_DEN,\n",
    "                         MAX_SEQ_LEN=MAX_SEQ_LEN,\n",
    "                         TRAING_STEPS=TRAING_STEPS,\n",
    "                         STATE_SIZE=STATE_SIZE,\n",
    "                         LR=LR,\n",
    "                         GRAD_CLIP=GRAD_CLIP,\n",
    "                         L2_NORM=L2_NORM,\n",
    "                         INPUT_FILE=input_file,\n",
    "                         ALPHA=ALPHA,\n",
    "                         BETA=BETA,\n",
    "                         ADD_TIME_FEATURE=ADD_TIME,\n",
    "                         FIND_PARAMETER=False,\n",
    "                         ANLP_LR=LR,\n",
    "                         DNN_MODEL=False,\n",
    "                         DISCOUNT=1,\n",
    "                         ONLY_TRAIN_ANLP=False,\n",
    "                         LOG_PREFIX=\"drsa\")\n",
    "RUNNING_MODEL.create_graph()\n",
    "RUNNING_MODEL.run_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
