{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z < b => e=1(event indicator), we calculate the probability of surviving, this mean in case of event happening, probability of surviving should be low => true y shold be zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow \n",
    "print(tensorflow.__version__)\n",
    "import pandas as pd\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.metrics import brier_score\n",
    "import numpy as np\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter\n",
    "from pycox.evaluation import EvalSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_data = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\ndb_p\\\\10\\\\train_yzb.txt', header=None, sep=\" \")\n",
    "test_data = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\ndb_p\\\\10\\\\test_yzb.txt', header=None, sep=\" \")\n",
    "#print(\"MAX_sequence_length: {}\".format(max(max(tr_data[2]),max(test_data[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "140\n",
      "146\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "#tr_data.head()\n",
    "print(max(tr_data[1]))\n",
    "print(max(tr_data[2]))\n",
    "print(max(test_data[1]))\n",
    "print(max(test_data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 102\n"
     ]
    }
   ],
   "source": [
    "test_o = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\ndb_p\\\\10\\\\testDRSA.txt', header=None)\n",
    "print('number of batches: {}'.format(test_o[26] [test_o[26] == 0].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_DEN: 912736\n"
     ]
    }
   ],
   "source": [
    "fet_o = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\ndb_p\\\\10\\\\feat_ind.txt', header=None, sep=\"\\t\")\n",
    "print('MAX_DEN: {}'.format(max(fet_o[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(test_o[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
    "from lifelines.utils import concordance_index\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pdb\n",
    "import signal\n",
    "import math\n",
    "TRAING_TIME = 15\n",
    "SHUFFLE = True\n",
    "LOAD_LITTLE_DATA = False\n",
    "\n",
    "class SparseData(): # it returns arrays of the data, shuffled\n",
    "\n",
    "    def shuffle(self):\n",
    "        if SHUFFLE:\n",
    "            np.random.shuffle(self.index)\n",
    "        return self.data[self.index], self.seqlen[self.index], self.labels[self.index], self.market_price[self.index]\n",
    "\n",
    "    def __init__(self, INPUT_FILE, win, all, discount): # INPUT_FILE: data file name, \n",
    "                                                        #all: true if the entire the dataset will be included, in case of doing lose, we need to get the entire data\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        self.market_price = []\n",
    "        fi = open(INPUT_FILE, 'r')\n",
    "        COUNT = 1\n",
    "        max_d = -1\n",
    "        self.finish_epoch = False\n",
    "        for line in fi:\n",
    "            if COUNT > 10000 and LOAD_LITTLE_DATA: # raiber: check if you are using the small version of the data\n",
    "                break\n",
    "            COUNT += 1\n",
    "            s = line.split(' ') # raiber: take each line (type str) and put it in S\n",
    "            #line = \"0 42 21 0:1 66:1 124:1 127:1 138:1 144:1 147:1 150:1 285:1 419:1 679:1 780:1 924:1 1011:1 1479:1\"\n",
    "            slen = len(s) # raiber: get the length of the features, here 18\n",
    "            t_indices = [] # [0, 66, 124, 127, 138, 144, 147, 150, 285, 419, 679, 780, 924, 1011, 1479]\n",
    "            for i in range(3, slen): # starting from the 3rd index, the 0, 1, 2 indexes has y, z, b respectively \n",
    "                w = s[i].split(':') #  x is the list of features (multi-hot encoded as feat_id:1).\n",
    "                td = int(w[0])\n",
    "                t_indices.append(td)\n",
    "                max_d = max(td, max_d)\n",
    "            market_price = int(s[1]) # z\n",
    "            bid_price = int(s[2]) # b\n",
    "            # notice that the market price is a varibale taking the \"z\" value and also defined at the beginning as array to contain the entire \"z\" values, please change the name of one of them \n",
    "            if all: # they go over the entire dataset, creating the feature array dataset (data variable) by appending each line t_indices to it \n",
    "                                                                    # observed time array (seqlen)  by appending each line \"b\" to it\n",
    "                                                                    # ture time array (market_price) by appending each line \"z\" true time to it\n",
    "                                                                    # if b <= z then the lable would be [1., 0.] otherwise would be [0., 1.]\n",
    "                if bid_price <= market_price: # if b <= z (survival zuncensored \n",
    "                    self.data.append(t_indices)  # data only conclude indices use this to get embedding\n",
    "                    self.seqlen.append(int(bid_price / discount))\n",
    "                    self.market_price.append(int(market_price / discount))\n",
    "                    self.labels.append([1., 0.])  # so far we always lose, it means we still survial\n",
    "                else: # if b > z\n",
    "                    self.data.append(t_indices)  # data only conclude indices use this to get embedding\n",
    "                    self.seqlen.append(int(bid_price / discount))\n",
    "                    self.market_price.append(int(market_price / discount))\n",
    "                    self.labels.append([0., 1.])  # we win means we dead\n",
    "\n",
    "            else: # if not all (all = False)\n",
    "                if bid_price <= market_price:  # if b <= z\n",
    "                    if not win:# if lose (survive)\n",
    "                        self.data.append(t_indices)  # data only conclude indices use this to get embedding\n",
    "                        #bid_price= 3\n",
    "                        #market_price = 2\n",
    "                        self.seqlen.append(int(bid_price / discount))\n",
    "                        self.market_price.append(int(market_price / discount))\n",
    "                        self.labels.append([1., 0.])  # so far we always lose, it means we still survial\n",
    "                else: # if b > z if our bid price (b) is bigger that the ture price we win\n",
    "                    if win: # dead \n",
    "                        #bid_price = 3\n",
    "                        #market_price = 2\n",
    "                        self.data.append(t_indices)  # data only conclude indices use this to get embedding\n",
    "                        self.seqlen.append(int(bid_price / discount))\n",
    "                        self.market_price.append(int(market_price / discount))\n",
    "                        self.labels.append([0., 1.])  # we win means we dead\n",
    "\n",
    "        self.max_d = max_d\n",
    "        fi.close()\n",
    "        self.size = len(self.data)\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.seqlen = np.array(self.seqlen)\n",
    "        self.market_price = np.array(self.market_price)\n",
    "        print(\"data size \", self.size, \"\\n\")\n",
    "        self.index = list(range(0, self.size)) # raiber modefied code to work with python 3.\n",
    "        self.data, self.seqlen, self.labels, self.market_price = self.shuffle() #  it shuffles the indxes \n",
    "        self.batch_id = 0\n",
    "\n",
    "\n",
    "    def next(self, batch_size): # returns a batch from data \n",
    "        if self.batch_id + batch_size > len(self.data):\n",
    "            self.data, self.seqlen, self.labels, self.market_price = self.shuffle()\n",
    "            self.batch_id = 0\n",
    "            self.finish_epoch = True\n",
    "        batch_data = self.data[self.batch_id:self.batch_id + batch_size]\n",
    "        batch_labels = self.labels[self.batch_id:self.batch_id + batch_size]\n",
    "        batch_seqlen = self.seqlen[self.batch_id:self.batch_id + batch_size]\n",
    "        batch_market_price = self.market_price[self.batch_id:self.batch_id + batch_size]\n",
    "        self.batch_id = self.batch_id + batch_size\n",
    "        return np.array(batch_data), np.array(batch_labels), np.array(batch_seqlen), np.array(batch_market_price)\n",
    "        \n",
    "\n",
    "class biSparseData():\n",
    "    def __init__(self, INPUT_FILE, discount):\n",
    "        random.seed(time.time())\n",
    "        #SparseData(INPUT_FILE, win, all, discount)\n",
    "        #self.winData.size = 12044, self.loseData.size = 39703, this is for the train data, loseData is the entier data \n",
    "        self.winData = SparseData(INPUT_FILE, True, False, discount) # only win data, uncensored \n",
    "        self.loseData = SparseData(INPUT_FILE, False, True, discount)#todo lose data get all data, even the win = False here, has no effect because the data\n",
    "        self.size = self.winData.size + self.loseData.size\n",
    "    def next(self, batch):\n",
    "        #win = int(random.random() * 100) % 11 == 1# todoe 1/10 get windata\n",
    "        win = int(random.random() * 100) % 11 <= 5 # int(random.random() * 100) % 11 gives values between 0 and 10,\n",
    "                                                      # the expression <= 5, will give us either true or false, \n",
    "                                                      # it is like take 50 % chance to get ture or false \n",
    "                                                      # which means randomly choose based on 50 probability from either win or lose data \n",
    "                                                      # again lose data contain the data\n",
    "        if win:\n",
    "            a, b, c, d = self.winData.next(batch)\n",
    "            return a, b, c, d, True\n",
    "        else:\n",
    "            a, b, c, d = self.loseData.next(batch)\n",
    "            return a, b, c, d, False\n",
    "\n",
    "\n",
    "class BASE_RNN():\n",
    "\n",
    "    train_data = None\n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random.normal(shape, stddev=0.1) # Outputs random values from a normal distribution.\n",
    "        # shape:\tA 1-D integer Tensor or Python array. The shape of the output tensor.\n",
    "        # stddev: standard deviation \n",
    "\n",
    "    def __init__(self,  EMB_DIM = 32,\n",
    "                        FEATURE_SIZE = 13,\n",
    "                        BATCH_SIZE = 128,\n",
    "                        MAX_DEN = 1580000,\n",
    "                        MAX_SEQ_LEN = 350,\n",
    "                        TRAING_STEPS = 100000,\n",
    "                        STATE_SIZE = 64,\n",
    "                        LR = 0.001,\n",
    "                        GRAD_CLIP = 5.0,\n",
    "                        L2_NORM = 0.001,\n",
    "                        INPUT_FILE = \"2259\",\n",
    "                        ALPHA = 1.0,\n",
    "                        BETA = 0.2,\n",
    "                        ADD_TIME_FEATURE=False,\n",
    "                        MIDDLE_FEATURE_SIZE = 30,\n",
    "                        LOG_FILE_NAME=None,\n",
    "                        FIND_PARAMETER = False,\n",
    "                        SAVE_LOG=True,\n",
    "                        OPEN_TEST=True,\n",
    "                        ONLY_TRAIN_ANLP=False,\n",
    "                        LOG_PREFIX=\"\",\n",
    "                        TEST_FREQUENT=True, # changed by raibrt to true \n",
    "                        ANLP_LR = 0.001,\n",
    "                        DNN_MODEL = False,\n",
    "                        QRNN_MODEL = False,\n",
    "                        GLOAL_STEP = 0,\n",
    "                        COV_SIZE = 1,\n",
    "                        DOUBLE_QRNN = False,\n",
    "                        ANLP_ROUND_ROBIN_RATE = 0.2,\n",
    "                        DISCOUNT = 1\n",
    "):\n",
    "        self.DISCOUNT = DISCOUNT\n",
    "        self.DOUBLE_QRNN = DOUBLE_QRNN\n",
    "        self.ANLP_ROUND_ROBIN_RATE = ANLP_ROUND_ROBIN_RATE\n",
    "        self.QRNN_MODEL = QRNN_MODEL\n",
    "        self.global_step = GLOAL_STEP\n",
    "        self.DNN_MODEL = DNN_MODEL\n",
    "        self.ANLP_LR = ANLP_LR\n",
    "        self.TEST_FREQUENT = TEST_FREQUENT\n",
    "        self.ONLY_TRAIN_ANLP = ONLY_TRAIN_ANLP\n",
    "        self.FIND_PARAMETER = FIND_PARAMETER\n",
    "        self.add_time_feature = ADD_TIME_FEATURE\n",
    "        self.MIDDLE_FEATURE_SIZE = MIDDLE_FEATURE_SIZE\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.TRAING_STEPS = TRAING_STEPS\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.STATE_SIZE = STATE_SIZE\n",
    "        self.EMB_DIM = EMB_DIM\n",
    "        self.FEATURE_SIZE = FEATURE_SIZE\n",
    "        self.MAX_DEN = MAX_DEN\n",
    "        self.MAX_SEQ_LEN = int(MAX_SEQ_LEN / self.DISCOUNT) + 10 # was changed by raiber to int to work with python 3\n",
    "        self.LR = LR\n",
    "        self.GRAD_CLIP = GRAD_CLIP\n",
    "        self.L2_NORM = L2_NORM\n",
    "        self.ALPHA = ALPHA\n",
    "        self.BETA = BETA\n",
    "        self.INPUT_FILE = INPUT_FILE # 2997\n",
    "        self.SAVE_LOG = SAVE_LOG\n",
    "        self.TRAIN_FILE = \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\ndb_p\\\\10\\\\train_yzb.txt\"\n",
    "        self.TEST_FILE = \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\ndb_p\\\\10\\\\test_yzb.txt\"\n",
    "        self.OPEN_TEST = OPEN_TEST # true\n",
    "        self.COV_SIZE = COV_SIZE # 1\n",
    "\n",
    "  \n",
    "\n",
    "        para = None\n",
    "        if LOG_FILE_NAME != None: # LOG_FILE_NAME is none \n",
    "            para = LOG_FILE_NAME\n",
    "        else:\n",
    "            para = LOG_PREFIX + str(self.EMB_DIM) + \"_\" + \\\n",
    "                str(BATCH_SIZE) + \"_\" + \\\n",
    "                str(self.STATE_SIZE) + \"_\" + \\\n",
    "                \"{:.6f}\".format(self.LR) + \"_\" + \"{:.6f}\".format(self.ANLP_LR) + \"_\" + \\\n",
    "                \"{:.6f}\".format(self.L2_NORM) + \"_\" + \\\n",
    "                INPUT_FILE + \"_\" + \\\n",
    "                \"{:.2f}\".format(self.ALPHA) + \"_\" \\\n",
    "                \"{:.2f}\".format(self.BETA) + \"_\" + str(ADD_TIME_FEATURE) + \\\n",
    "                   \"_\" + str(self.QRNN_MODEL) + \"_\" + str(self.COV_SIZE) + \"_\" + str(DISCOUNT)\n",
    "        print (para, '\\n')\n",
    "        self.filename = para\n",
    "        self.train_log_txt_filename = \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\\" + para + '.train.log.txt'\n",
    "        if os.path.exists(self.train_log_txt_filename):\n",
    "            self.exist = True\n",
    "        else:\n",
    "            if self.SAVE_LOG:\n",
    "                self.exist = False\n",
    "                self.train_log_txt = open(self.train_log_txt_filename, 'w')\n",
    "                self.train_log_txt.close()\n",
    "\n",
    "    def get_survival_data(self, model, sess):\n",
    "        alltestdata = SparseData(self.TEST_FILE, True, True) #all test data\n",
    "        ret = []\n",
    "        while alltestdata.finish_epoch == False:\n",
    "            test_batch_x, test_batch_y, test_batch_len, test_batch_market_price = alltestdata.next(self.BATCH_SIZE)\n",
    "            bid_loss, bid_test_prob, anlp, preds = sess.run(\n",
    "                [self.cost, self.predict, self.anlp_node, self.preds],\n",
    "                feed_dict={self.tf_x: test_batch_x,\n",
    "                           self.tf_y: test_batch_y,\n",
    "                           self.tf_bid_len: test_batch_len,\n",
    "                           self.tf_market_price: test_batch_market_price\n",
    "                           })\n",
    "            ret.append(preds)\n",
    "        return ret # return will have the preds \n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        self.train_data = biSparseData(self.TRAIN_FILE, self.DISCOUNT)\n",
    "        self.test_data_win = SparseData(self.TEST_FILE, True, False, self.DISCOUNT) # win, only win test data \n",
    "        self.test_data_lose = SparseData(self.TEST_FILE, False, False, self.DISCOUNT) #lose, only lose test data\n",
    "\n",
    "    def is_exist(self):\n",
    "        if self.SAVE_LOG == False:\n",
    "            return False\n",
    "        return self.exist\n",
    "\n",
    "\n",
    "    def create_graph(self):\n",
    "        BATCH_SIZE = self.BATCH_SIZE\n",
    "        self.tf_x = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE, self.FEATURE_SIZE], name=\"tf_x\")\n",
    "        self.tf_y = tf.compat.v1.placeholder(tf.float32, [BATCH_SIZE, 2], name=\"tf_y\")\n",
    "        self.tf_bid_len = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE], name=\"tf_len\")\n",
    "        self.tf_market_price = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE], name=\"tf_market_price\")\n",
    "        self.tf_control_parameter = tf.compat.v1.placeholder(tf.float32, [2], name=\"tf_control_parameter\")\n",
    "        alpha = self.tf_control_parameter[0]\n",
    "        beta = self.tf_control_parameter[1]\n",
    "        self.tf_rnn_len = tf.maximum(self.tf_bid_len, self.tf_market_price) + 2 # the longest time between b and z plus 2 \n",
    "        embeddings = tf.Variable(self.init_matrix([self.MAX_DEN, self.EMB_DIM]))\n",
    "        x_emds = tf.nn.embedding_lookup(params=embeddings, ids=self.tf_x)\n",
    "        input = tf.reshape(x_emds, [BATCH_SIZE, self.FEATURE_SIZE * self.EMB_DIM])\n",
    "        input_x = None\n",
    "        if self.add_time_feature: # which is true \n",
    "            middle_layer = tf.compat.v1.layers.dense(input, self.MIDDLE_FEATURE_SIZE, tf.nn.relu)  # hidden layer\n",
    "\n",
    "            def add_time(x):\n",
    "                y = tf.reshape(tf.tile(x, [self.MAX_SEQ_LEN]), [self.MAX_SEQ_LEN, self.MIDDLE_FEATURE_SIZE])\n",
    "                t = tf.reshape(tf.range(self.MAX_SEQ_LEN), [self.MAX_SEQ_LEN, 1])\n",
    "                z = tf.concat([y, tf.cast(t, dtype=tf.float32)], 1)\n",
    "                return z\n",
    "\n",
    "            input_x = tf.map_fn(add_time, middle_layer)\n",
    "\n",
    "        preds = None\n",
    "\n",
    "        if self.DNN_MODEL:\n",
    "            outlist = []\n",
    "            for i in range(0, self.BATCH_SIZE):\n",
    "                sigleout = tf.compat.v1.layers.dense(input_x[i], 1, tf.nn.sigmoid)\n",
    "                outlist.append(sigleout)\n",
    "            preds = tf.reshape(tf.stack(outlist, axis=0), [self.BATCH_SIZE, self.MAX_SEQ_LEN], name=\"preds\")\n",
    "        else:\n",
    "            # input_x = tf.reshape(tf.tile(input, [1, self.MAX_SEQ_LEN]), [BATCH_SIZE, self.MAX_SEQ_LEN, self.FEATURE_SIZE * self.EMB_DIM])\n",
    "            rnn_cell = None\n",
    "            rnn_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units=self.STATE_SIZE)\n",
    "\n",
    "\n",
    "            outputs, (h_c, h_n) = tf.compat.v1.nn.dynamic_rnn(\n",
    "                rnn_cell,                   # cell you have chosen\n",
    "                input_x,                    # input\n",
    "                initial_state=None,         # the initial hidden state\n",
    "                dtype=tf.float32,           # must given if set initial_state = None\n",
    "                time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)\n",
    "                sequence_length=self.tf_rnn_len\n",
    "            )\n",
    "\n",
    "            new_output = tf.reshape(outputs, [self.MAX_SEQ_LEN * BATCH_SIZE, self.STATE_SIZE])\n",
    "\n",
    "            with tf.compat.v1.variable_scope('softmax'):\n",
    "                W = tf.compat.v1.get_variable('W', [self.STATE_SIZE, 1])\n",
    "                b = tf.compat.v1.get_variable('b', [1], initializer=tf.compat.v1.constant_initializer(0))\n",
    "\n",
    "            logits = tf.matmul(new_output, W) + b\n",
    "            preds = tf.transpose(a=tf.nn.sigmoid(logits, name=\"preds\"), name=\"preds\")[0]\n",
    "\n",
    "\n",
    "        self.preds = preds\n",
    "        survival_rate = preds \n",
    "        death_rate = tf.subtract(tf.constant(1.0, dtype=tf.float32), survival_rate)\n",
    "        batch_rnn_survival_rate = tf.reshape(survival_rate, [BATCH_SIZE, self.MAX_SEQ_LEN])\n",
    "        batch_rnn_death_rate = tf.reshape(death_rate, [BATCH_SIZE, self.MAX_SEQ_LEN])\n",
    "        \n",
    "        # this line added by raiber, because I want the survival rate at every single time, from 1 to self.MAX_SEQ_LEN\n",
    "        # the rate _result below is a multipication of different survival rate from time 1 to the last time, so the survival time at last tiem only \n",
    "        self.survival_rate_different_times = batch_rnn_survival_rate\n",
    "        self.death_rate_different_times = batch_rnn_death_rate\n",
    "\n",
    "        map_parameter = tf.concat([batch_rnn_survival_rate,\n",
    "                                   tf.cast(tf.reshape(self.tf_bid_len, [BATCH_SIZE, 1]), tf.float32)],\n",
    "                                  1)\n",
    "        map_parameter = tf.concat([map_parameter,\n",
    "                                   tf.cast(tf.reshape(self.tf_market_price, [BATCH_SIZE, 1]), tf.float32)],\n",
    "                                  1)\n",
    "        def reduce_mul(x):\n",
    "            bid_len = tf.cast(x[self.MAX_SEQ_LEN], dtype=tf.int32)\n",
    "            market_len = tf.cast(x[self.MAX_SEQ_LEN + 1], dtype=tf.int32)\n",
    "            # survival_rate_last_one is the survival rate at the obsevation time, which is bid_length\n",
    "            # we are muliplying the survival rates at each single time from 1 to observation time \n",
    "            survival_rate_last_one = tf.reduce_prod(input_tensor=x[0:bid_len])\n",
    "            anlp_rate_last_one = tf.reduce_prod(input_tensor=x[0:market_len + 1])\n",
    "            anlp_rate_last_two = tf.reduce_prod(input_tensor=x[0:market_len])\n",
    "            ret = tf.stack([survival_rate_last_one, anlp_rate_last_one, anlp_rate_last_two])\n",
    "            return ret\n",
    "\n",
    "        self.mp_para = map_parameter\n",
    "        rate_result = tf.map_fn(reduce_mul, elems=map_parameter ,name=\"rate_result\")\n",
    "        self.rate_result = rate_result\n",
    "        log_minus = tf.math.log(tf.add(tf.transpose(a=rate_result)[2] - tf.transpose(a=rate_result)[1], 1e-20))#todo debug\n",
    "\n",
    "\n",
    "        self.anlp_node = -tf.reduce_sum(input_tensor=log_minus) / self.BATCH_SIZE #todo load name\n",
    "        self.anlp_node = tf.add(self.anlp_node, 0, name=\"anlp_node\")\n",
    "        self.final_survival_rate = tf.transpose(a=rate_result)[0]\n",
    "        final_dead_rate = tf.subtract(tf.constant(1.0, dtype=tf.float32), self.final_survival_rate)\n",
    "\n",
    "        self.predict = tf.transpose(a=tf.stack([self.final_survival_rate, final_dead_rate]), name=\"predict\")\n",
    "        cross_entropy = -tf.reduce_sum(input_tensor=self.tf_y*tf.math.log(tf.clip_by_value(self.predict,1e-10,1.0))) # tf.clip_by_value\n",
    "                                                                                                    #this operation returns a tensor of the same type and shape as t \n",
    "                                                                                                    #with its values clipped to clip_value_min and clip_value_max. \n",
    "                                                                                                    #Any values less than clip_value_min are set to clip_value_min. \n",
    "                                                                                                    #Any values greater than clip_value_max are set to clip_value_max.\n",
    "\n",
    "        tvars = tf.compat.v1.trainable_variables()\n",
    "        lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in tvars ]) * self.L2_NORM\n",
    "        cost = tf.add(cross_entropy, lossL2, name = \"cost\")  / self.BATCH_SIZE\n",
    "        self.cost = tf.add(cost, 0, name=\"cost\")\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.LR, beta2=0.99)#.minimize(cost)\n",
    "        optimizer_anlp = tf.compat.v1.train.AdamOptimizer(learning_rate=self.ANLP_LR, beta2=0.99)#.minimize(cost)\n",
    "\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(ys=self.cost, xs=tvars),\n",
    "                                          self.GRAD_CLIP,\n",
    "                                          )\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train_op\")\n",
    "        tf.compat.v1.add_to_collection('train_op', self.train_op)\n",
    "\n",
    "        anlp_grads, _ = tf.clip_by_global_norm(tf.gradients(ys=self.anlp_node, xs=tvars),\n",
    "                                          self.GRAD_CLIP,\n",
    "                                          )\n",
    "        self.anlp_train_op = optimizer_anlp.apply_gradients(zip(anlp_grads, tvars), name=\"anlp_train_op\")\n",
    "        tf.compat.v1.add_to_collection('anlp_train_op', self.anlp_train_op)\n",
    "\n",
    "\n",
    "        self.com_cost = tf.add(alpha * self.cost, beta * self.anlp_node)\n",
    "        com_grads, _ = tf.clip_by_global_norm(tf.gradients(ys=self.com_cost, xs=tvars),\n",
    "                                          self.GRAD_CLIP,\n",
    "                                          )\n",
    "\n",
    "        self.com_train_op = optimizer.apply_gradients(zip(com_grads, tvars), name=\"train_op\")\n",
    "        tf.compat.v1.add_to_collection('com_train_op', self.com_train_op)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(input=self.predict, axis=1), tf.argmax(input=self.tf_y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_pred, tf.float32), name=\"accuracy\")\n",
    "\n",
    "\n",
    "    def train_test(self,sess):\n",
    "        self.load_data()\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        self.sess = sess\n",
    "        sess.run(init)\n",
    "        saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
    "        self.saver = saver\n",
    "        TRAIN_LOG_STEP = int((self.train_data.size * 0.1) / self.BATCH_SIZE)\n",
    "        train_auc_arr = []\n",
    "        train_anlp_arr = []\n",
    "        train_loss_arr = []\n",
    "        train_auc_label = []\n",
    "        train_death_label = []#added by raiber\n",
    "        train_auc_prob = []\n",
    "        train_time = [] #added by raiber \n",
    "        train_m_time = [] #added by raiber \n",
    "        total_train_duration = 0\n",
    "        total_test_duration = 0\n",
    "        TEST_COUNT = 0\n",
    "        max_auc = -1\n",
    "        min_anlp = 200\n",
    "        enough_test = 0\n",
    "        last_loss = [9999.0, 9999.0]\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #added by raiber\n",
    "        self.max_valid = -99\n",
    "        self.stop_flag = 0\n",
    "        \n",
    "        for step in range(1, self.TRAING_STEPS + 1):\n",
    "            if self.stop_flag > 5: #for faster early stopping\n",
    "                break\n",
    "            self.global_step = step\n",
    "            batch_x, batch_y, batch_len, batch_market_price, win = self.train_data.next(self.BATCH_SIZE)\n",
    "            if self.ONLY_TRAIN_ANLP:\n",
    "                if win: #if win\n",
    "                    _, train_anlp, train_loss, train_outputs = sess.run([self.com_train_op, self.anlp_node, self.cost, self.predict],\n",
    "                                                                    feed_dict={self.tf_x: batch_x,\n",
    "                                                                             self.tf_y: batch_y,\n",
    "                                                                             self.tf_bid_len: batch_len,\n",
    "                                                                             self.tf_market_price:batch_market_price,\n",
    "                                                                             self.tf_control_parameter:[self.ALPHA, self.BETA]\n",
    "                                                       })\n",
    "                    train_anlp_arr.append(train_anlp)\n",
    "                    train_loss_arr.append(train_loss)\n",
    "                    train_auc_label.append(batch_y.T[0])\n",
    "                    train_auc_prob.append(np.array(train_outputs).T[0])\n",
    "                else:\n",
    "                    train_loss, train_outputs = sess.run([self.cost, self.predict], feed_dict={self.tf_x: batch_x,\n",
    "                                                       self.tf_y: batch_y,\n",
    "                                                       self.tf_bid_len: batch_len,\n",
    "                                                       self.tf_market_price:batch_market_price,\n",
    "                                                       self.tf_control_parameter:[self.ALPHA, self.BETA]\n",
    "                                                       })\n",
    "                    #print train_outputs\n",
    "                    train_loss_arr.append(train_loss)\n",
    "                    train_auc_label.append(batch_y.T[0])\n",
    "                    train_auc_prob.append(np.array(train_outputs).T[0])\n",
    "            else:\n",
    "                if win: #if win\n",
    "                    _, train_anlp, train_loss, train_outputs, preds= sess.run([self.com_train_op, self.anlp_node, self.cost, self.predict, self.preds],\n",
    "                                                                    feed_dict={self.tf_x: batch_x,\n",
    "                                                                             self.tf_y: batch_y,\n",
    "                                                                             self.tf_bid_len: batch_len,\n",
    "                                                                             self.tf_market_price:batch_market_price,\n",
    "                                                                             self.tf_control_parameter:[self.ALPHA, self.BETA]\n",
    "                                                       })\n",
    "                    train_anlp_arr.append(train_anlp)\n",
    "                    train_loss_arr.append(train_loss)\n",
    "                    train_auc_label.append(batch_y.T[0])\n",
    "                    train_death_label.append(batch_y.T[1])\n",
    "                    train_auc_prob.append(np.array(train_outputs).T[0])\n",
    "                    train_time.append(batch_market_price)\n",
    "                    train_m_time.append(batch_market_price)\n",
    "                else:\n",
    "                    _, train_loss, train_outputs = sess.run([self.train_op, self.cost, self.predict], feed_dict={self.tf_x: batch_x,\n",
    "                                                       self.tf_y: batch_y,\n",
    "                                                       self.tf_bid_len: batch_len,\n",
    "                                                       self.tf_market_price:batch_market_price,\n",
    "                                                       self.tf_control_parameter:[self.ALPHA, self.BETA]\n",
    "                                                       })\n",
    "                    #print train_outputs\n",
    "                    train_loss_arr.append(train_loss) \n",
    "                    train_auc_label.append(batch_y.T[0]) # in the first loop ww will have 100 list (number of steps), each one with 128 elements (batch size)\n",
    "                    train_death_label.append(batch_y.T[1])\n",
    "                    train_auc_prob.append(np.array(train_outputs).T[0])\n",
    "                    train_time.append(batch_len)\n",
    "                    train_m_time.append(batch_market_price)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                mean_anlp = np.array(train_anlp_arr[-99:]).mean()\n",
    "                mean_loss = np.array(train_loss_arr[-99:]).mean()\n",
    "                mean_auc = 0.0001\n",
    "                if not self.ONLY_TRAIN_ANLP:\n",
    "                    try:\n",
    "                        mean_auc = roc_auc_score(np.reshape(train_auc_label, [1, -1])[0], np.reshape(train_auc_prob, [1, -1])[0])\n",
    "                    except Exception:\n",
    "                        print(\"AUC ERROE\")\n",
    "                        continue\n",
    "\n",
    "                log = self.getStatStr(\"TRAIN\", self.global_step, mean_auc, mean_loss, mean_anlp)\n",
    "                print(log)\n",
    "                self.force_write(log)\n",
    "                train_loss_arr = []\n",
    "                train_anlp_arr = []\n",
    "                train_auc_label = []\n",
    "                train_auc_prob = []\n",
    "                if self.TEST_FREQUENT:\n",
    "                    self.run_test(sess)\n",
    "                    #self.save_model()\n",
    "\n",
    "            #if self.global_step < 1000: # raiber changed from 300 to 1000, just for experimenting \n",
    "            #    if self.global_step % 100 == 0:\n",
    "            #        self.run_test(sess, tr_time = train_time, tr_label = train_auc_label)\n",
    "                    #self.save_model()\n",
    "            #elif self.global_step < 2000:\n",
    "            #    if self.global_step % 500 == 0:\n",
    "            #        self.run_test(sess, tr_time = train_time, tr_label = train_auc_label)\n",
    "            #        #self.save_model()\n",
    "            #elif self.global_step < 10000:\n",
    "            #    if self.global_step % 2000 == 0:\n",
    "            #        self.run_test(sess, tr_time = train_time, tr_label = train_auc_label)\n",
    "                    #self.save_model()\n",
    "            #elif self.global_step <= 50000:\n",
    "            #    if self.global_step % 2000 == 0:\n",
    "            #        self.run_test(sess, tr_time = train_time, tr_label = train_auc_label)\n",
    "                    #self.save_model()\n",
    "            #else:\n",
    "            #    break\n",
    "\n",
    "    def run_model(self):\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.compat.v1.Session(config=config) as sess:\n",
    "            self.train_test(sess)\n",
    "\n",
    "    def save_model(self):\n",
    "        print(\"model name: \", self.filename, \" \", self.global_step, \"\\n\")\n",
    "        self.saver.save(self.sess, \"C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\saved_model\\\\model\" + self.filename, global_step=self.global_step)\n",
    "\n",
    "    def getStatStr(self, category ,step, mean_auc, mean_loss, mean_anlp):\n",
    "        statistics_log = str(self.INPUT_FILE) + \"\\t\" + category + \"\\t\" + str(step) + \"\\t\" \\\n",
    "                         \"{:.6f}\".format(mean_loss) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_auc) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_anlp) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(self.ALPHA * mean_loss + self.BETA * mean_anlp) + \\\n",
    "                         str(self.EMB_DIM) + \"\\t\" + str(self.BATCH_SIZE) + \"\\t\" + \\\n",
    "                         str(self.STATE_SIZE) + \"\\t\" + \\\n",
    "                         \"{:.6f}\".format(self.LR) + \"\\t\" + \\\n",
    "                         \"{:.6f}\".format(self.ANLP_LR) + \"\\t\" + \\\n",
    "                         \"{:.6}\".format(self.L2_NORM) + \"\\t\" +\\\n",
    "                         str(self.ALPHA) + '\\t' + \\\n",
    "                         str(self.BETA) + \"\\n\"\n",
    "        return statistics_log\n",
    "\n",
    "    def getStatStr_test(self, category ,step, mean_auc, mean_lc, mean_br, mean_loss, mean_anlp):\n",
    "        statistics_log = str(self.INPUT_FILE) + \"\\t\" + category + \"\\t\" + str(step) + \"\\t\" \\\n",
    "                         \"{:.6f}\".format(mean_loss) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_auc) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_lc) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_br) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(mean_anlp) + \"\\t\" + \\\n",
    "                         \"{:.4f}\".format(self.ALPHA * mean_loss + self.BETA * mean_anlp) + \\\n",
    "                         str(self.EMB_DIM) + \"\\t\" + str(self.BATCH_SIZE) + \"\\t\" + \\\n",
    "                         str(self.STATE_SIZE) + \"\\t\" + \\\n",
    "                         \"{:.6f}\".format(self.LR) + \"\\t\" + \\\n",
    "                         \"{:.6f}\".format(self.ANLP_LR) + \"\\t\" + \\\n",
    "                         \"{:.6}\".format(self.L2_NORM) + \"\\t\" +\\\n",
    "                         str(self.ALPHA) + '\\t' + \\\n",
    "                         str(self.BETA) + \"\\n\"\n",
    "        return statistics_log\n",
    "\n",
    "    def load(self, meta, ckpt, step): # this returns sess (session)\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        saver = tf.compat.v1.train.import_meta_graph(meta)\n",
    "        #self.load_data()\n",
    "        self.global_step = step\n",
    "        #with tf.Session(config=config) as sess:\n",
    "        sess = tf.compat.v1.Session(config=config)\n",
    "        saver.restore(sess, ckpt)\n",
    "        graph = tf.compat.v1.get_default_graph()\n",
    "        self.tf_x = graph.get_tensor_by_name(\"tf_x:0\")\n",
    "        self.tf_y = graph.get_tensor_by_name(\"tf_y:0\")\n",
    "        self.tf_bid_len = graph.get_tensor_by_name(\"tf_len:0\")\n",
    "        self.tf_market_price = graph.get_tensor_by_name(\"tf_market_price:0\")\n",
    "        self.accuracy = graph.get_tensor_by_name(\"accuracy:0\")\n",
    "        self.cost = graph.get_tensor_by_name(\"cost:0\")\n",
    "        self.predict = graph.get_tensor_by_name(\"predict:0\")\n",
    "        self.anlp_node = graph.get_tensor_by_name(\"anlp_node:0\")\n",
    "        self.train_op = tf.compat.v1.get_collection('train_op')[0]\n",
    "\n",
    "        #self.anlp_train_op = graph.get_collection(\"anlp_train_op\")[0]\n",
    "        #self.train _op = graph.get_tensor_by_name(\"train_op:0\")\n",
    "        self.preds = graph.get_tensor_by_name(\"preds:0\")\n",
    "        #self.com_train_op = tf.get_collection(\"com_train_op\")[0]\n",
    "        #self.tf_control_parameter = graph.get_tensor_by_name(\"tf_control_parameter:0\")\n",
    "        # self.train_log_txt.write(statistics_log)\n",
    "        return sess\n",
    "\n",
    "    def run_test(self, sess):\n",
    "        auc_arr = []\n",
    "        s_t_sr = []\n",
    "        s_t_dr = []\n",
    "        cind_arr = []#added by raiber\n",
    "        m_br_arr = []#added by raiber\n",
    "        br_arr = []#added by raiber\n",
    "        lc_arr = []#added by raiber\n",
    "        surv_cind_arr = []#added by raiber\n",
    "        surv_br_arr = []#added by raiber\n",
    "        loss_arr = []\n",
    "        anlp_arr = []\n",
    "        auc_prob = []\n",
    "        death_prob = []#added by raiber\n",
    "        auc_label = []\n",
    "        cindex_label = []#added by raiber\n",
    "        auc_time = [] # added by raiber, to get the batch time \n",
    "        m_time = [] # added by raiber\n",
    "        b_time = [] # added by raiber\n",
    "        #print self.test_data_win.size + self.test_data_lose.size, \"total size\"\n",
    "        total_time = 0\n",
    "        for i in range(0, int(self.test_data_win.size / self.BATCH_SIZE)):\n",
    "            test_batch_x, test_batch_y, test_batch_len, test_batch_market_price = self.test_data_win.next(\n",
    "                self.BATCH_SIZE)\n",
    "            start_time = time.time()\n",
    "            bid_loss, bid_test_prob, anlp, single_time_survival, single_time_death = sess.run(\n",
    "                [self.cost, self.predict, self.anlp_node, self.survival_rate_different_times, self.death_rate_different_times],\n",
    "                feed_dict={self.tf_x: test_batch_x,\n",
    "                           self.tf_y: test_batch_y,\n",
    "                           self.tf_bid_len: test_batch_len,\n",
    "                           self.tf_market_price:test_batch_market_price\n",
    "                           })\n",
    "            total_time += time.time() - start_time\n",
    "            s_t_sr.append(np.array(single_time_survival))\n",
    "            s_t_dr.append(np.array(single_time_death))\n",
    "            auc_prob.append(np.array(bid_test_prob).T[0]) # bid_test_prob is the result from self.predict, and self.predict is stacked and tranposed\n",
    "                                                          #array fron survival rates and deas rates, so when we do np.array(bid_test_prob).T[0]\n",
    "                                                          # we get back only the survival rates \n",
    "            death_prob.append(np.array(bid_test_prob).T[1])\n",
    "            auc_label.append(test_batch_y.T[0]) # because our label is either [0,1] or [1,0], we only care about the first index [0]\n",
    "            cindex_label.append(test_batch_y.T[1])\n",
    "            # changed by raiber from auc_label.append(test_batch_y.T[0]) to auc_label.append(test_batch_y.T[1])\n",
    "            anlp_arr.append(anlp)\n",
    "            loss_arr.append(bid_loss)\n",
    "            auc_time.append(test_batch_market_price) # added by raiber\n",
    "            m_time.append(test_batch_market_price)# added by raiber\n",
    "            b_time.append(test_batch_len)# added by raiber\n",
    "        mean_loss = np.array(loss_arr).mean()\n",
    "        mean_anlp = np.array(anlp_arr).mean()\n",
    "        log = self.getStatStr(\"TEST_WIN_DATA\", self.global_step, 0.000001, mean_loss, mean_anlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #result1, result2 = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "        #result1a, result2a = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "        #for t, t_time in enumerate(EVAL_TIMES):\n",
    "        #eval_horizon = int(t_time)\n",
    "\n",
    "        #if eval_horizon >= num_Category:\n",
    "        #    print( 'ERROR: evaluation horizon is out of range')\n",
    "        #    result1[:, t] = result2[:, t] = -1\n",
    "        #else:\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "        #    risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n",
    "        #    for k in range(num_Event):\n",
    "        #        result1a[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "        #        result2a[k, t] = brier_score(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "        #        result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "        #        result2[k, t] = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "        #FINAL1[:, :, out_itr] = result1\n",
    "        #FINAL2[:, :, out_itr] = result2\n",
    "\n",
    "\n",
    "        \n",
    "        print(log)\n",
    "        for i in range(0, int(self.test_data_lose.size / self.BATCH_SIZE)):\n",
    "            test_batch_x, test_batch_y, test_batch_len, test_batch_market_price = self.test_data_lose.next(\n",
    "                self.BATCH_SIZE)\n",
    "            bid_loss, bid_test_prob, anlp, single_time_survival, single_time_death= sess.run(\n",
    "                                   [self.cost, self.predict,self.anlp_node, self.survival_rate_different_times, self.death_rate_different_times],\n",
    "                                   feed_dict={self.tf_x: test_batch_x,\n",
    "                                              self.tf_y: test_batch_y,\n",
    "                                              self.tf_bid_len: test_batch_len,\n",
    "                                              self.tf_market_price: test_batch_market_price\n",
    "                                              })\n",
    "            s_t_sr.append(np.array(single_time_survival))\n",
    "            s_t_dr.append(np.array(single_time_death))\n",
    "            auc_prob.append(np.array(bid_test_prob).T[0])\n",
    "            death_prob.append(np.array(bid_test_prob).T[1])\n",
    "            auc_label.append(test_batch_y.T[0])\n",
    "            cindex_label.append(test_batch_y.T[1])\n",
    "            # changed by raiber from auc_label.append(test_batch_y.T[0]) to auc_label.append(test_batch_y.T[1])\n",
    "            anlp_arr.append(anlp)\n",
    "            loss_arr.append(bid_loss)\n",
    "            auc_time.append(test_batch_len) # added by raiber \n",
    "            m_time.append(test_batch_market_price)# added by raiber \n",
    "            b_time.append(test_batch_len)# added by raiber\n",
    "        if len(auc_prob) > 0:\n",
    "            #pdb.set_trace()\n",
    "            try:  \n",
    "                ll = np.array(s_t_sr).transpose(2,0,1).reshape(self.MAX_SEQ_LEN,-1)\n",
    "                l_time = np.reshape(np.array(auc_time), [1, -1])[0]\n",
    "                l_label = np.reshape(np.array(cindex_label), [1, -1])[0]\n",
    "                df = pd.DataFrame(ll[:l_time.max() + 1,:]) # ll[:l_time.max() + 1,:]\n",
    "                ev = EvalSurv(df, l_time, l_label, censor_surv='km')\n",
    "                pyind = ev.concordance_td()\n",
    "                time_grid = np.linspace(l_time.min(), l_time.max(), l_time.max())\n",
    "                pybr= ev.integrated_brier_score(time_grid) \n",
    "                #y_true = ((Time_survival <= Time) * Death).astype(float)\n",
    "                #arr[:, :2]\n",
    "                auc = roc_auc_score(np.reshape(np.array(auc_label), [1, -1])[0],\n",
    "                                np.reshape(np.array(auc_prob), [1, -1])[0])\n",
    "                #cind_m_time_e = concordance_index(np.reshape(np.array(m_time), [1, -1])[0], \n",
    "                #                                np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0])\n",
    "                #cind_b_time_e = concordance_index(np.reshape(np.array(b_time), [1, -1])[0], \n",
    "                #                                np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0])\n",
    "                #cind_b_time_cen = concordance_index(np.reshape(np.array(b_time), [1, -1])[0], \n",
    "                #                                np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0])\n",
    "                #surv_label = np.reshape(np.array(cindex_label), [1, -1])[0] > 0\n",
    "                #surv_c_ind1 = concordance_index_censored(surv_label, np.reshape(np.array(auc_time), [1, -1])[0], \n",
    "                #                np.reshape(np.array(death_prob), [1, -1])[0])\n",
    "                #cind_auc_time_e = surv_c_ind1[0]\n",
    "                \n",
    "                #print(\"m_time_e:{0}, b_time_e:{1}, b_time_cen:{2}, auctime_e:{3}, \".format(cind_m_time_e, cind_b_time_e, cind_b_time_cen, cind_auc_time_e))\n",
    "                \n",
    "                #### using t\n",
    "                #i_time = max(np.reshape(np.array(auc_time), [1, -1])[0]) - 1\n",
    "                #r_test = np.core.records.fromarrays([surv_label, np.reshape(np.array(auc_time), [1, -1])[0]], names='cens,time')\n",
    "                #bla, surv_brscore1 = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], i_time)\n",
    "                \n",
    "                #surv_brscore = np.zeros([3])\n",
    "                #bla, surv_brscore[0] = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], 18)\n",
    "                #bla, surv_brscore[1] = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], 36)\n",
    "                #bla, surv_brscore[2] = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], i_time)\n",
    "                \n",
    "                #surv = np.mean(surv_brscore)\n",
    "                #print(\"surv_brscore_auc_time:{0}\".format(surv_brscore))\n",
    "                \n",
    "                #### using z\n",
    "                #m_i_time = max(np.reshape(np.array(m_time), [1, -1])[0]) - 1\n",
    "                #m_r_test = np.core.records.fromarrays([surv_label, np.reshape(np.array(m_time), [1, -1])[0]], names='cens,time')\n",
    "                #m_bla, m_surv_brscore1 = brier_score(m_r_train,m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], m_i_time)\n",
    "                \n",
    "                #m_surv_brscore = np.zeros([3])\n",
    "                #m_bla, m_surv_brscore[0] = brier_score(m_r_train, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], 18)\n",
    "                #m_bla, m_surv_brscore[1] = brier_score(m_r_train, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], 36)\n",
    "                #m_bla, m_surv_brscore[2] = brier_score(m_r_train, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], 60)\n",
    "                \n",
    "                #m_surv = np.mean(m_surv_brscore)\n",
    "                #print(\"surv_brscore_m_time:{0}\".format(m_surv_brscore))\n",
    "                \n",
    "                \n",
    "                ####### train \n",
    "                #tr_tt = np.reshape(np.array(tr_time), [1, -1])[0]\n",
    "                #tr_m_tt = np.reshape(np.array(tr_m_time), [1, -1])[0]\n",
    "                #tr_surv_label = np.reshape(np.array(tr_label), [1, -1])[0] #> 0\n",
    "                \n",
    "                #surv_brscore1 = weighted_brier_score(tr_tt, tr_surv_label, np.reshape(np.array(death_prob), [1, -1])[0], \n",
    "                #                     np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0], 61)\n",
    "                #m_surv_brscore1 = weighted_brier_score(tr_m_tt, tr_surv_label, np.reshape(np.array(death_prob), [1, -1])[0], \n",
    "                #                     np.reshape(np.array(m_time), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0], 61)\n",
    "                ##\n",
    "                #idx_t = tr_tt < 52\n",
    "                #tr_tt_a = tr_tt[idx]\n",
    "                #tr_surv_label_a = tr_surv_label[idx]\n",
    "                ##\n",
    "                #r_train_n_a = np.core.records.fromarrays([tr_surv_label_a, tr_tt_a], names='cens,time')\n",
    "                #r_train_n = np.core.records.fromarrays([tr_surv_label, tr_tt ], names='cens,time')\n",
    "                ############\n",
    "                #tt = np.reshape(np.array(auc_time), [1, -1])[0]\n",
    "                #m_tt = np.reshape(np.array(m_time), [1, -1])[0]\n",
    "                #i_time = max(tt) \n",
    "                #lm_time = max(m_tt) - 1 \n",
    "                #m_i_time = min(max(m_tt),max(tr_tt)) + 1\n",
    "                #idx = m_tt < m_i_time\n",
    "                #m_tt_a = m_tt[idx]\n",
    "                #surv_label_a = surv_label[idx]\n",
    "                #r_test = np.core.records.fromarrays([surv_label, tt], names='cens,time')\n",
    "                #m_r_test = np.core.records.fromarrays([surv_label_a, m_tt_a], names='cens,time')\n",
    "                #bla, surv_brscore = brier_score(r_train, r_test, np.reshape(np.array(death_prob), [1, -1])[0], i_time)\n",
    "                #m_bla, m_surv_brscore = brier_score(r_train, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], m_i_time)\n",
    "                \n",
    "                ####train\n",
    "                #bla1, surv_brscore1 = brier_score(r_train_n, r_test, np.reshape(np.array(death_prob), [1, -1])[0], i_time)\n",
    "                #m_bla1, m_surv_brscore1 = brier_score(r_train_n, m_r_test, np.reshape(np.array(death_prob), [1, -1])[0], m_i_time)\n",
    "                ####\n",
    "                \n",
    "                #if eval_time is None:\n",
    "                #    eval_time = [int(np.percentile(tr_time, 25)), int(np.percentile(tr_time, 50)), int(np.percentile(tr_time, 75))]\n",
    "                ### VALIDATION  (based on average C-index of our interest)\n",
    "                #eval_time = [18,36,60]\n",
    "\n",
    "                ### EVALUATION\n",
    "                #va_result = np.zeros([len(eval_time)])\n",
    "                #va_result_m = np.zeros([len(eval_time)])\n",
    "                #br_result = np.zeros([len(eval_time)])\n",
    "                #br_result_m = np.zeros([len(eval_time)])\n",
    "                #s_t_sr_array = np.array(s_t_sr)\n",
    "                #s_t_dr_array = np.array(s_t_dr)\n",
    "                #for t, t_time in enumerate(eval_time):\n",
    "                #    eval_horizon = int(t_time)\n",
    "                #    \n",
    "                #    idx_observed = np.reshape(np.array(auc_time), [1, -1])[0] <= eval_horizon\n",
    "                #    labels = np.reshape(np.array(cindex_label), [1, -1])[0] * idx_observed\n",
    "                    #auc_ll = (labels < 1).astype(int)\n",
    "                    #idx_observed_m = np.reshape(np.array(m_time), [1, -1])[0] <= eval_horizon\n",
    "                    #labels_m = np.reshape(np.array(cindex_label), [1, -1])[0] * idx_observed_m \n",
    "                    \n",
    "                #    surv_prob = np.prod(s_t_sr_array[:,:,:(eval_horizon+1)], axis=2) #survival score until eval_time\n",
    "                #    risk_prob = np.prod(s_t_dr_array[:,:,:(eval_horizon+1)], axis=2)\n",
    "                    #auc_result[t] = roc_auc_score(auc_ll,\n",
    "                    #             np.reshape(np.array(surv_prob), [1, -1])[0])\n",
    "                    #va_result[t] = concordance_index(np.reshape(np.array(auc_time), [1, -1])[0], \n",
    "                    #                            np.reshape(np.array(surv_prob), [1, -1])[0], labels)\n",
    "                #    va_result[t] = c_index(np.reshape(np.array(risk_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0],\n",
    "                #            np.reshape(np.array(cindex_label), [1, -1])[0], eval_horizon)\n",
    "                #    br_result[t] = weighted_brier_score(train_o[17].values, train_o[18].values, np.reshape(np.array(risk_prob), [1, -1])[0], \n",
    "                #                     np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(cindex_label), [1, -1])[0], eval_horizon)\n",
    "                \n",
    "                    \n",
    "                    #va_result1[0, t] = brier_score(risk[:,k], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon)\n",
    "                    #va_result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon)\n",
    "                \n",
    "                #c1 = np.mean(va_result)\n",
    "                #print(\"hit_c_index:{0}\".format(va_result))\n",
    "                #print(va_result)\n",
    "                #c2 = np.mean(br_result)\n",
    "                #print(\"hit_br:{0}\".format(br_result))\n",
    "                #print(br_result)\n",
    "                \n",
    "\n",
    "                if auc >  self.max_valid:\n",
    "                    self.stop_flag = 0\n",
    "                    self.max_valid = auc \n",
    "                    name = 'test'\n",
    "                    print( 'updated.... average c-index = ' + str('%.4f' %(auc)))\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_s_t_sr'.format(name), np.array(s_t_sr))\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_s_t_dr'.format(name), np.array(s_t_dr))\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_cindex_label'.format(name), np.reshape(np.array(cindex_label), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_auc_label'.format(name), np.reshape(np.array(auc_label), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_auc_time'.format(name), np.reshape(np.array(auc_time), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_auc_prob'.format(name), np.reshape(np.array(auc_prob), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_death_prob'.format(name), np.reshape(np.array(death_prob), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_m_time'.format(name), np.reshape(np.array(m_time), [1, -1])[0])\n",
    "                    np.save('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\results\\\\ndb_p\\\\10\\\\{}_b_time'.format(name), np.reshape(np.array(b_time), [1, -1])[0])\n",
    "                    #self.save_model()\n",
    "                else:\n",
    "                    self.stop_flag += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "                ############## final time specific evaluation \n",
    "                #EVAL_TIMES = [18,36,60]\n",
    "                #for t, t_time in enumerate(EVAL_TIMES):\n",
    "                #    eval_horizon = int(t_time)\n",
    "                #    risk = np.prod(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n",
    "                #    result1a[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "                #    result2a[k, t] = brier_score(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "                #    result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "                #    result2[k, t] = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "                #FINAL1[:, :, out_itr] = result1\n",
    "                #FINAL2[:, :, out_itr] = result2\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #surv_brscore = brier_score(survival_train, survival_test, estimate, times)\n",
    "                #prob = np.reshape(np.array(auc_prob), [1, -1])[0]\n",
    "                #risk = 1 - prob\n",
    "                #eval_time = [20,40,60]\n",
    "                #va_result1 = np.zeros([1, len(eval_time)]) \n",
    "                #for ind, t_time in enumerate(eval_time):\n",
    "                #    eval_horizon = int(t_time)\n",
    "                #    y_true = ((np.reshape(np.array(auc_time), [1, -1])[0] <= t_time) * np.reshape(np.array(auc_label), [1, -1])[0]).astype(float)\n",
    "                #    va_result1[0, ind] = roc_auc_score(y_true, np.reshape(np.array(auc_prob), [1, -1])[0]) \n",
    "                #mean_C = (va_result1)/3\n",
    "                #c1 = c_index(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 20)\n",
    "                #c2 = c_index(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 40)\n",
    "                #c3 = c_index(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 60)\n",
    "                #mean_C = (c1+c2+c3)/3\n",
    "                \n",
    "                #b1 = brier_score(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 20)\n",
    "                #b2 = brier_score(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 40)\n",
    "                #b3 = brier_score(np.reshape(np.array(auc_prob), [1, -1])[0], np.reshape(np.array(auc_time), [1, -1])[0], np.reshape(np.array(auc_label), [1, -1])[0], 60)\n",
    "                #mean_B = (b1+b2+b3)/3\n",
    "\n",
    "                #eval_time = [20,40,60]\n",
    "                #va_result1 = np.zeros([1, len(eval_time)]) \n",
    "                #for ind, t_time in enumerate(eval_time):\n",
    "                #    eval_horizon = int(t_time)\n",
    "                #    idx_observed = np.reshape(np.array(auc_time), [1, -1])[0] <= eval_horizon\n",
    "                #    va_result1[0, ind] = roc_auc_score(np.reshape(np.array(auc_label), [1, -1])[0][idx_observed],\n",
    "                #                  np.reshape(np.array(auc_prob), [1, -1])[0][idx_observed]) \n",
    "                \n",
    "                \n",
    "                #raiber return\n",
    "                #eval_time = [20,40,60]\n",
    "                #va_result1 = np.zeros([1, len(eval_time)]) \n",
    "                #for ind, t_time in enumerate(eval_time): #raiber: enumerate give us the index and the value , so t will be the index\n",
    "                #     eval_horizon = int(t_time)\n",
    "                #     idx_observed = np.reshape(np.array(auc_time), [1, -1])[0] <= eval_horizon\n",
    "                     #print(\"t: {0}, p: {1}, e:{2}\".format(t[idx_observed].shape,predicted_event_times[idx_observed].shape, e[idx_observed].shape))\n",
    "                #     va_result1[0, ind] = concordance_index(event_times=(np.reshape(np.array(auc_time), [1, -1])[0][idx_observed]).tolist(), predicted_scores=(np.reshape(np.array(auc_prob), [1, -1])[0][idx_observed]).tolist(),\n",
    "                #                            event_observed=(np.reshape(np.array(auc_label), [1, -1])[0][idx_observed]).tolist())\n",
    "                #ci_index = np.mean(va_result1)\n",
    "                # end raiber \n",
    "            except Exception:\n",
    "                print(\"AUC ERROR\")\n",
    "                return\n",
    "            #auc = np.mean(va_result1)\n",
    "            auc_arr.append(auc)\n",
    "            lc_arr.append(pyind)\n",
    "            #surv_cind_arr.append(cind_auc_time_e)#added by raiber\n",
    "            br_arr.append(pybr)#added by raiber\n",
    "            #m_br_arr.append(surv_brscore1)#added by raiber\n",
    "            ############\n",
    "            #br_arr1.append(surv_brscore1)#added by raiber\n",
    "            #m_br_arr1.append(m_surv_brscore1)#added by raiber\n",
    "            ###########\n",
    "        mean_loss = np.array(loss_arr).mean()\n",
    "        mean_auc = np.array(auc_arr).mean()\n",
    "        #mean_c_surv = np.array(surv_cind_arr).mean()\n",
    "        mean_lc = np.array(lc_arr).mean()\n",
    "        mean_br = np.array(br_arr).mean() #added by raiber\n",
    "        #mean_br_m = np.array(m_br_arr).mean() #added by raiber\n",
    "        ###########\n",
    "        #mean_br1 = np.array(br_arr1).mean() #added by raiber\n",
    "        #mean_br_m1 = np.array(m_br_arr1).mean() #added by raiber\n",
    "        ############\n",
    "        mean_anlp = np.array(anlp_arr).mean() \n",
    "        #mean_br, mean_br_m\n",
    "        log = self.getStatStr_test(\"TEST\", self.global_step, mean_auc, mean_lc, mean_br, mean_loss, mean_anlp)\n",
    "        self.force_write(log)\n",
    "        print(log)\n",
    "        return mean_auc, mean_loss, mean_anlp\n",
    "\n",
    "    def force_write(self, log):\n",
    "        if not self.SAVE_LOG:\n",
    "            return\n",
    "        self.train_log_txt = open(self.train_log_txt_filename, 'a')\n",
    "        self.train_log_txt.write(log)\n",
    "        self.train_log_txt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drsa32_102_128_0.000100_0.000100_0.001000_2259_1.20_0.20_True_False_1_1 \n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-6-3525d5fd75f6>:272: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-3525d5fd75f6>:293: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-3525d5fd75f6>:302: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:281: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
      "data size  7579 \n",
      "\n",
      "data size  8000 \n",
      "\n",
      "data size  1898 \n",
      "\n",
      "data size  102 \n",
      "\n",
      "2259\tTRAIN\t100\t1.536856\t0.4046\t7.0954\t3.263332\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t100\t1.277628\t0.0000\t5.8645\t2.706032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.3146\n",
      "2259\tTEST\t100\t1.508562\t0.3146\t0.4929\t0.2936\t6.0886\t3.028032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t200\t1.293536\t0.2918\t5.4620\t2.644732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t200\t1.119436\t0.0000\t5.1892\t2.381232\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.3384\n",
      "2259\tTEST\t200\t1.339422\t0.3384\t0.5589\t0.2886\t5.4152\t2.690432\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t300\t1.148283\t0.3794\t5.0448\t2.386932\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t300\t0.978382\t0.0000\t4.9523\t2.164532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.3873\n",
      "2259\tTEST\t300\t1.166302\t0.3873\t0.6335\t0.2891\t5.1223\t2.424032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t400\t0.979569\t0.4127\t4.8556\t2.146632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t400\t0.847093\t0.0000\t4.7979\t1.976132\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.4582\n",
      "2259\tTEST\t400\t1.020976\t0.4582\t0.6891\t0.2861\t4.9529\t2.215832\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t500\t0.869571\t0.5986\t4.7845\t2.000432\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t500\t0.725251\t0.0000\t4.7886\t1.828032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.7010\n",
      "2259\tTEST\t500\t0.851566\t0.7010\t0.7231\t0.2838\t4.8856\t1.999032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t600\t0.716992\t0.7979\t4.5407\t1.768532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t600\t0.640955\t0.0000\t4.4164\t1.652432\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.7299\n",
      "2259\tTEST\t600\t0.760941\t0.7299\t0.7577\t0.2713\t4.5405\t1.821232\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t700\t0.617577\t0.8842\t4.3028\t1.601632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t700\t0.531808\t0.0000\t4.5037\t1.538932\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.7578\n",
      "2259\tTEST\t700\t0.664966\t0.7578\t0.7817\t0.2634\t4.6464\t1.727232\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t800\t0.529876\t0.9208\t4.1296\t1.461832\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t800\t0.489180\t0.0000\t4.1298\t1.413032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8293\n",
      "2259\tTEST\t800\t0.577116\t0.8293\t0.7993\t0.2631\t4.2452\t1.541632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t900\t0.450647\t0.9233\t3.9348\t1.327732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t900\t0.456136\t0.0000\t3.8881\t1.325032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t900\t0.535629\t0.8135\t0.8119\t0.2608\t4.0168\t1.446132\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1000\t0.390119\t0.9501\t3.7554\t1.219232\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1000\t0.397273\t0.0000\t3.8566\t1.248132\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8542\n",
      "2259\tTEST\t1000\t0.466493\t0.8542\t0.8204\t0.2561\t3.9728\t1.354332\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1100\t0.335010\t0.9707\t3.6862\t1.139332\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1100\t0.325633\t0.0000\t3.9373\t1.178232\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t1100\t0.419503\t0.8340\t0.8284\t0.2479\t4.1042\t1.324332\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1200\t0.284478\t0.9793\t3.5262\t1.046632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1200\t0.304656\t0.0000\t3.9002\t1.145632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8590\n",
      "2259\tTEST\t1200\t0.383392\t0.8590\t0.8328\t0.2433\t4.0554\t1.271132\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1300\t0.244589\t0.9784\t3.3655\t0.966632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1300\t0.275668\t0.0000\t3.8683\t1.104532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8603\n",
      "2259\tTEST\t1300\t0.355649\t0.8603\t0.8397\t0.2393\t4.0360\t1.234032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1400\t0.213275\t0.9806\t3.2250\t0.900932\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1400\t0.232033\t0.0000\t4.0167\t1.081832\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t1400\t0.327571\t0.8558\t0.8429\t0.2344\t4.2247\t1.238032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1500\t0.180364\t0.9910\t3.1326\t0.843032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1500\t0.225332\t0.0000\t3.9961\t1.069632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8648\n",
      "2259\tTEST\t1500\t0.312632\t0.8648\t0.8461\t0.2328\t4.1878\t1.212732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1600\t0.157055\t0.9925\t3.0651\t0.801532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1600\t0.198090\t0.0000\t4.1782\t1.073332\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8652\n",
      "2259\tTEST\t1600\t0.297465\t0.8652\t0.8478\t0.2281\t4.3891\t1.234832\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1700\t0.136314\t0.9944\t3.0068\t0.764932\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1700\t0.260325\t0.0000\t3.8126\t1.074932\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8910\n",
      "2259\tTEST\t1700\t0.310397\t0.8910\t0.8479\t0.2338\t3.9697\t1.166432\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t1800\t0.117390\t0.9969\t2.8651\t0.713932\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1800\t0.252006\t0.0000\t3.8568\t1.073832\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t1800\t0.309227\t0.8822\t0.8508\t0.2310\t4.0483\t1.180732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2259\tTRAIN\t1900\t0.100242\t0.9982\t2.8654\t0.693432\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t1900\t0.220845\t0.0000\t4.0649\t1.078032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.8972\n",
      "2259\tTEST\t1900\t0.282367\t0.8972\t0.8509\t0.2255\t4.2471\t1.188332\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2000\t0.089990\t0.9973\t2.7274\t0.653532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2000\t0.201231\t0.0000\t4.1176\t1.065032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2000\t0.277254\t0.8849\t0.8530\t0.2242\t4.3361\t1.199932\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2100\t0.080137\t0.9984\t2.6223\t0.620632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2100\t0.184788\t0.0000\t4.2990\t1.081532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2100\t0.264392\t0.8916\t0.8541\t0.2221\t4.5475\t1.226832\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2200\t0.071157\t0.9994\t2.6307\t0.611532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2200\t0.231318\t0.0000\t4.1996\t1.117532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "updated.... average c-index = 0.9000\n",
      "2259\tTEST\t2200\t0.289824\t0.9000\t0.8540\t0.2227\t4.4045\t1.228732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2300\t0.066145\t0.9990\t2.5158\t0.582532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2300\t0.218321\t0.0000\t4.3146\t1.124932\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2300\t0.301353\t0.8786\t0.8536\t0.2201\t4.5971\t1.281132\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2400\t0.058917\t0.9998\t2.4101\t0.552732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2400\t0.236184\t0.0000\t4.5579\t1.195032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2400\t0.316579\t0.8869\t0.8542\t0.2147\t4.8326\t1.346432\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2500\t0.054778\t0.9997\t2.3430\t0.534332\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2500\t0.251589\t0.0000\t4.4941\t1.200732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2500\t0.318425\t0.8939\t0.8551\t0.2184\t4.7508\t1.332332\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2600\t0.052792\t0.9990\t2.2731\t0.518032\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2600\t0.236650\t0.0000\t4.5675\t1.197532\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2600\t0.317309\t0.8857\t0.8563\t0.2180\t4.8597\t1.352732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2700\t0.048403\t0.9996\t2.2228\t0.502632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2700\t0.190725\t0.0000\t5.1764\t1.264132\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2700\t0.301852\t0.8860\t0.8540\t0.2144\t5.5050\t1.463232\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTRAIN\t2800\t0.045581\t0.9998\t2.1018\t0.475132\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST_WIN_DATA\t2800\t0.235436\t0.0000\t5.0508\t1.292732\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n",
      "2259\tTEST\t2800\t0.335206\t0.8821\t0.8542\t0.2140\t5.3819\t1.478632\t102\t128\t0.000100\t0.000100\t0.001\t1.2\t0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from BASE_MODEL import BASE_RNN\n",
    "import sys\n",
    "\n",
    "#default parameter\n",
    "FEATURE_SIZE = 26 # dataset input fields countnn\n",
    "\n",
    "MAX_DEN = 912736 # max input data demension 97419, 2047294, 2047341, here is the max the embedding can get \n",
    "EMB_DIM = 32\n",
    "BATCH_SIZE = 102\n",
    "# max time for mortgage data 93\n",
    "MAX_SEQ_LEN = int(146) # Dimension of time horizon of interest (equivalent to the output dimension per survival)\n",
    "TRAING_STEPS = 10000000 # changed by raiber from 10000000\n",
    "\n",
    "STATE_SIZE = 128\n",
    "GRAD_CLIP = 5.0\n",
    "L2_NORM = 0.001\n",
    "ADD_TIME = True\n",
    "ALPHA = 1.2 # coefficient for cross entropy\n",
    "BETA = 0.2 # coefficient for anlp\n",
    "input_file=\"2259\" #toy dataset\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    print (\"Please input learning rate. ex. 0.0001\")\n",
    "    sys.exit(0)\n",
    "\n",
    "LR = float(0.0001)\n",
    "LR_ANLP = LR\n",
    "RUNNING_MODEL = BASE_RNN(EMB_DIM=EMB_DIM,\n",
    "                         FEATURE_SIZE=FEATURE_SIZE,\n",
    "                         BATCH_SIZE=BATCH_SIZE,\n",
    "                         MAX_DEN=MAX_DEN,\n",
    "                         MAX_SEQ_LEN=MAX_SEQ_LEN,\n",
    "                         TRAING_STEPS=TRAING_STEPS,\n",
    "                         STATE_SIZE=STATE_SIZE,\n",
    "                         LR=LR,\n",
    "                         GRAD_CLIP=GRAD_CLIP,\n",
    "                         L2_NORM=L2_NORM,\n",
    "                         INPUT_FILE=input_file,\n",
    "                         ALPHA=ALPHA,\n",
    "                         BETA=BETA,\n",
    "                         ADD_TIME_FEATURE=ADD_TIME,\n",
    "                         FIND_PARAMETER=False,\n",
    "                         ANLP_LR=LR,\n",
    "                         DNN_MODEL=False,\n",
    "                         DISCOUNT=1,\n",
    "                         ONLY_TRAIN_ANLP=False,\n",
    "                         LOG_PREFIX=\"drsa\")\n",
    "RUNNING_MODEL.create_graph()\n",
    "RUNNING_MODEL.run_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
