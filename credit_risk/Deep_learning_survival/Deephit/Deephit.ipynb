{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "from termcolor import colored\n",
    "import time, datetime, os\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from lifelines import KaplanMeierFitter\n",
    "from pycox.evaluation import EvalSurv\n",
    "seed = 1234\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This provide time-dependent Concordance index and Brier Score:\n",
    "    - Use weighted_c_index and weighted_brier_score, which are the unbiased estimates.\n",
    "    \n",
    "See equations and descriptions eq. (11) and (12) of the following paper:\n",
    "    - C. Lee, W. R. Zame, A. Alaa, M. van der Schaar, \"Temporal Quilting for Survival Analysis\", AISTATS 2019\n",
    "'''\n",
    "\n",
    "\n",
    "### C(t)-INDEX CALCULATION\n",
    "def c_index(Prediction, Time_survival, Death, Time):\n",
    "    '''\n",
    "        Raiber: okay, so it does not make a diffrence if the predictions are scores or risks(softmax probabilities) \n",
    "        This is a cause-specific c(t)-index\n",
    "        - Prediction      : risk at Time (higher --> more risky) <class 'numpy.ndarray'> \n",
    "          shape (7997,)\n",
    "        - Time_survival   : survival/censoring time   <class 'numpy.ndarray'>\n",
    "        shape (7997, 1)\n",
    "        - Death           :   <class 'numpy.ndarray'>\n",
    "            > 1: death\n",
    "            > 0: censored (including death from other cause)\n",
    "        shape (7997,)\n",
    "        - Time            : time of evaluation (time-horizon when evaluating C-index)   <class 'int'>\n",
    "        scalar value \n",
    "    '''\n",
    "    N = len(Prediction)# N = 7997 \n",
    "    #A.shape (7997, 7997) for validation \n",
    "    A = np.zeros((N,N))\n",
    "    Q = np.zeros((N,N))\n",
    "    N_t = np.zeros((N,N))\n",
    "    Num = 0\n",
    "    Den = 0\n",
    "    for i in range(N):\n",
    "        # np.where return the index of the values that fullfil the condition \n",
    "        # let assume i = 0, then np.where(Time_survival[i] < Time_survival) will return the indexes \n",
    "        # from 0 until 7996 (because the length is 7996) that have a bigger time then the index 0\n",
    "        # and the assign to A[0, list of biiger times that the value in 0 ] the value 1 \n",
    "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
    "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
    "  \n",
    "        if (Time_survival[i]<=Time and Death[i]==1):\n",
    "            N_t[i,:] = 1\n",
    "\n",
    "    Num  = np.sum(((A)*N_t)*Q)\n",
    "    Den  = np.sum((A)*N_t)\n",
    "\n",
    "    if Num == 0 and Den == 0:\n",
    "        result = -1 # not able to compute c-index!\n",
    "    else:\n",
    "        result = float(Num/Den)\n",
    "\n",
    "    return result\n",
    "\n",
    "### BRIER-SCORE\n",
    "def brier_score(Prediction, Time_survival, Death, Time):\n",
    "    N = len(Prediction)\n",
    "    y_true = ((Time_survival <= Time) * Death).astype(float)\n",
    "\n",
    "    return np.mean((Prediction - y_true)**2)\n",
    "\n",
    "    # result2[k, t] = brier_score_loss(risk[:, k], ((te_time[:,0] <= eval_horizon) * (te_label[:,0] == k+1)).astype(int))\n",
    "\n",
    "\n",
    "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
    "def CensoringProb(Y, T):\n",
    "\n",
    "    T = T.reshape([-1]) # (N,) - np array\n",
    "    Y = Y.reshape([-1]) # (N,) - np array\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
    "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
    "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "### C(t)-INDEX CALCULATION: this account for the weighted average for unbaised estimation\n",
    "def weighted_c_index(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
    "    '''\n",
    "        This is a cause-specific c(t)-index\n",
    "        - Prediction      : risk at Time (higher --> more risky)\n",
    "        - Time_survival   : survival/censoring time\n",
    "        - Death           :\n",
    "            > 1: death\n",
    "            > 0: censored (including death from other cause)\n",
    "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
    "    '''\n",
    "    G = CensoringProb(Y_train, T_train)\n",
    "\n",
    "    N = len(Prediction)\n",
    "    A = np.zeros((N,N))\n",
    "    Q = np.zeros((N,N))\n",
    "    N_t = np.zeros((N,N))\n",
    "    Num = 0\n",
    "    Den = 0\n",
    "    for i in range(N):\n",
    "        tmp_idx = np.where(G[0,:] >= T_test[i])[0]\n",
    "\n",
    "        if len(tmp_idx) == 0:\n",
    "            W = (1./G[1, -1])**2\n",
    "        else:\n",
    "            W = (1./G[1, tmp_idx[0]])**2\n",
    "\n",
    "        A[i, np.where(T_test[i] < T_test)] = 1. * W\n",
    "        Q[i, np.where(Prediction[i] > Prediction)] = 1. # give weights\n",
    "\n",
    "        if (T_test[i]<=Time and Y_test[i]==1):\n",
    "            N_t[i,:] = 1.\n",
    "\n",
    "    Num  = np.sum(((A)*N_t)*Q)\n",
    "    Den  = np.sum((A)*N_t)\n",
    "\n",
    "    if Num == 0 and Den == 0:\n",
    "        result = -1 # not able to compute c-index!\n",
    "    else:\n",
    "        result = float(Num/Den)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# this account for the weighted average for unbaised estimation\n",
    "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
    "    G = CensoringProb(Y_train, T_train)\n",
    "    N = len(Prediction)\n",
    "\n",
    "    W = np.zeros(len(Y_test))\n",
    "    Y_tilde = (T_test > Time).astype(float)\n",
    "\n",
    "    for i in range(N):\n",
    "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
    "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
    "\n",
    "        if len(tmp_idx1) == 0:\n",
    "            G1 = G[1, -1]\n",
    "        else:\n",
    "            G1 = G[1, tmp_idx1[0]]\n",
    "\n",
    "        if len(tmp_idx2) == 0:\n",
    "            G2 = G[1, -1]\n",
    "        else:\n",
    "            G2 = G[1, tmp_idx2[0]]\n",
    "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
    "\n",
    "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
    "\n",
    "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEEDFORWARD NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEEDFORWARD NETWORK\n",
    "def create_FCNet(inputs, num_layers, h_dim, h_fn, o_dim, o_fn, w_init, keep_prob=1.0, w_reg=None):\n",
    "    '''\n",
    "        GOAL             : Create FC network with different specifications \n",
    "        inputs (tensor)  : input tensor\n",
    "        num_layers       : number of layers in FCNet\n",
    "        h_dim  (int)     : number of hidden units\n",
    "        h_fn             : activation function for hidden layers (default: tf.nn.relu)\n",
    "        o_dim  (int)     : number of output units\n",
    "        o_fn             : activation function for output layers (defalut: None)\n",
    "        w_init           : initialization for weight matrix (defalut: Xavier)\n",
    "        keep_prob        : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
    "    '''\n",
    "    # default active functions (hidden: relu, out: None)\n",
    "    if h_fn is None:\n",
    "        h_fn = tf.nn.relu\n",
    "    if o_fn is None:\n",
    "        o_fn = None\n",
    "\n",
    "    # default initialization functions (weight: Xavier, bias: None)\n",
    "    if w_init is None:\n",
    "        w_init = tf.contrib.layers.xavier_initializer() # Xavier initialization\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        if num_layers == 1:\n",
    "            out = FC_Net(inputs, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "        else:\n",
    "            if layer == 0:\n",
    "                h = FC_Net(inputs, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "                if not keep_prob is None: # if keep_prob has a value (is not none)\n",
    "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
    "\n",
    "            elif layer > 0 and layer != (num_layers-1): # layer > 0 and is not the last layer: (i'e: middle hidden layers)\n",
    "                h = FC_Net(h, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "                if not keep_prob is None:\n",
    "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
    "\n",
    "            else: # layer == num_layers-1 (the last layer)\n",
    "                out = FC_Net(h, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepHit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This declare DeepHit architecture:\n",
    "INPUTS:\n",
    "    - input_dims: dictionary of dimension information\n",
    "        > x_dim: dimension of features\n",
    "        > num_Event: number of competing events (this does not include censoring label)\n",
    "        > num_Category: dimension of time horizon of interest, i.e., |T| where T = {0, 1, ..., T_max-1}\n",
    "                      : this is equivalent to the output dimension\n",
    "    - network_settings:\n",
    "        > h_dim_shared & num_layers_shared: number of nodes and number of fully-connected layers for the shared subnetwork\n",
    "        > h_dim_CS & num_layers_CS: number of nodes and number of fully-connected layers for the cause-specific subnetworks\n",
    "        > active_fn: 'relu', 'elu', 'tanh'\n",
    "        > initial_W: Xavier initialization is used as a baseline\n",
    "LOSS FUNCTIONS:\n",
    "    - 1. loglikelihood (this includes log-likelihood of subjects who are censored)\n",
    "    - 2. rankding loss (this is calculated only for acceptable pairs; see the paper for the definition)\n",
    "    - 3. calibration loss (this is to reduce the calibration loss; this is not included in the paper version)\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "_EPSILON = 1e-08\n",
    "\n",
    "\n",
    "\n",
    "##### USER-DEFINED FUNCTIONS\n",
    "def log(x):\n",
    "    return tf.log(x + _EPSILON)\n",
    "\n",
    "def div(x, y):\n",
    "    return tf.div(x, (y + _EPSILON))\n",
    "\n",
    "\n",
    "class Model_DeepHit:\n",
    "    def __init__(self, sess, name, input_dims, network_settings):\n",
    "        self.sess               = sess\n",
    "        self.name               = name\n",
    "\n",
    "        # INPUT DIMENSIONS\n",
    "        self.x_dim              = input_dims['x_dim']\n",
    "\n",
    "        self.num_Event          = input_dims['num_Event']\n",
    "        self.num_Category       = input_dims['num_Category']\n",
    "\n",
    "        # NETWORK HYPER-PARMETERS\n",
    "        self.h_dim_shared       = network_settings['h_dim_shared']\n",
    "        self.h_dim_CS           = network_settings['h_dim_CS']\n",
    "        self.num_layers_shared  = network_settings['num_layers_shared']\n",
    "        self.num_layers_CS      = network_settings['num_layers_CS']\n",
    "\n",
    "        self.active_fn          = network_settings['active_fn']\n",
    "        self.initial_W          = network_settings['initial_W']\n",
    "        self.reg_W              = tf.contrib.layers.l2_regularizer(scale=1e-4)\n",
    "        self.reg_W_out          = tf.contrib.layers.l1_regularizer(scale=1e-4)\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            #### PLACEHOLDER DECLARATION\n",
    "            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "            self.lr_rate     = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            self.keep_prob   = tf.placeholder(tf.float32, [], name='keep_probability')   #keeping rate\n",
    "            self.a           = tf.placeholder(tf.float32, [], name='alpha')\n",
    "            self.b           = tf.placeholder(tf.float32, [], name='beta')\n",
    "            self.c           = tf.placeholder(tf.float32, [], name='gamma')\n",
    "\n",
    "            self.x           = tf.placeholder(tf.float32, shape=[None, self.x_dim], name='inputs')\n",
    "            self.k           = tf.placeholder(tf.float32, shape=[None, 1], name='labels')     #event/censoring label (censoring:0)\n",
    "            self.t           = tf.placeholder(tf.float32, shape=[None, 1], name='timetoevents')\n",
    "\n",
    "            self.fc_mask1    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category], name='mask1')  #for Loss 1\n",
    "            self.fc_mask2    = tf.placeholder(tf.float32, shape=[None, self.num_Category], name='mask2')  #for Loss 2 / Loss 3\n",
    "\n",
    "\n",
    "            ##### SHARED SUBNETWORK w/ FCNETS\n",
    "            # let say we want to creat 2 hidden shared layers, with each of them having dim = 50, input (?,17)\n",
    "            # 1. Layer == 0\n",
    "            # create an hidden layer net with 17 input and 50 nodes and apply relu activation \n",
    "            #function to it, and then add a dropout to it\n",
    "            # 2. layer == 1 \n",
    "            # create another layer with 50 nodes and apply relu activation (no dropout)\n",
    "            # you get the shared_out shape (?,50)\n",
    "\n",
    "            shared_out = create_FCNet(self.x, self.num_layers_shared, self.h_dim_shared, self.active_fn, self.h_dim_shared, self.active_fn, self.initial_W, self.keep_prob, self.reg_W)\n",
    "            #last_x shape (?,17)\n",
    "            last_x = self.x  #for residual connection\n",
    "\n",
    "            h = tf.concat([last_x, shared_out], axis=1) # shape (?,67) 67 = 50 + 17\n",
    "\n",
    "            #(num_layers_CS) layers for cause-specific (num_Event subNets)\n",
    "            out = []\n",
    "            # for each event create a cause-specific network, let assume that we have only one event \n",
    "            # lets assume num_layers_CS = 1 and h_dim_CS = 50\n",
    "            for _ in range(self.num_Event):\n",
    "                cs_out = create_FCNet(h, (self.num_layers_CS), self.h_dim_CS, self.active_fn, self.h_dim_CS, self.active_fn, self.initial_W, self.keep_prob, self.reg_W)\n",
    "                out.append(cs_out)\n",
    "            # out shape (?,50)\n",
    "            out = tf.stack(out, axis=1) # stack referenced on subject\n",
    "            # out shape=(?, 1, 50) after stack\n",
    "            out = tf.reshape(out, [-1, self.num_Event*self.h_dim_CS])\n",
    "            # out shape (?,50) after reshape, because num_Event = 1 in this case \n",
    "            out = tf.nn.dropout(out, keep_prob=self.keep_prob)\n",
    "\n",
    "            out = FC_Net(out, self.num_Event * self.num_Category, activation_fn=tf.nn.softmax, \n",
    "                         weights_initializer=self.initial_W, weights_regularizer=self.reg_W_out, scope=\"Output\")\n",
    "            self.out = tf.reshape(out, [-1, self.num_Event, self.num_Category])\n",
    "\n",
    "\n",
    "            ##### GET LOSS FUNCTIONS\n",
    "            self.loss_Log_Likelihood()      #get loss1: Log-Likelihood loss\n",
    "            self.loss_Ranking()             #get loss2: Ranking loss\n",
    "            self.loss_Calibration()         #get loss3: Calibration loss\n",
    "\n",
    "            self.LOSS_TOTAL = self.a*self.LOSS_1 + self.b*self.LOSS_2 + self.c*self.LOSS_3 + tf.losses.get_regularization_loss()\n",
    "            self.solver = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_TOTAL)\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 1 -- Log-likelihood loss\n",
    "    def loss_Log_Likelihood(self):\n",
    "        I_1 = tf.sign(self.k)\n",
    "\n",
    "        #for uncenosred: log P(T=t,K=k|x)\n",
    "        tmp1 = tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keep_dims=True)\n",
    "        tmp1 = I_1 * log(tmp1)\n",
    "\n",
    "        #for censored: log \\sum P(T>t|x)\n",
    "        tmp2 = tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keep_dims=True)\n",
    "        tmp2 = (1. - I_1) * log(tmp2)\n",
    "\n",
    "        self.LOSS_1 = - tf.reduce_mean(tmp1 + 1.0*tmp2)\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 2 -- Ranking loss\n",
    "    def loss_Ranking(self):\n",
    "        sigma1 = tf.constant(0.1, dtype=tf.float32)\n",
    "\n",
    "        eta = []\n",
    "        for e in range(self.num_Event):\n",
    "            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n",
    "            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n",
    "            I_2 = tf.diag(tf.squeeze(I_2))\n",
    "            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n",
    "\n",
    "            R = tf.matmul(tmp_e, tf.transpose(self.fc_mask2)) #no need to divide by each individual dominator\n",
    "            # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n",
    "\n",
    "            diag_R = tf.reshape(tf.diag_part(R), [-1, 1])\n",
    "            R = tf.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n",
    "            R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n",
    "\n",
    "            T = tf.nn.relu(tf.sign(tf.matmul(one_vector, tf.transpose(self.t)) - tf.matmul(self.t, tf.transpose(one_vector))))\n",
    "            # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
    "\n",
    "            T = tf.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n",
    "\n",
    "            tmp_eta = tf.reduce_mean(T * tf.exp(-R/sigma1), reduction_indices=1, keep_dims=True)\n",
    "\n",
    "            eta.append(tmp_eta)\n",
    "        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
    "        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keep_dims=True)\n",
    "\n",
    "        self.LOSS_2 = tf.reduce_sum(eta) #sum over num_Events\n",
    "\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 3 -- Calibration Loss\n",
    "    def loss_Calibration(self):\n",
    "        eta = []\n",
    "        for e in range(self.num_Event):\n",
    "            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n",
    "            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n",
    "            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n",
    "\n",
    "            r = tf.reduce_sum(tmp_e * self.fc_mask2, axis=0) #no need to divide by each individual dominator\n",
    "            tmp_eta = tf.reduce_mean((r - I_2)**2, reduction_indices=1, keep_dims=True)\n",
    "\n",
    "            eta.append(tmp_eta)\n",
    "        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
    "        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keep_dims=True)\n",
    "\n",
    "        self.LOSS_3 = tf.reduce_sum(eta) #sum over num_Events\n",
    "\n",
    "    \n",
    "    def get_cost(self, DATA, MASK, PARAMETERS, keep_prob, lr_train):\n",
    "        (x_mb, k_mb, t_mb) = DATA\n",
    "        (m1_mb, m2_mb) = MASK\n",
    "        (alpha, beta, gamma) = PARAMETERS\n",
    "        return self.sess.run(self.LOSS_TOTAL, \n",
    "                             feed_dict={self.x:x_mb, self.k:k_mb, self.t:t_mb, self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, \n",
    "                                        self.a:alpha, self.b:beta, self.c:gamma, \n",
    "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
    "\n",
    "    def train(self, DATA, MASK, PARAMETERS, keep_prob, lr_train):\n",
    "        (x_mb, k_mb, t_mb) = DATA\n",
    "        (m1_mb, m2_mb) = MASK\n",
    "        (alpha, beta, gamma) = PARAMETERS\n",
    "        return self.sess.run([self.solver, self.LOSS_TOTAL], \n",
    "                             feed_dict={self.x:x_mb, self.k:k_mb, self.t:t_mb, self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, \n",
    "                                        self.a:alpha, self.b:beta, self.c:gamma, \n",
    "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
    "    \n",
    "    def predict(self, x_test, keep_prob=1.0): # we are making prediction on new data, we only use the features, no time, no event \n",
    "        return self.sess.run(self.out, feed_dict={self.x: x_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    # def predict(self, x_test, MASK, keep_prob=1.0):\n",
    "    #     (m1_test, m2_test) = MASK\n",
    "    #     return self.sess.run(self.out, \n",
    "    #                          feed_dict={self.x: x_test, self.rnn_mask1:m1_test, self.rnn_mask2:m2_test, self.keep_prob: keep_prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This provide the dimension/data/mask to train/test the network.\n",
    "Once must construct a function similar to \"import_dataset_SYNTHETIC\":\n",
    "    - DATA FORMAT:\n",
    "        > data: covariates with x_dim dimension.\n",
    "        > label: 0: censoring, 1 ~ K: K competing(single) risk(s)\n",
    "        > time: time-to-event or time-to-censoring\n",
    "    - Based on the data, creat mask1 and mask2 that are required to calculate loss functions.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "##### DEFINE USER-FUNCTIONS #####\n",
    "def f_get_Normalization(X, norm_mode):\n",
    "    num_Patient, num_Feature = np.shape(X)\n",
    "\n",
    "    if norm_mode == 'standard': #zero mean unit variance\n",
    "        for j in range(num_Feature):\n",
    "            if np.std(X[:,j]) != 0:\n",
    "                X[:,j] = (X[:,j] - np.mean(X[:, j]))/np.std(X[:,j])\n",
    "            else:\n",
    "                X[:,j] = (X[:,j] - np.mean(X[:, j]))\n",
    "    elif norm_mode == 'normal': #min-max normalization\n",
    "        for j in range(num_Feature):\n",
    "            X[:,j] = (X[:,j] - np.min(X[:,j]))/(np.max(X[:,j]) - np.min(X[:,j]))\n",
    "    else:\n",
    "        print(\"INPUT MODE ERROR!\")\n",
    "\n",
    "    return X\n",
    "\n",
    "### MASK FUNCTIONS\n",
    "'''\n",
    "    fc_mask2      : To calculate LOSS_1 (log-likelihood loss)\n",
    "    fc_mask3      : To calculate LOSS_2 (ranking loss)\n",
    "'''\n",
    "def f_get_fc_mask2(time, label, num_Event, num_Category):\n",
    "    '''\n",
    "        mask4 is required to get the log-likelihood loss\n",
    "        mask4 size is [N, num_Event, num_Category]\n",
    "            if not censored : one element = 1 (0 elsewhere)\n",
    "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n",
    "    for i in range(np.shape(time)[0]):\n",
    "        if label[i,0] != 0:  #not censored\n",
    "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
    "        else: #label[i,2]==0: censored\n",
    "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask3(time, meas_time, num_Category):\n",
    "    '''\n",
    "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
    "        mask5 size is [N, num_Category].\n",
    "        - For longitudinal measurements:\n",
    "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
    "             denom is not needed since comparing is done over the same denom\n",
    "        - For single measurement:\n",
    "             1's from start to the event time(inclusive)\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(time)[0], num_Category]) # for the first loss function\n",
    "    if np.shape(meas_time):  #lonogitudinal measurements\n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t1 = int(meas_time[i, 0]) # last measurement time\n",
    "            t2 = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    else:                    #single measurement\n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    return mask\n",
    "\n",
    "def import_dataset_mort_d(norm_mode='standard'):\n",
    "    in_filename = 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\datasets\\\\mortgage\\\\WideFormatMortgageAfterRemovingNull.csv'\n",
    "    df = pd.read_csv(in_filename, sep=',')\n",
    "    \n",
    "    df = df.drop([\"id\", \"first_time\", \"payoff_time\", \"status_time\", \"time\"], axis=1)\n",
    "    \n",
    "    label           = np.asarray(df[['default_time']])\n",
    "    time            = np.asarray(df[['duration']])\n",
    "    data            = np.asarray(df[cols])\n",
    "    data            = f_get_Normalization(data, norm_mode)\n",
    "\n",
    "    num_Category    = int(np.max(time) * 1.2)  #to have enough time-horizon\n",
    "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
    "\n",
    "    x_dim           = np.shape(data)[1]\n",
    "\n",
    "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
    "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
    "\n",
    "    DIM             = (x_dim)\n",
    "    DATA            = (data, time, label)\n",
    "    MASK            = (mask1, mask2)\n",
    "\n",
    "    return DIM, DATA, MASK\n",
    "\n",
    "def import_dataset_mort_p(norm_mode='standard'):\n",
    "    in_filename = 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\datasets\\\\mortgage\\\\WideFormatMortgageAfterRemovingNull.csv'\n",
    "    df = pd.read_csv(in_filename, sep=',')\n",
    "    \n",
    "    df = df.drop([\"id\", \"first_time\", \"default_time\", \"status_time\", \"time\"], axis=1)\n",
    "    \n",
    "    label           = np.asarray(df[['payoff_time']])\n",
    "    time            = np.asarray(df[['duration']])\n",
    "    data            = np.asarray(df[cols])\n",
    "    data            = f_get_Normalization(data, norm_mode)\n",
    "\n",
    "    num_Category    = int(np.max(time) * 1.2)  #to have enough time-horizon\n",
    "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
    "\n",
    "    x_dim           = np.shape(data)[1]\n",
    "\n",
    "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
    "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
    "\n",
    "    DIM             = (x_dim)\n",
    "    DATA            = (data, time, label)\n",
    "    MASK            = (mask1, mask2)\n",
    "\n",
    "    return DIM, DATA, MASK\n",
    "\n",
    "\n",
    "def import_dataset_ndb_d(d_nub, norm_mode='standard'):\n",
    "    data_n = d_nub\n",
    "    in_filename = 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\datasets\\\\data batches\\\\ndb' + data_n + '.csv'\n",
    "    df = pd.read_csv(in_filename, sep=',')\n",
    "    \n",
    "    df = df.drop(['label', 'payoff', 'current_year'], axis=1)\n",
    "    \n",
    "    label           = np.asarray(df[['default']])\n",
    "    time            = np.asarray(df[['time']])\n",
    "    data            = np.asarray(df[cols])\n",
    "    data            = f_get_Normalization(data, norm_mode)\n",
    "\n",
    "    num_Category    = int(np.max(time) * 1.2)  #to have enough time-horizon\n",
    "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
    "\n",
    "    x_dim           = np.shape(data)[1]\n",
    "\n",
    "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
    "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
    "\n",
    "    DIM             = (x_dim)\n",
    "    DATA            = (data, time, label)\n",
    "    MASK            = (mask1, mask2)\n",
    "\n",
    "    return DIM, DATA, MASK\n",
    "\n",
    "\n",
    "def import_dataset_ndb_p(d_nub, norm_mode='standard'):\n",
    "    data_n = d_nub\n",
    "    in_filename = 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\datasets\\\\data batches\\\\ndb' + data_n + '.csv'\n",
    "    df = pd.read_csv(in_filename, sep=',')\n",
    "    \n",
    "    df = df.drop(['label', 'default', 'current_year'], axis=1)\n",
    "    \n",
    "    label           = np.asarray(df[['payoff']])\n",
    "    time            = np.asarray(df[['time']])\n",
    "    data            = np.asarray(df[cols])\n",
    "    data            = f_get_Normalization(data, norm_mode)\n",
    "\n",
    "    num_Category    = int(np.max(time) * 1.2)  #to have enough time-horizon\n",
    "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
    "\n",
    "    x_dim           = np.shape(data)[1]\n",
    "\n",
    "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
    "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
    "\n",
    "    DIM             = (x_dim)\n",
    "    DATA            = (data, time, label)\n",
    "    MASK            = (mask1, mask2)\n",
    "\n",
    "    return DIM, DATA, MASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This train DeepHit, and outputs the validation performance for random search.\n",
    "INPUTS:\n",
    "    - DATA = (data, time, label)\n",
    "    - MASK = (mask1, mask2)\n",
    "    - in_parser: dictionary of hyperparameters\n",
    "    - out_itr: the training/testing split indicator\n",
    "    - eval_time: None or a list (e.g. [12, 24, 36]) at which the validation of the network is performed\n",
    "    - MAX_VALUE: maximum validation value\n",
    "    - OUT_ITERATION: total number of training/testing splits\n",
    "    - seed: random seed for training/testing/validation\n",
    "OUTPUTS:\n",
    "    - the validation performance of the trained network\n",
    "    - save the trained network in the folder directed by \"in_parser['out_path'] + '/itr_' + str(out_itr)\"\n",
    "'''\n",
    "\n",
    "_EPSILON = 1e-08\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "# import sys\n",
    "\n",
    "from termcolor import colored\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### USER-DEFINED FUNCTIONS\n",
    "def log(x):\n",
    "    return tf.log(x + 1e-8)\n",
    "\n",
    "def div(x, y):\n",
    "    return tf.div(x, (y + 1e-8))\n",
    "\n",
    "def f_get_minibatch(mb_size, x, label, time, mask1, mask2):\n",
    "    idx = range(np.shape(x)[0])\n",
    "    idx = random.sample(idx, mb_size)\n",
    "\n",
    "    x_mb = x[idx, :].astype(np.float32)\n",
    "    k_mb = label[idx, :].astype(np.float32) # censoring(0)/event(1,2,..) label\n",
    "    t_mb = time[idx, :].astype(np.float32)\n",
    "    m1_mb = mask1[idx, :, :].astype(np.float32) #fc_mask\n",
    "    m2_mb = mask2[idx, :].astype(np.float32) #fc_mask\n",
    "    return x_mb, k_mb, t_mb, m1_mb, m2_mb\n",
    "\n",
    "\n",
    "def get_valid_performance(DATA, MASK, in_parser, out_itr, eval_time=None, MAX_VALUE = -99, OUT_ITERATION=5, seed=1234):\n",
    "    ##### DATA & MASK\n",
    "    (data, time, label)  = DATA\n",
    "    (mask1, mask2)       = MASK\n",
    "\n",
    "    x_dim                       = np.shape(data)[1]\n",
    "    _, num_Event, num_Category  = np.shape(mask1)  # dim of mask1: [subj, Num_Event, Num_Category]\n",
    "    \n",
    "    ACTIVATION_FN               = {'relu': tf.nn.relu, 'elu': tf.nn.elu, 'tanh': tf.nn.tanh}\n",
    "\n",
    "    ##### HYPER-PARAMETERS\n",
    "    mb_size                     = in_parser['mb_size']\n",
    "\n",
    "    iteration                   = in_parser['iteration']\n",
    "\n",
    "    keep_prob                   = in_parser['keep_prob']\n",
    "    lr_train                    = in_parser['lr_train']\n",
    "\n",
    "\n",
    "    alpha                       = in_parser['alpha']  #for log-likelihood loss\n",
    "    beta                        = in_parser['beta']  #for ranking loss\n",
    "    gamma                       = in_parser['gamma']  #for RNN-prediction loss\n",
    "    parameter_name              = 'a' + str('%02.0f' %(10*alpha)) + 'b' + str('%02.0f' %(10*beta)) + 'c' + str('%02.0f' %(10*gamma))\n",
    "\n",
    "    initial_W                   = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "\n",
    "    ##### MAKE DICTIONARIES\n",
    "    # INPUT DIMENSIONS\n",
    "    input_dims                  = { 'x_dim'         : x_dim,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category}\n",
    "\n",
    "    # NETWORK HYPER-PARMETERS\n",
    "    network_settings            = { 'h_dim_shared'       : in_parser['h_dim_shared'],\n",
    "                                    'num_layers_shared'  : in_parser['num_layers_shared'],\n",
    "                                    'h_dim_CS'           : in_parser['h_dim_CS'],\n",
    "                                    'num_layers_CS'      : in_parser['num_layers_CS'],\n",
    "                                    'active_fn'          : ACTIVATION_FN[in_parser['active_fn']],\n",
    "                                    'initial_W'          : initial_W }\n",
    "\n",
    "\n",
    "    file_path_final = in_parser['out_path'] + '\\\\itr_' + str(out_itr)\n",
    "\n",
    "    #change parameters...\n",
    "    if not os.path.exists(file_path_final + '\\\\models\\\\'):\n",
    "        os.makedirs(file_path_final + '\\\\models\\\\')\n",
    "\n",
    "\n",
    "    print (file_path_final + ' (a:' + str(alpha) + ' b:' + str(beta) + ' c:' + str(gamma) + ')' )\n",
    "\n",
    "    ##### CREATE DEEPFHT NETWORK\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    ### TRAINING-TESTING SPLIT\n",
    "    (tr_data,te_data, tr_time,te_time, tr_label,te_label, \n",
    "     tr_mask1,te_mask1, tr_mask2,te_mask2)  = train_test_split(data, time, label, mask1, mask2, test_size=0.20, random_state=seed) \n",
    "    \n",
    "\n",
    "    (tr_data,va_data, tr_time,va_time, tr_label,va_label, \n",
    "     tr_mask1,va_mask1, tr_mask2,va_mask2)  = train_test_split(tr_data, tr_time, tr_label, tr_mask1, tr_mask2, test_size=0.20, random_state=seed) \n",
    "    \n",
    "    max_valid = -99\n",
    "    stop_flag = 0\n",
    "\n",
    "    if eval_time is None:\n",
    "        eval_time = [int(np.percentile(tr_time, 25)), int(np.percentile(tr_time, 50)), int(np.percentile(tr_time, 75))]\n",
    "\n",
    "\n",
    "    ### TRAINING - MAIN\n",
    "    print( \"MAIN TRAINING ...\")\n",
    "    print( \"EVALUATION TIMES: \" + str(eval_time))\n",
    "\n",
    "    avg_loss = 0\n",
    "    for itr in range(iteration):\n",
    "        if stop_flag > 5: #for faster early stopping\n",
    "            break\n",
    "        else:\n",
    "            x_mb, k_mb, t_mb, m1_mb, m2_mb = f_get_minibatch(mb_size, tr_data, tr_label, tr_time, tr_mask1, tr_mask2)\n",
    "            DATA = (x_mb, k_mb, t_mb)\n",
    "            MASK = (m1_mb, m2_mb)\n",
    "            PARAMETERS = (alpha, beta, gamma)\n",
    "            _, loss_curr = model.train(DATA, MASK, PARAMETERS, keep_prob, lr_train)\n",
    "            avg_loss += loss_curr/1000\n",
    "                \n",
    "            if (itr+1)%1000 == 0:\n",
    "                print('|| ITR: ' + str('%04d' % (itr + 1)) + ' | Loss: ' + colored(str('%.4f' %(avg_loss)), 'yellow' , attrs=['bold']))\n",
    "                avg_loss = 0\n",
    "\n",
    "            ### VALIDATION  (based on average C-index of our interest)\n",
    "            if (itr+1)%1000 == 0:\n",
    "                ### PREDICTION\n",
    "                pred = model.predict(va_data)\n",
    "\n",
    "                ### EVALUATION\n",
    "                va_result1 = np.zeros([num_Event, len(eval_time)])\n",
    "                va_result2 = np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "                for t, t_time in enumerate(eval_time):\n",
    "                    eval_horizon = int(t_time)\n",
    "\n",
    "                    if eval_horizon >= num_Category:\n",
    "                        print('ERROR: evaluation horizon is out of range')\n",
    "                        va_result1[:, t] = va_result2[:, t] = -1\n",
    "                    else:\n",
    "                        risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "                        for k in range(num_Event):\n",
    "                            va_result1[k, t] = c_index(risk[:,k], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "                            #va_result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon)\n",
    "                tmp_valid = np.mean(va_result1)\n",
    "\n",
    "\n",
    "                if tmp_valid >  max_valid:\n",
    "                    stop_flag = 0\n",
    "                    max_valid = tmp_valid\n",
    "                    print( 'updated.... average c-index = ' + str('%.4f' %(tmp_valid)))\n",
    "\n",
    "                    if max_valid > MAX_VALUE:\n",
    "                        saver.save(sess, file_path_final + '\\\\models\\\\model_itr_' + str(out_itr))\n",
    "                else:\n",
    "                    stop_flag += 1\n",
    "\n",
    "    return max_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 0\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\envs\\Deephit\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-344e6d04d0c3>:31: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-c82eb53956ad>:127: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\raibe\\Anaconda3\\envs\\Deephit\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m146.0172\u001b[0m\n",
      "updated.... average c-index = 0.5012\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m121.4007\u001b[0m\n",
      "updated.... average c-index = 0.5260\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m112.9979\u001b[0m\n",
      "updated.... average c-index = 0.5802\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m108.9708\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m104.1632\u001b[0m\n",
      "updated.... average c-index = 0.6084\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m99.2412\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m95.7064\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m93.8178\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m90.7206\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m89.0769\u001b[0m\n",
      "updated.... average c-index = 0.6164\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m87.3344\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m85.5203\u001b[0m\n",
      "updated.... average c-index = 0.6220\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m84.1034\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m83.1595\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m82.3287\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m82.1357\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m79.6267\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m79.6021\u001b[0m\n",
      "Current best: 0.621987130045259\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 1\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m3.5099\u001b[0m\n",
      "updated.... average c-index = 0.5699\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m3.2266\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m3.1171\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m3.0044\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m2.9143\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m2.8667\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m2.8054\u001b[0m\n",
      "Current best: 0.5698881808017757\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 2\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m4.1248\u001b[0m\n",
      "updated.... average c-index = 0.6322\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m3.6869\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m3.4427\u001b[0m\n",
      "updated.... average c-index = 0.6540\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m3.2989\u001b[0m\n",
      "updated.... average c-index = 0.6724\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m3.2231\u001b[0m\n",
      "updated.... average c-index = 0.6879\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m3.1460\u001b[0m\n",
      "updated.... average c-index = 0.7123\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m3.0559\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m2.9916\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m2.9369\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m2.9010\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m2.8571\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m2.8142\u001b[0m\n",
      "Current best: 0.7122799303300167\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 3\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 2, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m101.2506\u001b[0m\n",
      "updated.... average c-index = 0.4701\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m88.0407\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m84.1063\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m76.5854\u001b[0m\n",
      "updated.... average c-index = 0.4777\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m73.4584\u001b[0m\n",
      "updated.... average c-index = 0.4868\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m71.6774\u001b[0m\n",
      "updated.... average c-index = 0.5006\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m69.1878\u001b[0m\n",
      "updated.... average c-index = 0.5206\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m66.9348\u001b[0m\n",
      "updated.... average c-index = 0.5312\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m65.4694\u001b[0m\n",
      "updated.... average c-index = 0.5343\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m64.3128\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m62.2523\u001b[0m\n",
      "updated.... average c-index = 0.5393\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m61.2865\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m58.9356\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m58.4052\u001b[0m\n",
      "updated.... average c-index = 0.5431\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m57.5662\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m55.8601\u001b[0m\n",
      "updated.... average c-index = 0.5474\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m54.6063\u001b[0m\n",
      "updated.... average c-index = 0.5510\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m53.6360\u001b[0m\n",
      "updated.... average c-index = 0.5621\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m52.4921\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m51.8285\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m51.6283\u001b[0m\n",
      "updated.... average c-index = 0.5754\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m50.9086\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m50.6055\u001b[0m\n",
      "updated.... average c-index = 0.5776\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m50.2793\u001b[0m\n",
      "updated.... average c-index = 0.5848\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m49.7868\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m49.2423\u001b[0m\n",
      "updated.... average c-index = 0.5946\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m48.3883\u001b[0m\n",
      "updated.... average c-index = 0.6054\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m48.3700\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m47.9252\u001b[0m\n",
      "updated.... average c-index = 0.6087\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m47.6602\u001b[0m\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m47.6377\u001b[0m\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m47.6025\u001b[0m\n",
      "updated.... average c-index = 0.6121\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m47.0986\u001b[0m\n",
      "updated.... average c-index = 0.6127\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m46.8831\u001b[0m\n",
      "updated.... average c-index = 0.6137\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m46.9827\u001b[0m\n",
      "updated.... average c-index = 0.6153\n",
      "|| ITR: 36000 | Loss: \u001b[1m\u001b[33m46.4510\u001b[0m\n",
      "|| ITR: 37000 | Loss: \u001b[1m\u001b[33m46.3616\u001b[0m\n",
      "|| ITR: 38000 | Loss: \u001b[1m\u001b[33m46.0573\u001b[0m\n",
      "updated.... average c-index = 0.6254\n",
      "|| ITR: 39000 | Loss: \u001b[1m\u001b[33m46.1354\u001b[0m\n",
      "updated.... average c-index = 0.6275\n",
      "|| ITR: 40000 | Loss: \u001b[1m\u001b[33m46.0587\u001b[0m\n",
      "|| ITR: 41000 | Loss: \u001b[1m\u001b[33m46.3242\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| ITR: 42000 | Loss: \u001b[1m\u001b[33m45.3552\u001b[0m\n",
      "|| ITR: 43000 | Loss: \u001b[1m\u001b[33m45.4066\u001b[0m\n",
      "|| ITR: 44000 | Loss: \u001b[1m\u001b[33m46.0259\u001b[0m\n",
      "|| ITR: 45000 | Loss: \u001b[1m\u001b[33m44.8945\u001b[0m\n",
      "Current best: 0.627518627764844\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 4\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m76.6738\u001b[0m\n",
      "updated.... average c-index = 0.4964\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m60.9661\u001b[0m\n",
      "updated.... average c-index = 0.5324\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m55.9529\u001b[0m\n",
      "updated.... average c-index = 0.5700\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m52.8526\u001b[0m\n",
      "updated.... average c-index = 0.5801\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m50.3627\u001b[0m\n",
      "updated.... average c-index = 0.6051\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m49.2941\u001b[0m\n",
      "updated.... average c-index = 0.6178\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m48.1596\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m46.6122\u001b[0m\n",
      "updated.... average c-index = 0.6179\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m46.4431\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m45.9820\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m44.3742\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m44.3598\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m43.2150\u001b[0m\n",
      "updated.... average c-index = 0.6197\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m42.7901\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m42.4970\u001b[0m\n",
      "updated.... average c-index = 0.6199\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m41.8470\u001b[0m\n",
      "updated.... average c-index = 0.6221\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m41.7535\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m41.2776\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m40.4580\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m40.2169\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m39.5449\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m39.4906\u001b[0m\n",
      "updated.... average c-index = 0.6353\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m39.7399\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m39.1040\u001b[0m\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m38.2103\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m38.4569\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m38.4486\u001b[0m\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m38.1568\u001b[0m\n",
      "Current best: 0.6353375836797378\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 5\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m9.7983\u001b[0m\n",
      "updated.... average c-index = 0.5222\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m8.5390\u001b[0m\n",
      "updated.... average c-index = 0.5373\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m8.0652\u001b[0m\n",
      "updated.... average c-index = 0.5636\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m7.8217\u001b[0m\n",
      "updated.... average c-index = 0.5784\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m7.5279\u001b[0m\n",
      "updated.... average c-index = 0.5938\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m7.3524\u001b[0m\n",
      "updated.... average c-index = 0.5993\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m7.1063\u001b[0m\n",
      "updated.... average c-index = 0.6034\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m7.0291\u001b[0m\n",
      "updated.... average c-index = 0.6124\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m6.9189\u001b[0m\n",
      "updated.... average c-index = 0.6175\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m6.7076\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m6.6118\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m6.4952\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m6.4125\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m6.1972\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m6.1533\u001b[0m\n",
      "Current best: 0.6175271772021972\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 6\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 5, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m19.9610\u001b[0m\n",
      "updated.... average c-index = 0.4886\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m17.1620\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m15.8129\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m14.7199\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m14.1288\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m13.6132\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m13.2188\u001b[0m\n",
      "Current best: 0.48861978214323787\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 7\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 200, 'num_layers_shared': 5, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.2796\u001b[0m\n",
      "updated.... average c-index = 0.5296\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.4630\u001b[0m\n",
      "updated.... average c-index = 0.5743\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m5.1991\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m4.9824\u001b[0m\n",
      "updated.... average c-index = 0.5789\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m4.7930\u001b[0m\n",
      "updated.... average c-index = 0.5805\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m4.6190\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m4.5269\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m4.4259\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m4.3170\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m4.1722\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m4.0841\u001b[0m\n",
      "Current best: 0.5805487515766053\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 8\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m50.1837\u001b[0m\n",
      "updated.... average c-index = 0.4944\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m43.3952\u001b[0m\n",
      "updated.... average c-index = 0.5216\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m39.7715\u001b[0m\n",
      "updated.... average c-index = 0.5597\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m37.5783\u001b[0m\n",
      "updated.... average c-index = 0.5826\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m35.9825\u001b[0m\n",
      "updated.... average c-index = 0.6276\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m34.4879\u001b[0m\n",
      "updated.... average c-index = 0.6354\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m33.9321\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m32.5977\u001b[0m\n",
      "updated.... average c-index = 0.6468\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m32.0038\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m31.2847\u001b[0m\n",
      "updated.... average c-index = 0.6585\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m30.6127\u001b[0m\n",
      "updated.... average c-index = 0.6705\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m30.1732\u001b[0m\n",
      "updated.... average c-index = 0.6717\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m29.6925\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m28.8425\u001b[0m\n",
      "updated.... average c-index = 0.6782\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m28.4200\u001b[0m\n",
      "updated.... average c-index = 0.6836\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m28.2465\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m28.3374\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m27.6213\u001b[0m\n",
      "updated.... average c-index = 0.6875\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m27.5289\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m26.8893\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m26.7931\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m26.3619\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m26.1155\u001b[0m\n",
      "updated.... average c-index = 0.6894\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m25.9877\u001b[0m\n",
      "updated.... average c-index = 0.6958\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m26.1975\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m25.7163\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m25.2693\u001b[0m\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m25.2003\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m25.0403\u001b[0m\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m24.5377\u001b[0m\n",
      "Current best: 0.6958283162437778\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 9\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 5, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:1.0 c:0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m20.4415\u001b[0m\n",
      "updated.... average c-index = 0.6652\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m18.4457\u001b[0m\n",
      "updated.... average c-index = 0.7479\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m16.8088\u001b[0m\n",
      "updated.... average c-index = 0.8033\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m15.9764\u001b[0m\n",
      "updated.... average c-index = 0.8291\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m15.3927\u001b[0m\n",
      "updated.... average c-index = 0.8529\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m14.7856\u001b[0m\n",
      "updated.... average c-index = 0.8635\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m14.3604\u001b[0m\n",
      "updated.... average c-index = 0.8734\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m13.9981\u001b[0m\n",
      "updated.... average c-index = 0.8780\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m13.6684\u001b[0m\n",
      "updated.... average c-index = 0.8843\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m13.2254\u001b[0m\n",
      "updated.... average c-index = 0.8878\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m12.8647\u001b[0m\n",
      "updated.... average c-index = 0.8931\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m12.7219\u001b[0m\n",
      "updated.... average c-index = 0.8961\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m12.4399\u001b[0m\n",
      "updated.... average c-index = 0.9003\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m12.3363\u001b[0m\n",
      "updated.... average c-index = 0.9034\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m11.9776\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m11.9426\u001b[0m\n",
      "updated.... average c-index = 0.9050\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m11.5938\u001b[0m\n",
      "updated.... average c-index = 0.9069\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m11.6354\u001b[0m\n",
      "updated.... average c-index = 0.9079\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m11.3824\u001b[0m\n",
      "updated.... average c-index = 0.9096\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m11.1874\u001b[0m\n",
      "updated.... average c-index = 0.9099\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m11.1856\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m11.0713\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m10.8701\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m10.8333\u001b[0m\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m10.7306\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m10.6635\u001b[0m\n",
      "Current best: 0.9098857456992738\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 10\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:1.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m9.1983\u001b[0m\n",
      "updated.... average c-index = 0.6456\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m7.5867\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m7.1164\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m6.8041\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m6.5699\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m6.2811\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m6.2207\u001b[0m\n",
      "Current best: 0.6456179858437993\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 11\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 5, 'num_layers_CS': 5, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m52.7059\u001b[0m\n",
      "updated.... average c-index = 0.6309\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m47.5064\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m43.8915\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m41.1020\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m39.9460\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m37.7197\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m36.5428\u001b[0m\n",
      "Current best: 0.6309445314535291\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 12\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 3, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.0057\u001b[0m\n",
      "updated.... average c-index = 0.4727\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.2859\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.9439\u001b[0m\n",
      "updated.... average c-index = 0.4825\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m4.7100\u001b[0m\n",
      "updated.... average c-index = 0.4852\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m4.5094\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m4.3690\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m4.2838\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m4.1979\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m4.1193\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m4.0491\u001b[0m\n",
      "Current best: 0.485236320166406\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 13\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m22.9594\u001b[0m\n",
      "updated.... average c-index = 0.4898\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m19.1222\u001b[0m\n",
      "updated.... average c-index = 0.5032\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m18.0591\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m17.3771\u001b[0m\n",
      "updated.... average c-index = 0.5107\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m16.8741\u001b[0m\n",
      "updated.... average c-index = 0.5187\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m16.4828\u001b[0m\n",
      "updated.... average c-index = 0.5266\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m15.7386\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m16.1844\u001b[0m\n",
      "updated.... average c-index = 0.5395\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m15.1753\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m15.4069\u001b[0m\n",
      "updated.... average c-index = 0.5421\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m14.8103\u001b[0m\n",
      "updated.... average c-index = 0.5645\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m15.0609\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m14.5828\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m14.2794\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m14.2020\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m14.1222\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m13.8824\u001b[0m\n",
      "Current best: 0.5645184766418295\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 14\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 50, 'num_layers_shared': 5, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m4.2377\u001b[0m\n",
      "updated.... average c-index = 0.6941\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m3.8218\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m3.5818\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m3.4573\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m3.3519\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m3.2533\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m3.1168\u001b[0m\n",
      "Current best: 0.6941429044338551\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 15\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.2557\u001b[0m\n",
      "updated.... average c-index = 0.5160\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m6.1965\u001b[0m\n",
      "updated.... average c-index = 0.5322\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m5.2344\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m5.1229\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m5.0002\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m4.8141\u001b[0m\n",
      "updated.... average c-index = 0.5412\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m4.7437\u001b[0m\n",
      "updated.... average c-index = 0.5432\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m4.6506\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m4.5057\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m4.3806\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m4.3949\u001b[0m\n",
      "updated.... average c-index = 0.5482\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m4.3100\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m4.2513\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m4.2430\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m4.0965\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m4.0386\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m4.0099\u001b[0m\n",
      "Current best: 0.5482231679052761\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 16\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:1.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m10.3830\u001b[0m\n",
      "updated.... average c-index = 0.6611\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m9.0024\u001b[0m\n",
      "updated.... average c-index = 0.7052\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m8.3391\u001b[0m\n",
      "updated.... average c-index = 0.7370\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m7.7203\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m7.2585\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m6.9395\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m6.8114\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m6.6859\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m6.4946\u001b[0m\n",
      "Current best: 0.7369984007099962\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 17\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 2, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m3.1511\u001b[0m\n",
      "updated.... average c-index = 0.5382\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m2.9451\u001b[0m\n",
      "updated.... average c-index = 0.5413\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m2.8724\u001b[0m\n",
      "updated.... average c-index = 0.5512\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m2.7944\u001b[0m\n",
      "updated.... average c-index = 0.5649\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m2.7465\u001b[0m\n",
      "updated.... average c-index = 0.5808\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m2.7101\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m2.6840\u001b[0m\n",
      "updated.... average c-index = 0.5841\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m2.6118\u001b[0m\n",
      "updated.... average c-index = 0.5881\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m2.6225\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m2.5963\u001b[0m\n",
      "updated.... average c-index = 0.5890\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m2.5302\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m2.5078\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m2.4977\u001b[0m\n",
      "updated.... average c-index = 0.5969\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m2.4828\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m2.4625\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m2.4153\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m2.4020\u001b[0m\n",
      "updated.... average c-index = 0.6025\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m2.3666\u001b[0m\n",
      "updated.... average c-index = 0.6223\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m2.3866\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m2.4105\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m2.3371\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m2.3397\u001b[0m\n",
      "updated.... average c-index = 0.6251\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m2.3345\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m2.2954\u001b[0m\n",
      "updated.... average c-index = 0.6361\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m2.2981\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m2.2905\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m2.2693\u001b[0m\n",
      "updated.... average c-index = 0.6473\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m2.2430\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m2.2360\u001b[0m\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m2.2405\u001b[0m\n",
      "updated.... average c-index = 0.6580\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m2.2194\u001b[0m\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m2.2245\u001b[0m\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m2.2154\u001b[0m\n",
      "updated.... average c-index = 0.6666\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m2.1874\u001b[0m\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m2.1847\u001b[0m\n",
      "updated.... average c-index = 0.6677\n",
      "|| ITR: 36000 | Loss: \u001b[1m\u001b[33m2.1732\u001b[0m\n",
      "|| ITR: 37000 | Loss: \u001b[1m\u001b[33m2.1501\u001b[0m\n",
      "|| ITR: 38000 | Loss: \u001b[1m\u001b[33m2.1812\u001b[0m\n",
      "|| ITR: 39000 | Loss: \u001b[1m\u001b[33m2.1504\u001b[0m\n",
      "|| ITR: 40000 | Loss: \u001b[1m\u001b[33m2.1419\u001b[0m\n",
      "|| ITR: 41000 | Loss: \u001b[1m\u001b[33m2.1583\u001b[0m\n",
      "Current best: 0.6677195293960816\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 18\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m70.7338\u001b[0m\n",
      "updated.... average c-index = 0.4804\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m58.0057\u001b[0m\n",
      "updated.... average c-index = 0.4994\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m54.8713\u001b[0m\n",
      "updated.... average c-index = 0.5459\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m51.5764\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m49.6651\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m47.6708\u001b[0m\n",
      "updated.... average c-index = 0.5558\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m46.6986\u001b[0m\n",
      "updated.... average c-index = 0.5817\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m45.7779\u001b[0m\n",
      "updated.... average c-index = 0.6031\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m44.5997\u001b[0m\n",
      "updated.... average c-index = 0.6039\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m43.4989\u001b[0m\n",
      "updated.... average c-index = 0.6040\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m43.0312\u001b[0m\n",
      "updated.... average c-index = 0.6135\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m41.8885\u001b[0m\n",
      "updated.... average c-index = 0.6373\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m41.5569\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m40.0349\u001b[0m\n",
      "updated.... average c-index = 0.6499\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m39.8219\u001b[0m\n",
      "updated.... average c-index = 0.6584\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m39.5486\u001b[0m\n",
      "updated.... average c-index = 0.6608\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m39.1684\u001b[0m\n",
      "updated.... average c-index = 0.6673\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m38.3993\u001b[0m\n",
      "updated.... average c-index = 0.6918\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m37.8857\u001b[0m\n",
      "updated.... average c-index = 0.6945\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m37.6143\u001b[0m\n",
      "updated.... average c-index = 0.6967\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m36.9536\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m36.8494\u001b[0m\n",
      "updated.... average c-index = 0.7066\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m37.0297\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m36.8244\u001b[0m\n",
      "updated.... average c-index = 0.7108\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m36.6892\u001b[0m\n",
      "updated.... average c-index = 0.7247\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m36.1011\u001b[0m\n",
      "updated.... average c-index = 0.7369\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m35.8347\u001b[0m\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m35.4623\u001b[0m\n",
      "updated.... average c-index = 0.7404\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m35.5806\u001b[0m\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m34.8560\u001b[0m\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m35.0559\u001b[0m\n",
      "updated.... average c-index = 0.7497\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m35.1133\u001b[0m\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m34.5485\u001b[0m\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m34.4073\u001b[0m\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m34.4610\u001b[0m\n",
      "|| ITR: 36000 | Loss: \u001b[1m\u001b[33m33.9918\u001b[0m\n",
      "|| ITR: 37000 | Loss: \u001b[1m\u001b[33m33.6325\u001b[0m\n",
      "Current best: 0.7497468848125499\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 19\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 1, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.0974\u001b[0m\n",
      "updated.... average c-index = 0.5289\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.2883\u001b[0m\n",
      "updated.... average c-index = 0.5402\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m5.0126\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m4.9050\u001b[0m\n",
      "updated.... average c-index = 0.5475\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m4.7556\u001b[0m\n",
      "updated.... average c-index = 0.5588\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m4.6631\u001b[0m\n",
      "updated.... average c-index = 0.5655\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m4.6335\u001b[0m\n",
      "updated.... average c-index = 0.5752\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m4.5738\u001b[0m\n",
      "updated.... average c-index = 0.5882\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m4.5676\u001b[0m\n",
      "updated.... average c-index = 0.5888\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m4.4513\u001b[0m\n",
      "updated.... average c-index = 0.5985\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m4.3470\u001b[0m\n",
      "updated.... average c-index = 0.6093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m4.3385\u001b[0m\n",
      "updated.... average c-index = 0.6108\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m4.3104\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m4.3060\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m4.2222\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m4.1920\u001b[0m\n",
      "updated.... average c-index = 0.6146\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m4.1247\u001b[0m\n",
      "updated.... average c-index = 0.6167\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m4.0675\u001b[0m\n",
      "updated.... average c-index = 0.6172\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m4.0945\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m4.0409\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m4.0442\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m3.9980\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m3.9833\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m3.9125\u001b[0m\n",
      "Current best: 0.6172007550469107\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 20\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 200, 'num_layers_shared': 5, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m50.6128\u001b[0m\n",
      "updated.... average c-index = 0.3901\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m43.5781\u001b[0m\n",
      "updated.... average c-index = 0.4683\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m39.5060\u001b[0m\n",
      "updated.... average c-index = 0.5077\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m37.6533\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m36.2309\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m35.0531\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m34.0139\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m33.0070\u001b[0m\n",
      "updated.... average c-index = 0.5109\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m32.4200\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m32.2260\u001b[0m\n",
      "updated.... average c-index = 0.5184\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m31.5607\u001b[0m\n",
      "updated.... average c-index = 0.5206\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m30.5049\u001b[0m\n",
      "updated.... average c-index = 0.5399\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m29.7344\u001b[0m\n",
      "updated.... average c-index = 0.5757\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m29.6461\u001b[0m\n",
      "updated.... average c-index = 0.5808\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m28.5523\u001b[0m\n",
      "updated.... average c-index = 0.5986\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m28.5587\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m28.2614\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m27.4375\u001b[0m\n",
      "updated.... average c-index = 0.6163\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m27.4140\u001b[0m\n",
      "updated.... average c-index = 0.6191\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m26.9290\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m26.9651\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m26.7397\u001b[0m\n",
      "updated.... average c-index = 0.6195\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m26.1625\u001b[0m\n",
      "updated.... average c-index = 0.6231\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m25.9287\u001b[0m\n",
      "updated.... average c-index = 0.6283\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m25.8144\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m25.6802\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m25.1279\u001b[0m\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m25.1416\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m24.6301\u001b[0m\n",
      "updated.... average c-index = 0.6285\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m24.8714\u001b[0m\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m24.7229\u001b[0m\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m24.3538\u001b[0m\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m24.3491\u001b[0m\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m24.0530\u001b[0m\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m23.8561\u001b[0m\n",
      "Current best: 0.628493393271515\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 21\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.4642\u001b[0m\n",
      "updated.... average c-index = 0.5246\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.7660\u001b[0m\n",
      "updated.... average c-index = 0.5313\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m5.5161\u001b[0m\n",
      "updated.... average c-index = 0.5440\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m5.3330\u001b[0m\n",
      "updated.... average c-index = 0.5654\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m5.2076\u001b[0m\n",
      "updated.... average c-index = 0.5696\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m5.0882\u001b[0m\n",
      "updated.... average c-index = 0.5705\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m5.0559\u001b[0m\n",
      "updated.... average c-index = 0.5991\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m4.9293\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m4.8673\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m4.7618\u001b[0m\n",
      "updated.... average c-index = 0.5995\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m4.7681\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m4.7067\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m4.7124\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m4.6089\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m4.5726\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m4.5056\u001b[0m\n",
      "updated.... average c-index = 0.6001\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m4.5124\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m4.4686\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m4.3612\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m4.3206\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m4.2891\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m4.2204\u001b[0m\n",
      "updated.... average c-index = 0.6051\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m4.2669\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m4.2134\u001b[0m\n",
      "updated.... average c-index = 0.6169\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m4.1369\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m4.1510\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m4.1004\u001b[0m\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m4.0825\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m4.0608\u001b[0m\n",
      "updated.... average c-index = 0.6194\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m4.0601\u001b[0m\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m3.9935\u001b[0m\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m4.0138\u001b[0m\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m3.9311\u001b[0m\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m3.9384\u001b[0m\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m3.9291\u001b[0m\n",
      "updated.... average c-index = 0.6218\n",
      "|| ITR: 36000 | Loss: \u001b[1m\u001b[33m3.9146\u001b[0m\n",
      "|| ITR: 37000 | Loss: \u001b[1m\u001b[33m3.9351\u001b[0m\n",
      "|| ITR: 38000 | Loss: \u001b[1m\u001b[33m3.8731\u001b[0m\n",
      "|| ITR: 39000 | Loss: \u001b[1m\u001b[33m3.8183\u001b[0m\n",
      "|| ITR: 40000 | Loss: \u001b[1m\u001b[33m3.8220\u001b[0m\n",
      "updated.... average c-index = 0.6244\n",
      "|| ITR: 41000 | Loss: \u001b[1m\u001b[33m3.7553\u001b[0m\n",
      "|| ITR: 42000 | Loss: \u001b[1m\u001b[33m3.7560\u001b[0m\n",
      "|| ITR: 43000 | Loss: \u001b[1m\u001b[33m3.8175\u001b[0m\n",
      "|| ITR: 44000 | Loss: \u001b[1m\u001b[33m3.8189\u001b[0m\n",
      "|| ITR: 45000 | Loss: \u001b[1m\u001b[33m3.7896\u001b[0m\n",
      "|| ITR: 46000 | Loss: \u001b[1m\u001b[33m3.6961\u001b[0m\n",
      "Current best: 0.6243925457308571\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 22\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 300, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m21.7723\u001b[0m\n",
      "updated.... average c-index = 0.4617\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m18.7698\u001b[0m\n",
      "updated.... average c-index = 0.4732\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m17.5264\u001b[0m\n",
      "updated.... average c-index = 0.5056\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m16.7450\u001b[0m\n",
      "updated.... average c-index = 0.5401\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m16.2673\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m16.0051\u001b[0m\n",
      "updated.... average c-index = 0.5612\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m15.9504\u001b[0m\n",
      "updated.... average c-index = 0.5628\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m15.3557\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m14.9990\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m14.8837\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m14.6965\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m14.5874\u001b[0m\n",
      "updated.... average c-index = 0.5654\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m14.3748\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m14.1520\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m14.2772\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m13.6319\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m13.7944\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m13.5157\u001b[0m\n",
      "Current best: 0.5653686604096895\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 23\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 300, 'num_layers_shared': 1, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.9660\u001b[0m\n",
      "updated.... average c-index = 0.4876\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.3721\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m5.1085\u001b[0m\n",
      "updated.... average c-index = 0.5183\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m5.0449\u001b[0m\n",
      "updated.... average c-index = 0.5477\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m4.8307\u001b[0m\n",
      "updated.... average c-index = 0.5532\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m4.7727\u001b[0m\n",
      "updated.... average c-index = 0.5753\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m4.7041\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m4.6096\u001b[0m\n",
      "updated.... average c-index = 0.5907\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m4.5837\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m4.4596\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m4.4196\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m4.3779\u001b[0m\n",
      "updated.... average c-index = 0.5915\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m4.2505\u001b[0m\n",
      "updated.... average c-index = 0.6048\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m4.2413\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m4.1857\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m4.1792\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m4.1319\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m4.0923\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m4.0603\u001b[0m\n",
      "Current best: 0.6047594708855065\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 24\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m85.5123\u001b[0m\n",
      "updated.... average c-index = 0.4608\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m68.0114\u001b[0m\n",
      "updated.... average c-index = 0.5060\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m63.4731\u001b[0m\n",
      "updated.... average c-index = 0.5085\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m59.5465\u001b[0m\n",
      "updated.... average c-index = 0.5170\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m57.9300\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m55.4169\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m53.9319\u001b[0m\n",
      "updated.... average c-index = 0.5210\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m52.9047\u001b[0m\n",
      "updated.... average c-index = 0.5249\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m51.4813\u001b[0m\n",
      "updated.... average c-index = 0.5331\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m50.6500\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m49.4917\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m48.3558\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m47.5859\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m47.0361\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m46.5772\u001b[0m\n",
      "updated.... average c-index = 0.5332\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m46.0927\u001b[0m\n",
      "updated.... average c-index = 0.5398\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m45.3182\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m45.2573\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m44.8831\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m43.5671\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m44.0441\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m43.1157\u001b[0m\n",
      "Current best: 0.5398311481494381\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 25\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m42.3987\u001b[0m\n",
      "updated.... average c-index = 0.4628\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m37.3828\u001b[0m\n",
      "updated.... average c-index = 0.5107\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m34.5505\u001b[0m\n",
      "updated.... average c-index = 0.5424\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m32.5505\u001b[0m\n",
      "updated.... average c-index = 0.5695\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m31.3265\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m30.4997\u001b[0m\n",
      "updated.... average c-index = 0.5762\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m29.7559\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m29.3448\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m28.2506\u001b[0m\n",
      "updated.... average c-index = 0.5855\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m28.1008\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m27.1424\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m27.2858\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m26.2224\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m26.0358\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m25.2095\u001b[0m\n",
      "updated.... average c-index = 0.5888\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m25.1234\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m24.8237\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m24.5730\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m23.8065\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m23.9186\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m23.4215\u001b[0m\n",
      "Current best: 0.5888264263251055\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 26\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m24.9363\u001b[0m\n",
      "updated.... average c-index = 0.5100\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m22.0561\u001b[0m\n",
      "updated.... average c-index = 0.5676\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m20.8825\u001b[0m\n",
      "updated.... average c-index = 0.5756\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m19.4874\u001b[0m\n",
      "updated.... average c-index = 0.6208\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m18.9051\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m18.4530\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m18.1352\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m17.7222\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m17.0434\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m16.6611\u001b[0m\n",
      "Current best: 0.6207575269452127\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 27\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 50, 'num_layers_shared': 3, 'num_layers_CS': 5, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m45.3227\u001b[0m\n",
      "updated.... average c-index = 0.5818\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m40.8795\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m38.2719\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m36.6406\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m34.6121\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m33.5254\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m32.2962\u001b[0m\n",
      "Current best: 0.581792179135026\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 28\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 300, 'num_layers_shared': 1, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m15.8468\u001b[0m\n",
      "updated.... average c-index = 0.5396\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m13.6244\u001b[0m\n",
      "updated.... average c-index = 0.5722\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m12.7762\u001b[0m\n",
      "updated.... average c-index = 0.6177\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m12.1039\u001b[0m\n",
      "updated.... average c-index = 0.6267\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m11.6538\u001b[0m\n",
      "updated.... average c-index = 0.6483\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m11.2795\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m10.9105\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m10.6434\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m10.5197\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m10.3542\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m10.1350\u001b[0m\n",
      "Current best: 0.6482678605651105\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 29\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m99.9252\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated.... average c-index = 0.4867\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m80.9455\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m73.7523\u001b[0m\n",
      "updated.... average c-index = 0.4955\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m70.4118\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m66.9399\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m64.6638\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m62.2212\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m59.1093\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m58.4632\u001b[0m\n",
      "Current best: 0.4955421059449721\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 30\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:1.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m36.6457\u001b[0m\n",
      "updated.... average c-index = 0.6119\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m30.2221\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m27.7614\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m25.6496\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m24.0383\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m22.6575\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m21.8618\u001b[0m\n",
      "Current best: 0.6118602019060613\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 31\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m25.3352\u001b[0m\n",
      "updated.... average c-index = 0.4332\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m21.3716\u001b[0m\n",
      "updated.... average c-index = 0.4573\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m19.7367\u001b[0m\n",
      "updated.... average c-index = 0.4946\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m18.8162\u001b[0m\n",
      "updated.... average c-index = 0.5127\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m18.0098\u001b[0m\n",
      "updated.... average c-index = 0.5231\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m17.2895\u001b[0m\n",
      "updated.... average c-index = 0.5280\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m17.2403\u001b[0m\n",
      "updated.... average c-index = 0.5392\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m16.7064\u001b[0m\n",
      "updated.... average c-index = 0.5434\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m16.7493\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m16.4888\u001b[0m\n",
      "updated.... average c-index = 0.5488\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m15.9061\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m16.0015\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m16.0207\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m15.2858\u001b[0m\n",
      "updated.... average c-index = 0.5541\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m15.3504\u001b[0m\n",
      "updated.... average c-index = 0.5564\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m15.2254\u001b[0m\n",
      "updated.... average c-index = 0.5605\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m15.3334\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m15.0728\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m14.8620\u001b[0m\n",
      "updated.... average c-index = 0.5676\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m14.6145\u001b[0m\n",
      "updated.... average c-index = 0.5687\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m14.2503\u001b[0m\n",
      "updated.... average c-index = 0.5771\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m14.4147\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m14.1412\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m14.2479\u001b[0m\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m13.9902\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m14.0155\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m13.8749\u001b[0m\n",
      "updated.... average c-index = 0.5818\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m13.4291\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m13.6252\u001b[0m\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m13.4529\u001b[0m\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m13.4055\u001b[0m\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m13.3753\u001b[0m\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m13.1165\u001b[0m\n",
      "Current best: 0.5817789223515949\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 32\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m4.0712\u001b[0m\n",
      "updated.... average c-index = 0.4755\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m3.7169\u001b[0m\n",
      "updated.... average c-index = 0.5353\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m3.5645\u001b[0m\n",
      "updated.... average c-index = 0.5562\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m3.4642\u001b[0m\n",
      "updated.... average c-index = 0.5702\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m3.3766\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m3.3104\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m3.2591\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m3.1697\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m3.1379\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m3.0652\u001b[0m\n",
      "Current best: 0.570217742563794\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 33\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.9671\u001b[0m\n",
      "updated.... average c-index = 0.5195\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.0797\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.7360\u001b[0m\n",
      "updated.... average c-index = 0.5421\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m4.4290\u001b[0m\n",
      "updated.... average c-index = 0.5586\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m4.2563\u001b[0m\n",
      "updated.... average c-index = 0.5997\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m4.1254\u001b[0m\n",
      "updated.... average c-index = 0.6153\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m3.9663\u001b[0m\n",
      "updated.... average c-index = 0.6213\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m3.8723\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m3.8033\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m3.7265\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m3.6654\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m3.6194\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m3.6021\u001b[0m\n",
      "Current best: 0.6213347352951412\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 34\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.2166\u001b[0m\n",
      "updated.... average c-index = 0.5643\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m4.3667\u001b[0m\n",
      "updated.... average c-index = 0.5688\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.0574\u001b[0m\n",
      "updated.... average c-index = 0.5843\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m3.8876\u001b[0m\n",
      "updated.... average c-index = 0.5850\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m3.7485\u001b[0m\n",
      "updated.... average c-index = 0.5944\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m3.6112\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m3.5067\u001b[0m\n",
      "updated.... average c-index = 0.5945\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m3.4374\u001b[0m\n",
      "updated.... average c-index = 0.5969\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m3.3487\u001b[0m\n",
      "updated.... average c-index = 0.6018\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m3.2946\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m3.2455\u001b[0m\n",
      "updated.... average c-index = 0.6035\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m3.2101\u001b[0m\n",
      "updated.... average c-index = 0.6133\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m3.1694\u001b[0m\n",
      "updated.... average c-index = 0.6157\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m3.1083\u001b[0m\n",
      "updated.... average c-index = 0.6255\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m3.0758\u001b[0m\n",
      "updated.... average c-index = 0.6379\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m3.0457\u001b[0m\n",
      "updated.... average c-index = 0.6383\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m3.0290\u001b[0m\n",
      "updated.... average c-index = 0.6485\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m2.9675\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m2.9708\u001b[0m\n",
      "updated.... average c-index = 0.6493\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m2.9214\u001b[0m\n",
      "updated.... average c-index = 0.6547\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m2.9158\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m2.8656\u001b[0m\n",
      "updated.... average c-index = 0.6645\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m2.8621\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated.... average c-index = 0.6718\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m2.8458\u001b[0m\n",
      "updated.... average c-index = 0.6790\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m2.8350\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m2.8106\u001b[0m\n",
      "updated.... average c-index = 0.6817\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m2.7854\u001b[0m\n",
      "updated.... average c-index = 0.6897\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m2.7868\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m2.7580\u001b[0m\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m2.7229\u001b[0m\n",
      "updated.... average c-index = 0.6932\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m2.6961\u001b[0m\n",
      "updated.... average c-index = 0.6945\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m2.6762\u001b[0m\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m2.6891\u001b[0m\n",
      "updated.... average c-index = 0.7082\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m2.6820\u001b[0m\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m2.6541\u001b[0m\n",
      "|| ITR: 36000 | Loss: \u001b[1m\u001b[33m2.6409\u001b[0m\n",
      "|| ITR: 37000 | Loss: \u001b[1m\u001b[33m2.6410\u001b[0m\n",
      "updated.... average c-index = 0.7134\n",
      "|| ITR: 38000 | Loss: \u001b[1m\u001b[33m2.6226\u001b[0m\n",
      "updated.... average c-index = 0.7219\n",
      "|| ITR: 39000 | Loss: \u001b[1m\u001b[33m2.6311\u001b[0m\n",
      "|| ITR: 40000 | Loss: \u001b[1m\u001b[33m2.5879\u001b[0m\n",
      "|| ITR: 41000 | Loss: \u001b[1m\u001b[33m2.5805\u001b[0m\n",
      "|| ITR: 42000 | Loss: \u001b[1m\u001b[33m2.5762\u001b[0m\n",
      "|| ITR: 43000 | Loss: \u001b[1m\u001b[33m2.5874\u001b[0m\n",
      "updated.... average c-index = 0.7304\n",
      "|| ITR: 44000 | Loss: \u001b[1m\u001b[33m2.5760\u001b[0m\n",
      "updated.... average c-index = 0.7305\n",
      "|| ITR: 45000 | Loss: \u001b[1m\u001b[33m2.5723\u001b[0m\n",
      "|| ITR: 46000 | Loss: \u001b[1m\u001b[33m2.5577\u001b[0m\n",
      "updated.... average c-index = 0.7350\n",
      "|| ITR: 47000 | Loss: \u001b[1m\u001b[33m2.5400\u001b[0m\n",
      "|| ITR: 48000 | Loss: \u001b[1m\u001b[33m2.5285\u001b[0m\n",
      "updated.... average c-index = 0.7373\n",
      "|| ITR: 49000 | Loss: \u001b[1m\u001b[33m2.5212\u001b[0m\n",
      "|| ITR: 50000 | Loss: \u001b[1m\u001b[33m2.5175\u001b[0m\n",
      "Current best: 0.7372780701724521\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 35\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 300, 'num_layers_shared': 5, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m73.1396\u001b[0m\n",
      "updated.... average c-index = 0.4643\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m60.5914\u001b[0m\n",
      "updated.... average c-index = 0.4872\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m57.1217\u001b[0m\n",
      "updated.... average c-index = 0.5111\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m54.2429\u001b[0m\n",
      "updated.... average c-index = 0.5141\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m52.0256\u001b[0m\n",
      "updated.... average c-index = 0.5227\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m51.2589\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m49.1771\u001b[0m\n",
      "updated.... average c-index = 0.5237\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m48.2311\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m46.9116\u001b[0m\n",
      "updated.... average c-index = 0.5401\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m45.7003\u001b[0m\n",
      "updated.... average c-index = 0.5408\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m44.6961\u001b[0m\n",
      "updated.... average c-index = 0.5437\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m43.5789\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m42.6976\u001b[0m\n",
      "updated.... average c-index = 0.5589\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m42.7715\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m42.5162\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m41.8022\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m41.1006\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m40.6844\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m40.3109\u001b[0m\n",
      "Current best: 0.558891466388224\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 36\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m19.5917\u001b[0m\n",
      "updated.... average c-index = 0.4243\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m16.7950\u001b[0m\n",
      "updated.... average c-index = 0.4483\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m15.4604\u001b[0m\n",
      "updated.... average c-index = 0.5316\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m14.5198\u001b[0m\n",
      "updated.... average c-index = 0.5405\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m13.6682\u001b[0m\n",
      "updated.... average c-index = 0.5720\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m13.1976\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m12.5293\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m12.1102\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m11.7521\u001b[0m\n",
      "updated.... average c-index = 0.5932\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m11.3702\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m11.1367\u001b[0m\n",
      "updated.... average c-index = 0.6122\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m10.9843\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m10.8015\u001b[0m\n",
      "updated.... average c-index = 0.6129\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m10.5582\u001b[0m\n",
      "updated.... average c-index = 0.6328\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m10.4182\u001b[0m\n",
      "updated.... average c-index = 0.6460\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m10.2791\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m10.1801\u001b[0m\n",
      "updated.... average c-index = 0.6581\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m9.9355\u001b[0m\n",
      "updated.... average c-index = 0.6807\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m10.0375\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m9.8675\u001b[0m\n",
      "updated.... average c-index = 0.6907\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m9.8646\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m9.7583\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m9.6621\u001b[0m\n",
      "updated.... average c-index = 0.6911\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m9.6457\u001b[0m\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m9.5894\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m9.5119\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m9.4756\u001b[0m\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m9.4064\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m9.3065\u001b[0m\n",
      "Current best: 0.691078106311116\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 37\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 1, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:1.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m31.7774\u001b[0m\n",
      "updated.... average c-index = 0.4933\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m27.2495\u001b[0m\n",
      "updated.... average c-index = 0.5110\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m24.7251\u001b[0m\n",
      "updated.... average c-index = 0.5245\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m23.7957\u001b[0m\n",
      "updated.... average c-index = 0.5412\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m22.8535\u001b[0m\n",
      "updated.... average c-index = 0.5505\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m22.0234\u001b[0m\n",
      "updated.... average c-index = 0.5649\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m21.6650\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m20.8044\u001b[0m\n",
      "updated.... average c-index = 0.5684\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m20.4152\u001b[0m\n",
      "updated.... average c-index = 0.5817\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m20.2142\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m19.6695\u001b[0m\n",
      "updated.... average c-index = 0.5903\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m19.2478\u001b[0m\n",
      "updated.... average c-index = 0.5974\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m18.8875\u001b[0m\n",
      "updated.... average c-index = 0.6112\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m18.4445\u001b[0m\n",
      "updated.... average c-index = 0.6172\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m18.2719\u001b[0m\n",
      "updated.... average c-index = 0.6247\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m18.2356\u001b[0m\n",
      "updated.... average c-index = 0.6264\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m17.8171\u001b[0m\n",
      "updated.... average c-index = 0.6367\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m17.6327\u001b[0m\n",
      "updated.... average c-index = 0.6522\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m17.3629\u001b[0m\n",
      "updated.... average c-index = 0.6667\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m17.0352\u001b[0m\n",
      "updated.... average c-index = 0.6767\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m16.9399\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m16.7033\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m16.8749\u001b[0m\n",
      "updated.... average c-index = 0.6783\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m16.7013\u001b[0m\n",
      "updated.... average c-index = 0.6895\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m16.2858\u001b[0m\n",
      "updated.... average c-index = 0.6901\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m16.2414\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m16.1057\u001b[0m\n",
      "updated.... average c-index = 0.6962\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m16.1595\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m15.8799\u001b[0m\n",
      "updated.... average c-index = 0.7000\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m15.6787\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated.... average c-index = 0.7031\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m15.7492\u001b[0m\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m15.6340\u001b[0m\n",
      "updated.... average c-index = 0.7060\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m15.6130\u001b[0m\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m15.3096\u001b[0m\n",
      "updated.... average c-index = 0.7064\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m15.3138\u001b[0m\n",
      "updated.... average c-index = 0.7177\n",
      "|| ITR: 36000 | Loss: \u001b[1m\u001b[33m15.2302\u001b[0m\n",
      "|| ITR: 37000 | Loss: \u001b[1m\u001b[33m14.9880\u001b[0m\n",
      "|| ITR: 38000 | Loss: \u001b[1m\u001b[33m14.9121\u001b[0m\n",
      "|| ITR: 39000 | Loss: \u001b[1m\u001b[33m15.1062\u001b[0m\n",
      "|| ITR: 40000 | Loss: \u001b[1m\u001b[33m14.8104\u001b[0m\n",
      "|| ITR: 41000 | Loss: \u001b[1m\u001b[33m14.6785\u001b[0m\n",
      "updated.... average c-index = 0.7245\n",
      "|| ITR: 42000 | Loss: \u001b[1m\u001b[33m14.7663\u001b[0m\n",
      "|| ITR: 43000 | Loss: \u001b[1m\u001b[33m14.5707\u001b[0m\n",
      "|| ITR: 44000 | Loss: \u001b[1m\u001b[33m14.7212\u001b[0m\n",
      "|| ITR: 45000 | Loss: \u001b[1m\u001b[33m14.4029\u001b[0m\n",
      "|| ITR: 46000 | Loss: \u001b[1m\u001b[33m14.4108\u001b[0m\n",
      "|| ITR: 47000 | Loss: \u001b[1m\u001b[33m14.3907\u001b[0m\n",
      "Current best: 0.72449230351846\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 38\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 50, 'num_layers_shared': 5, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.1771\u001b[0m\n",
      "updated.... average c-index = 0.4684\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m4.6557\u001b[0m\n",
      "updated.... average c-index = 0.4846\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.4569\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m4.2475\u001b[0m\n",
      "updated.... average c-index = 0.4878\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m4.0335\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m3.8581\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m3.7287\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m3.6476\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m3.5682\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m3.5397\u001b[0m\n",
      "Current best: 0.4878334393947002\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 39\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 100, 'num_layers_shared': 5, 'num_layers_CS': 2, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m85.6954\u001b[0m\n",
      "updated.... average c-index = 0.5315\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m73.2250\u001b[0m\n",
      "updated.... average c-index = 0.5849\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m66.2558\u001b[0m\n",
      "updated.... average c-index = 0.6346\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m62.0003\u001b[0m\n",
      "updated.... average c-index = 0.6583\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m57.9301\u001b[0m\n",
      "updated.... average c-index = 0.6666\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m55.8637\u001b[0m\n",
      "updated.... average c-index = 0.6815\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m53.7508\u001b[0m\n",
      "updated.... average c-index = 0.7178\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m50.4515\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m48.8140\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m47.1459\u001b[0m\n",
      "updated.... average c-index = 0.7438\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m45.8553\u001b[0m\n",
      "updated.... average c-index = 0.7440\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m44.7387\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m44.2650\u001b[0m\n",
      "updated.... average c-index = 0.7523\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m43.6685\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m42.7276\u001b[0m\n",
      "updated.... average c-index = 0.7526\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m42.2536\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m41.7879\u001b[0m\n",
      "updated.... average c-index = 0.7594\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m41.7906\u001b[0m\n",
      "updated.... average c-index = 0.7646\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m41.0910\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m40.8698\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m40.6324\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m39.9432\u001b[0m\n",
      "updated.... average c-index = 0.7774\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m39.3470\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m39.1012\u001b[0m\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m39.3704\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m39.1584\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m38.4915\u001b[0m\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m37.9709\u001b[0m\n",
      "Current best: 0.7774075076726769\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 40\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 300, 'num_layers_shared': 5, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m16.0407\u001b[0m\n",
      "updated.... average c-index = 0.6461\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m12.6857\u001b[0m\n",
      "updated.... average c-index = 0.6906\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m11.4620\u001b[0m\n",
      "updated.... average c-index = 0.7156\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m10.6716\u001b[0m\n",
      "updated.... average c-index = 0.7242\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m10.1092\u001b[0m\n",
      "updated.... average c-index = 0.7247\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m9.6747\u001b[0m\n",
      "updated.... average c-index = 0.7272\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m9.4265\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m9.1173\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m8.9776\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m8.6844\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m8.5387\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m8.3905\u001b[0m\n",
      "Current best: 0.7271619877798025\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 41\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:1.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m16.6208\u001b[0m\n",
      "updated.... average c-index = 0.5363\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m13.7641\u001b[0m\n",
      "updated.... average c-index = 0.5678\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m12.4982\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m11.7764\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m11.2459\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m10.8191\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m10.5036\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m9.9793\u001b[0m\n",
      "Current best: 0.5678372801509539\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 42\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m3.7106\u001b[0m\n",
      "updated.... average c-index = 0.7260\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m3.2012\u001b[0m\n",
      "updated.... average c-index = 0.7455\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m3.0120\u001b[0m\n",
      "updated.... average c-index = 0.7637\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m2.9013\u001b[0m\n",
      "updated.... average c-index = 0.7738\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m2.7969\u001b[0m\n",
      "updated.... average c-index = 0.7887\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m2.7386\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m2.6769\u001b[0m\n",
      "updated.... average c-index = 0.8022\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m2.6551\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m2.6093\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m2.5706\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m2.5498\u001b[0m\n",
      "updated.... average c-index = 0.8030\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m2.5093\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m2.5000\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m2.4657\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m2.4447\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m2.4493\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m2.4218\u001b[0m\n",
      "Current best: 0.8030342519740904\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 43\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:1.0 c:0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m18.4511\u001b[0m\n",
      "updated.... average c-index = 0.5138\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m15.8374\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m14.8542\u001b[0m\n",
      "updated.... average c-index = 0.5187\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m13.8360\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m13.9288\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m13.1362\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m12.9533\u001b[0m\n",
      "updated.... average c-index = 0.5201\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m12.6625\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m12.4671\u001b[0m\n",
      "updated.... average c-index = 0.5220\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m12.1164\u001b[0m\n",
      "updated.... average c-index = 0.5255\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m12.0450\u001b[0m\n",
      "updated.... average c-index = 0.5260\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m11.6880\u001b[0m\n",
      "updated.... average c-index = 0.5333\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m11.4993\u001b[0m\n",
      "updated.... average c-index = 0.5452\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m11.3171\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m11.0782\u001b[0m\n",
      "updated.... average c-index = 0.5515\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m10.9864\u001b[0m\n",
      "updated.... average c-index = 0.5579\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m10.8993\u001b[0m\n",
      "updated.... average c-index = 0.5751\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m10.7232\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m10.5298\u001b[0m\n",
      "updated.... average c-index = 0.5757\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m10.4386\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m10.4705\u001b[0m\n",
      "updated.... average c-index = 0.5784\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m10.3325\u001b[0m\n",
      "updated.... average c-index = 0.5827\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m10.3086\u001b[0m\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m10.1694\u001b[0m\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m10.1823\u001b[0m\n",
      "updated.... average c-index = 0.5935\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m10.0711\u001b[0m\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m9.9235\u001b[0m\n",
      "updated.... average c-index = 0.5965\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m9.9311\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m9.8978\u001b[0m\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m9.6165\u001b[0m\n",
      "updated.... average c-index = 0.6086\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m9.6908\u001b[0m\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m9.8329\u001b[0m\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m9.7743\u001b[0m\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m9.5968\u001b[0m\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m9.6104\u001b[0m\n",
      "|| ITR: 36000 | Loss: \u001b[1m\u001b[33m9.4923\u001b[0m\n",
      "Current best: 0.6086109580826083\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 44\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.1 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m3.3918\u001b[0m\n",
      "updated.... average c-index = 0.7030\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m3.1105\u001b[0m\n",
      "updated.... average c-index = 0.7235\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m3.1092\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m2.9303\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m2.9219\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m2.8641\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m2.8024\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m2.7870\u001b[0m\n",
      "Current best: 0.723495196497261\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 45\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m9.3289\u001b[0m\n",
      "updated.... average c-index = 0.4782\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m7.9429\u001b[0m\n",
      "updated.... average c-index = 0.4878\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m7.6213\u001b[0m\n",
      "updated.... average c-index = 0.4885\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m7.2318\u001b[0m\n",
      "updated.... average c-index = 0.5021\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m7.0743\u001b[0m\n",
      "updated.... average c-index = 0.5030\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m6.8351\u001b[0m\n",
      "updated.... average c-index = 0.5106\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m6.7061\u001b[0m\n",
      "updated.... average c-index = 0.5182\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m6.5918\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m6.4861\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m6.3338\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m6.3752\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m6.3040\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m6.2080\u001b[0m\n",
      "updated.... average c-index = 0.5303\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m6.0557\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m6.0414\u001b[0m\n",
      "updated.... average c-index = 0.5326\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m5.9290\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m5.9048\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m5.8719\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m5.7867\u001b[0m\n",
      "updated.... average c-index = 0.5350\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m5.8114\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m5.7473\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m5.7338\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m5.6964\u001b[0m\n",
      "updated.... average c-index = 0.5454\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m5.6400\u001b[0m\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m5.6489\u001b[0m\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m5.5973\u001b[0m\n",
      "updated.... average c-index = 0.5507\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m5.5702\u001b[0m\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m5.5500\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m5.5401\u001b[0m\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m5.5541\u001b[0m\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m5.4414\u001b[0m\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m5.4199\u001b[0m\n",
      "Current best: 0.5506853879783318\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 46\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 3, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m144.0601\u001b[0m\n",
      "updated.... average c-index = 0.4056\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m117.4478\u001b[0m\n",
      "updated.... average c-index = 0.4326\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m107.7781\u001b[0m\n",
      "updated.... average c-index = 0.4343\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m101.9195\u001b[0m\n",
      "updated.... average c-index = 0.4472\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m96.5130\u001b[0m\n",
      "updated.... average c-index = 0.4715\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m93.2500\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m90.2955\u001b[0m\n",
      "updated.... average c-index = 0.4913\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m87.3686\u001b[0m\n",
      "updated.... average c-index = 0.4930\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m85.7843\u001b[0m\n",
      "updated.... average c-index = 0.5101\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m83.5699\u001b[0m\n",
      "updated.... average c-index = 0.5160\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m81.2527\u001b[0m\n",
      "updated.... average c-index = 0.5215\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m79.1983\u001b[0m\n",
      "updated.... average c-index = 0.5388\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m77.8399\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m77.0178\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m75.7596\u001b[0m\n",
      "updated.... average c-index = 0.5410\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m75.0827\u001b[0m\n",
      "updated.... average c-index = 0.5489\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m74.6900\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m73.3923\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m72.4946\u001b[0m\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m71.5169\u001b[0m\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m71.3358\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m71.0622\u001b[0m\n",
      "Current best: 0.5488986954668509\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 47\n",
      "{'mb_size': 32, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m44.5679\u001b[0m\n",
      "updated.... average c-index = 0.4759\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m39.4961\u001b[0m\n",
      "updated.... average c-index = 0.5197\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m35.9051\u001b[0m\n",
      "updated.... average c-index = 0.5916\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m34.3119\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m33.1377\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m31.6618\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m30.5895\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m29.6947\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m29.2053\u001b[0m\n",
      "Current best: 0.5915908041415753\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 48\n",
      "{'mb_size': 64, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:1.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m19.1145\u001b[0m\n",
      "updated.... average c-index = 0.4737\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m16.8957\u001b[0m\n",
      "updated.... average c-index = 0.4954\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m15.6919\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m14.7190\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m14.0978\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m13.5104\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m13.2277\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m12.5835\u001b[0m\n",
      "Current best: 0.49542658612263485\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 49\n",
      "{'mb_size': 128, 'iteration': 50000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\mort_p\\\\1'}\n",
      "C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [20, 40, 60]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m15.9236\u001b[0m\n",
      "updated.... average c-index = 0.5255\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m12.9128\u001b[0m\n",
      "updated.... average c-index = 0.5372\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m12.0690\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m11.2823\u001b[0m\n",
      "updated.... average c-index = 0.5613\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m10.7190\u001b[0m\n",
      "updated.... average c-index = 0.5736\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m10.2441\u001b[0m\n",
      "updated.... average c-index = 0.5923\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m9.9308\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m9.7372\u001b[0m\n",
      "updated.... average c-index = 0.6061\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m9.4277\u001b[0m\n",
      "updated.... average c-index = 0.6167\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m9.1303\u001b[0m\n",
      "updated.... average c-index = 0.6204\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m8.9776\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m8.7652\u001b[0m\n",
      "updated.... average c-index = 0.6274\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m8.5756\u001b[0m\n",
      "updated.... average c-index = 0.6322\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m8.5810\u001b[0m\n",
      "updated.... average c-index = 0.6414\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m8.4348\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m8.2879\u001b[0m\n",
      "updated.... average c-index = 0.6503\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m8.2266\u001b[0m\n",
      "updated.... average c-index = 0.6538\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m8.0452\u001b[0m\n",
      "|| ITR: 19000 | Loss: \u001b[1m\u001b[33m7.9315\u001b[0m\n",
      "updated.... average c-index = 0.6606\n",
      "|| ITR: 20000 | Loss: \u001b[1m\u001b[33m7.8380\u001b[0m\n",
      "updated.... average c-index = 0.6632\n",
      "|| ITR: 21000 | Loss: \u001b[1m\u001b[33m7.7954\u001b[0m\n",
      "|| ITR: 22000 | Loss: \u001b[1m\u001b[33m7.7506\u001b[0m\n",
      "|| ITR: 23000 | Loss: \u001b[1m\u001b[33m7.6757\u001b[0m\n",
      "updated.... average c-index = 0.6635\n",
      "|| ITR: 24000 | Loss: \u001b[1m\u001b[33m7.5275\u001b[0m\n",
      "updated.... average c-index = 0.6643\n",
      "|| ITR: 25000 | Loss: \u001b[1m\u001b[33m7.5212\u001b[0m\n",
      "updated.... average c-index = 0.6662\n",
      "|| ITR: 26000 | Loss: \u001b[1m\u001b[33m7.4824\u001b[0m\n",
      "updated.... average c-index = 0.6674\n",
      "|| ITR: 27000 | Loss: \u001b[1m\u001b[33m7.3714\u001b[0m\n",
      "updated.... average c-index = 0.6683\n",
      "|| ITR: 28000 | Loss: \u001b[1m\u001b[33m7.3695\u001b[0m\n",
      "|| ITR: 29000 | Loss: \u001b[1m\u001b[33m7.3081\u001b[0m\n",
      "updated.... average c-index = 0.6738\n",
      "|| ITR: 30000 | Loss: \u001b[1m\u001b[33m7.2266\u001b[0m\n",
      "updated.... average c-index = 0.6740\n",
      "|| ITR: 31000 | Loss: \u001b[1m\u001b[33m7.1951\u001b[0m\n",
      "updated.... average c-index = 0.6741\n",
      "|| ITR: 32000 | Loss: \u001b[1m\u001b[33m7.1234\u001b[0m\n",
      "|| ITR: 33000 | Loss: \u001b[1m\u001b[33m7.1060\u001b[0m\n",
      "|| ITR: 34000 | Loss: \u001b[1m\u001b[33m7.0057\u001b[0m\n",
      "updated.... average c-index = 0.6820\n",
      "|| ITR: 35000 | Loss: \u001b[1m\u001b[33m6.9639\u001b[0m\n",
      "|| ITR: 36000 | Loss: \u001b[1m\u001b[33m6.9733\u001b[0m\n",
      "|| ITR: 37000 | Loss: \u001b[1m\u001b[33m6.8193\u001b[0m\n",
      "|| ITR: 38000 | Loss: \u001b[1m\u001b[33m6.8490\u001b[0m\n",
      "updated.... average c-index = 0.6824\n",
      "|| ITR: 39000 | Loss: \u001b[1m\u001b[33m6.7879\u001b[0m\n",
      "|| ITR: 40000 | Loss: \u001b[1m\u001b[33m6.7483\u001b[0m\n",
      "updated.... average c-index = 0.6892\n",
      "|| ITR: 41000 | Loss: \u001b[1m\u001b[33m6.6998\u001b[0m\n",
      "|| ITR: 42000 | Loss: \u001b[1m\u001b[33m6.6003\u001b[0m\n",
      "updated.... average c-index = 0.6924\n",
      "|| ITR: 43000 | Loss: \u001b[1m\u001b[33m6.6735\u001b[0m\n",
      "|| ITR: 44000 | Loss: \u001b[1m\u001b[33m6.6444\u001b[0m\n",
      "updated.... average c-index = 0.6934\n",
      "|| ITR: 45000 | Loss: \u001b[1m\u001b[33m6.5477\u001b[0m\n",
      "updated.... average c-index = 0.6948\n",
      "|| ITR: 46000 | Loss: \u001b[1m\u001b[33m6.5197\u001b[0m\n",
      "updated.... average c-index = 0.7006\n",
      "|| ITR: 47000 | Loss: \u001b[1m\u001b[33m6.5139\u001b[0m\n",
      "|| ITR: 48000 | Loss: \u001b[1m\u001b[33m6.4661\u001b[0m\n",
      "updated.... average c-index = 0.7032\n",
      "|| ITR: 49000 | Loss: \u001b[1m\u001b[33m6.4627\u001b[0m\n",
      "|| ITR: 50000 | Loss: \u001b[1m\u001b[33m6.4792\u001b[0m\n",
      "updated.... average c-index = 0.7039\n",
      "Current best: 0.7038684415744475\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This runs random search to find the optimized hyper-parameters using cross-validation\n",
    "INPUTS:\n",
    "    - OUT_ITERATION: # of training/testing splits\n",
    "    - RS_ITERATION: # of random search iteration\n",
    "    - data_mode: mode to select the time-to-event data from \"import_data.py\"\n",
    "    - seed: random seed for training/testing/validation splits\n",
    "    - EVAL_TIMES: list of time-horizons at which the performance is maximized; \n",
    "                  the validation is performed at given EVAL_TIMES (e.g., [12, 24, 36])\n",
    "OUTPUTS:\n",
    "    - \"hyperparameters_log.txt\" is the output\n",
    "    - Once the hyper parameters are optimized, run \"summarize_results.py\" to get the final results.\n",
    "'''\n",
    "\n",
    "# this saves the current hyperparameters\n",
    "def save_logging(dictionary, log_name):\n",
    "    with open(log_name, 'w') as f:\n",
    "        for key, value in dictionary.items():\n",
    "            f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "# this open can calls the saved hyperparameters\n",
    "def load_logging(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        def is_float(input):\n",
    "            try:\n",
    "                num = float(input)\n",
    "            except ValueError:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key,value = line.strip().split(':', 1)\n",
    "                if value.isdigit():\n",
    "                    data[key] = int(value)\n",
    "                elif is_float(value):\n",
    "                    data[key] = float(value)\n",
    "                elif value == 'None':\n",
    "                    data[key] = None\n",
    "                else:\n",
    "                    data[key] = value\n",
    "            else:\n",
    "                pass # deal with bad lines of text here    \n",
    "    return data\n",
    "\n",
    "\n",
    "# this randomly select hyperparamters based on the given list of candidates\n",
    "def get_random_hyperparameters(out_path):\n",
    "    SET_BATCH_SIZE    = [32, 64, 128] #mb_size\n",
    " \n",
    "    SET_LAYERS        = [1,2,3,5] #number of layers\n",
    "    SET_NODES         = [50, 100, 200, 300] #number of nodes\n",
    "\n",
    "    SET_ACTIVATION_FN = ['relu', 'elu', 'tanh'] #non-linear activation functions\n",
    "\n",
    "    SET_ALPHA         = [0.1, 0.5, 1.0, 3.0, 5.0] #alpha values -> log-likelihood loss \n",
    "    SET_BETA          = [0.1, 0.5, 1.0, 3.0, 5.0] #beta values -> ranking loss\n",
    "    SET_GAMMA         = [0.1, 0.5, 1.0, 3.0, 5.0] #gamma values -> calibration loss\n",
    "\n",
    "    new_parser = {'mb_size': SET_BATCH_SIZE[np.random.randint(len(SET_BATCH_SIZE))],\n",
    "\n",
    "                 'iteration': 50000,\n",
    "\n",
    "                 'keep_prob': 0.6,\n",
    "                 'lr_train': 1e-4,\n",
    "\n",
    "                 'h_dim_shared': SET_NODES[np.random.randint(len(SET_NODES))],\n",
    "                 'h_dim_CS': SET_NODES[np.random.randint(len(SET_NODES))],\n",
    "                 'num_layers_shared':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n",
    "                 'num_layers_CS':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n",
    "                 'active_fn': SET_ACTIVATION_FN[np.random.randint(len(SET_ACTIVATION_FN))],\n",
    "\n",
    "                 'alpha':1.0, #default (set alpha = 1.0 and change beta and gamma)\n",
    "                 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n",
    "                 'gamma':0,   #default (no calibration loss)\n",
    "                 # 'alpha':SET_ALPHA[np.random.randint(len(SET_ALPHA))],\n",
    "                 # 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n",
    "                 # 'gamma':SET_GAMMA[np.random.randint(len(SET_GAMMA))],\n",
    "\n",
    "                 'out_path':out_path}\n",
    "    \n",
    "    return new_parser #outputs the dictionary of the randomly-chosen hyperparamters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### MAIN SETTING\n",
    "OUT_ITERATION               = 1\n",
    "RS_ITERATION                = 50\n",
    "data_number                 = '1'\n",
    "data_mode                   = 'mort_p'\n",
    "seed                        = 1234\n",
    "\n",
    "\n",
    "##### IMPORT DATASET\n",
    "'''\n",
    "    num_Category            = typically, max event/censoring time * 1.2 (to make enough time horizon)\n",
    "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
    "    max_length              = maximum number of measurements\n",
    "    x_dim                   = data dimension including delta (num_features)\n",
    "    mask1, mask2            = used for cause-specific network (FCNet structure)\n",
    "    EVAL_TIMES              = set specific evaluation time horizons at which the validatoin performance is maximized. \n",
    "                              (This must be selected based on the dataset)\n",
    "'''\n",
    "\n",
    "\n",
    "if data_mode == 'mort_d':\n",
    "    cols = ['orig_time', 'mat_time', 'balance_time', 'LTV_time', 'interest_rate_time', 'hpi_time', 'gdp_time', 'uer_time', \n",
    "            'balance_orig_time', 'FICO_orig_time', 'LTV_orig_time', 'Interest_Rate_orig_time', 'hpi_orig_time', \n",
    "            'REtype_CO_orig_time', 'REtype_PU_orig_time', 'REtype_SF_orig_time', 'investor_orig_time']\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = import_dataset_mort_d(norm_mode = 'standard')\n",
    "    EVAL_TIMES = [20, 40, 60]\n",
    "\n",
    "if data_mode == 'mort_p':\n",
    "    cols = ['orig_time', 'mat_time', 'balance_time', 'LTV_time', 'interest_rate_time', 'hpi_time', 'gdp_time', 'uer_time', \n",
    "            'balance_orig_time', 'FICO_orig_time', 'LTV_orig_time', 'Interest_Rate_orig_time', 'hpi_orig_time', \n",
    "            'REtype_CO_orig_time', 'REtype_PU_orig_time', 'REtype_SF_orig_time', 'investor_orig_time']\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = import_dataset_mort_p(norm_mode = 'standard')\n",
    "    EVAL_TIMES = [20, 40, 60]\n",
    "\n",
    "if data_mode == 'ndb_d':\n",
    "    cols = ['int.rate', 'orig.upb', 'fico.score', 'dti.r',\n",
    "       'ltv.r', 'bal.repaid', 't.act.12m', 't.del.30d.12m', 't.del.60d.12m',\n",
    "       'hpi.st.d.t.o', 'hpi.zip.o', 'hpi.zip.d.t.o', 'ppi.c.FRMA',\n",
    "       'TB10Y.d.t.o', 'FRMA30Y.d.t.o', 'ppi.o.FRMA', 'equity.est',\n",
    "       'hpi.st.log12m', 'hpi.r.st.us', 'hpi.r.zip.st', 'st.unemp.r12m',\n",
    "       'st.unemp.r3m', 'TB10Y.r12m', 'T10Y3MM', 'T10Y3MM.r12m']\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = import_dataset_ndb_d(data_number, norm_mode = 'standard')\n",
    "    EVAL_TIMES = [24, 48, 72]\n",
    "if data_mode == 'ndb_p':\n",
    "    cols = ['int.rate', 'orig.upb', 'fico.score', 'dti.r',\n",
    "       'ltv.r', 'bal.repaid', 't.act.12m', 't.del.30d.12m', 't.del.60d.12m',\n",
    "       'hpi.st.d.t.o', 'hpi.zip.o', 'hpi.zip.d.t.o', 'ppi.c.FRMA',\n",
    "       'TB10Y.d.t.o', 'FRMA30Y.d.t.o', 'ppi.o.FRMA', 'equity.est',\n",
    "       'hpi.st.log12m', 'hpi.r.st.us', 'hpi.r.zip.st', 'st.unemp.r12m',\n",
    "       'st.unemp.r3m', 'TB10Y.r12m', 'T10Y3MM', 'T10Y3MM.r12m']\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = import_dataset_ndb_p(data_number, norm_mode = 'standard')\n",
    "    EVAL_TIMES = [24, 48, 72]\n",
    "\n",
    "\n",
    "\n",
    "DATA = (data, time, label)\n",
    "MASK = (mask1, mask2) #masks are required to calculate loss functions without for-loops.\n",
    "\n",
    "out_path      = 'C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\' + data_mode + '\\\\' + data_number\n",
    "\n",
    "for itr in range(OUT_ITERATION):\n",
    "    \n",
    "    if not os.path.exists(out_path + '\\\\itr_' + str(itr) + '\\\\'):\n",
    "        os.makedirs(out_path + '\\\\itr_' + str(itr) + '\\\\')\n",
    "\n",
    "    max_valid = 0.\n",
    "    log_name = out_path + '\\\\itr_' + str(itr) + '\\\\hyperparameters_log.txt'\n",
    "\n",
    "    for r_itr in range(RS_ITERATION):\n",
    "        print('OUTER_ITERATION: ' + str(itr))\n",
    "        print('Random search... itr: ' + str(r_itr))\n",
    "        new_parser = get_random_hyperparameters(out_path)\n",
    "        print(new_parser)\n",
    "\n",
    "        # get validation performance given the hyperparameters\n",
    "        tmp_max = get_valid_performance(DATA, MASK, new_parser, itr, EVAL_TIMES, MAX_VALUE=max_valid)\n",
    "\n",
    "        if tmp_max > max_valid:\n",
    "            max_valid = tmp_max\n",
    "            max_parser = new_parser\n",
    "            save_logging(max_parser, log_name)  #save the hyperparameters if this provides the maximum validation performance\n",
    "\n",
    "        print('Current best: ' + str(max_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITR: 1 DATA MODE: mort_p (a:1.0 b:1.0 c:0)\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\raibe\\Desktop\\Thesis Code\\Deephit\\results\\mort_p\\1\\itr_0\\models\\model_itr_0\n",
      "hit index: 0.9048046119174702, brier: 0.5613763650191309\n",
      "hit index: 0.7028905743683171, brier: 0.010139913207136749\n",
      "out index: 0.8999651378941377\n"
     ]
    }
   ],
   "source": [
    "_EPSILON = 1e-08\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter\n",
    "from pycox.evaluation import EvalSurv\n",
    "import os\n",
    "# import sys\n",
    "\n",
    "from termcolor import colored\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_logging(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        def is_float(input):\n",
    "            try:\n",
    "                num = float(input)\n",
    "            except ValueError:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key,value = line.strip().split(':', 1)\n",
    "                if value.isdigit():\n",
    "                    data[key] = int(value)\n",
    "                elif is_float(value):\n",
    "                    data[key] = float(value)\n",
    "                elif value == 'None':\n",
    "                    data[key] = None\n",
    "                else:\n",
    "                    data[key] = value\n",
    "            else:\n",
    "                pass # deal with bad lines of text here    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "##### MAIN SETTING\n",
    "OUT_ITERATION               = 1\n",
    "data_number                 = '1'\n",
    "data_mode                   = 'mort_p' \n",
    "seed                        = 1234\n",
    "\n",
    "#EVAL_TIMES                  = [20, 40, 60] # evalution times (for C-index and Brier-Score)\n",
    "\n",
    "\n",
    "##### IMPORT DATASET\n",
    "'''\n",
    "    num_Category            = max event/censoring time * 1.2 (to make enough time horizon)\n",
    "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
    "    max_length              = maximum number of measurements\n",
    "    x_dim                   = data dimension including delta (num_features)\n",
    "    mask1, mask2            = used for cause-specific network (FCNet structure)\n",
    "'''\n",
    "\n",
    "if data_mode == 'mort_d':\n",
    "    cols = ['orig_time', 'mat_time', 'balance_time', 'LTV_time', 'interest_rate_time', 'hpi_time', 'gdp_time', 'uer_time', \n",
    "            'balance_orig_time', 'FICO_orig_time', 'LTV_orig_time', 'Interest_Rate_orig_time', 'hpi_orig_time', \n",
    "            'REtype_CO_orig_time', 'REtype_PU_orig_time', 'REtype_SF_orig_time', 'investor_orig_time']\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = import_dataset_mort_d(norm_mode = 'standard')\n",
    "    EVAL_TIMES = [20, 40, 60]\n",
    "\n",
    "if data_mode == 'mort_p':\n",
    "    cols = ['orig_time', 'mat_time', 'balance_time', 'LTV_time', 'interest_rate_time', 'hpi_time', 'gdp_time', 'uer_time', \n",
    "            'balance_orig_time', 'FICO_orig_time', 'LTV_orig_time', 'Interest_Rate_orig_time', 'hpi_orig_time', \n",
    "            'REtype_CO_orig_time', 'REtype_PU_orig_time', 'REtype_SF_orig_time', 'investor_orig_time']\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = import_dataset_mort_p(norm_mode = 'standard')\n",
    "    EVAL_TIMES = [20, 40, 60]\n",
    "    \n",
    "if data_mode == 'ndb_d':\n",
    "    cols = ['int.rate', 'orig.upb', 'fico.score', 'dti.r',\n",
    "       'ltv.r', 'bal.repaid', 't.act.12m', 't.del.30d.12m', 't.del.60d.12m',\n",
    "       'hpi.st.d.t.o', 'hpi.zip.o', 'hpi.zip.d.t.o', 'ppi.c.FRMA',\n",
    "       'TB10Y.d.t.o', 'FRMA30Y.d.t.o', 'ppi.o.FRMA', 'equity.est',\n",
    "       'hpi.st.log12m', 'hpi.r.st.us', 'hpi.r.zip.st', 'st.unemp.r12m',\n",
    "       'st.unemp.r3m', 'TB10Y.r12m', 'T10Y3MM', 'T10Y3MM.r12m']\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = import_dataset_ndb_d(data_number, norm_mode = 'standard')\n",
    "    EVAL_TIMES = [24, 48, 72]\n",
    "if data_mode == 'ndb_p':\n",
    "    cols = ['int.rate', 'orig.upb', 'fico.score', 'dti.r',\n",
    "       'ltv.r', 'bal.repaid', 't.act.12m', 't.del.30d.12m', 't.del.60d.12m',\n",
    "       'hpi.st.d.t.o', 'hpi.zip.o', 'hpi.zip.d.t.o', 'ppi.c.FRMA',\n",
    "       'TB10Y.d.t.o', 'FRMA30Y.d.t.o', 'ppi.o.FRMA', 'equity.est',\n",
    "       'hpi.st.log12m', 'hpi.r.st.us', 'hpi.r.zip.st', 'st.unemp.r12m',\n",
    "       'st.unemp.r3m', 'TB10Y.r12m', 'T10Y3MM', 'T10Y3MM.r12m']\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = import_dataset_ndb_p(data_number, norm_mode = 'standard')\n",
    "    EVAL_TIMES = [24, 48, 72]\n",
    "#else:\n",
    "#    print('ERROR:  DATA_MODE NOT FOUND !!!')\n",
    "\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(mask1)  # dim of mask1: [subj, Num_Event, Num_Category]\n",
    "\n",
    "\n",
    "\n",
    "in_path = 'C:\\\\Users\\\\raibe\\Desktop\\\\Thesis Code\\\\Deephit\\\\results\\\\' + data_mode + '\\\\' + data_number  #+ '/results/'\n",
    "\n",
    "#if not os.path.exists(in_path):\n",
    "#    os.makedirs(in_path)\n",
    "\n",
    "\n",
    "FINAL1 = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "FINAL2 = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "FINAL1a = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "FINAL2a = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "lifelines_c = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "surv_br = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "\n",
    "\n",
    "for out_itr in range(OUT_ITERATION):\n",
    "    #in_hypfile = in_path + '/itr_' + str(out_itr) + '/hyperparameters_log.txt'\n",
    "    in_hypfile = in_path + '\\\\hyperparameters_log.txt'   \n",
    "    in_parser = load_logging(in_hypfile)\n",
    "\n",
    "\n",
    "    ##### HYPER-PARAMETERS\n",
    "    mb_size                     = in_parser['mb_size']\n",
    "\n",
    "    iteration                   = in_parser['iteration']\n",
    "\n",
    "    keep_prob                   = in_parser['keep_prob']\n",
    "    lr_train                    = in_parser['lr_train']\n",
    "\n",
    "    h_dim_shared                = in_parser['h_dim_shared']\n",
    "    h_dim_CS                    = in_parser['h_dim_CS']\n",
    "    num_layers_shared           = in_parser['num_layers_shared']\n",
    "    num_layers_CS               = in_parser['num_layers_CS']\n",
    "\n",
    "    if in_parser['active_fn'] == 'relu':\n",
    "        active_fn                = tf.nn.relu\n",
    "    elif in_parser['active_fn'] == 'elu':\n",
    "        active_fn                = tf.nn.elu\n",
    "    elif in_parser['active_fn'] == 'tanh':\n",
    "        active_fn                = tf.nn.tanh\n",
    "    else:\n",
    "        print('Error!')\n",
    "\n",
    "\n",
    "    initial_W                   = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    alpha                       = in_parser['alpha']  #for log-likelihood loss\n",
    "    beta                        = in_parser['beta']  #for ranking loss\n",
    "    gamma                       = in_parser['gamma']  #for RNN-prediction loss\n",
    "    parameter_name              = 'a' + str('%02.0f' %(10*alpha)) + 'b' + str('%02.0f' %(10*beta)) + 'c' + str('%02.0f' %(10*gamma))\n",
    "\n",
    "\n",
    "    ##### MAKE DICTIONARIES\n",
    "    # INPUT DIMENSIONS\n",
    "    input_dims                  = { 'x_dim'         : x_dim,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category}\n",
    "\n",
    "    # NETWORK HYPER-PARMETERS\n",
    "    network_settings            = { 'h_dim_shared'         : h_dim_shared,\n",
    "                                    'h_dim_CS'          : h_dim_CS,\n",
    "                                    'num_layers_shared'    : num_layers_shared,\n",
    "                                    'num_layers_CS'    : num_layers_CS,\n",
    "                                    'active_fn'      : active_fn,\n",
    "                                    'initial_W'         : initial_W }\n",
    "\n",
    "\n",
    "    # for out_itr in range(OUT_ITERATION):\n",
    "    print ('ITR: ' + str(out_itr+1) + ' DATA MODE: ' + data_mode + ' (a:' + str(alpha) + ' b:' + str(beta) + ' c:' + str(gamma) + ')' )\n",
    "    ##### CREATE DEEPFHT NETWORK\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ### TRAINING-TESTING SPLIT\n",
    "    (tr_data,te_data, tr_time,te_time, tr_label,te_label, \n",
    "     tr_mask1,te_mask1, tr_mask2,te_mask2)  = train_test_split(data, time, label, mask1, mask2, test_size=0.20, random_state=seed) \n",
    "\n",
    "    (tr_data,va_data, tr_time,va_time, tr_label,va_label, \n",
    "     tr_mask1,va_mask1, tr_mask2,va_mask2)  = train_test_split(tr_data, tr_time, tr_label, tr_mask1, tr_mask2, test_size=0.20, random_state=seed) \n",
    "    \n",
    "    ##### PREDICTION & EVALUATION\n",
    "    saver.restore(sess, in_path + '\\\\itr_' + str(out_itr) +  '\\\\models\\\\model_itr_' + str(out_itr))\n",
    "\n",
    "    ### PREDICTION\n",
    "    pred = model.predict(te_data)\n",
    "    \n",
    "    ### EVALUATION\n",
    "    #result1, result2 = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "    #result1a, result2a = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "    #for t, t_time in enumerate(EVAL_TIMES):\n",
    "    #    eval_horizon = int(t_time)\n",
    "\n",
    "    #    if eval_horizon >= num_Category:\n",
    "    #        print( 'ERROR: evaluation horizon is out of range')\n",
    "    #        result1[:, t] = result2[:, t] = -1\n",
    "    #    else:\n",
    "    #        # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "    #        risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n",
    "    #        for k in range(num_Event):\n",
    "    #            #pdb.set_trace()\n",
    "    #            result1a[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "    #            result2a[k, t] = brier_score(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "    #            result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "    #            result2[k, t] = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "    #FINAL1[:, :, out_itr] = result1\n",
    "    #FINAL2[:, :, out_itr] = result2\n",
    "    #FINAL1a[:, :, out_itr] = result1a\n",
    "    #FINAL2a[:, :, out_itr] = result2a\n",
    "    k = 0\n",
    "    risk = np.sum(pred[:,:,:(60+1)], axis=2)\n",
    "    hit_ind = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), 60+1) #-1 for no event (not comparable)\n",
    "    hit_br = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), 72+1) #-1 for no event (not comparable)\n",
    "\n",
    "    prob = np.subtract(1.0, risk)\n",
    "    te_lbl = (te_label[:,0] == k+1).astype(int)\n",
    "    te_evt_time = np.reshape(np.array(te_time), [1, -1])[0]\n",
    "    c_lifelines = concordance_index(event_times=te_evt_time, predicted_scores=prob, event_observed=te_lbl)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### for Surv library \n",
    "    #train_o = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\ndb_p\\\\8\\\\trainDRSA.txt', header = None)\n",
    "    #train_z = pd.read_csv('C:\\\\Users\\\\raibe\\\\Desktop\\\\Thesis Code\\\\DRSA\\\\data\\\\ndb_p\\\\8\\\\train_yzb.txt', header = None, sep=\" \")\n",
    "\n",
    "    ####\n",
    "    #tr_evt_time = np.reshape(np.array(tr_time), [1, -1])[0]\n",
    "    #tr_lbl = (tr_label[:,0] == k+1).astype(float)\n",
    "    #tr_lbl_b = np.array(tr_lbl, dtype=bool)\n",
    "    ##true_z = train_z.iloc[:,1].values\n",
    "    ##t_label = train_o.iloc[:,26].values\n",
    "    ##t_label = np.array(t_label, dtype=bool)\n",
    "    #r_train = np.core.records.fromarrays([tr_lbl_b, tr_evt_time], names='cens,time')\n",
    "    ###\n",
    "    #true_t = train_o.iloc[:,25].values\n",
    "    ##t_label = train_o.iloc[:,26].values\n",
    "    #te_lbl_b = np.array(te_lbl, dtype=bool)\n",
    "    #r_test = np.core.records.fromarrays([te_lbl_b, te_evt_time], names='cens,time')\n",
    "    prob_l = np.subtract(1.0, np.array(pred[:,:,:(60+1)]))\n",
    "    #i_br = integrated_brier_score(r_train, r_test, prob, 59)\n",
    "    ll_q = np.array(prob_l).transpose(2,0,1).reshape(60+1,-1)\n",
    "    df_q = pd.DataFrame(ll_q) # ll[:l_time.max() + 1,:]\n",
    "    ev = EvalSurv(df_q, te_evt_time, te_lbl, censor_surv='km')\n",
    "    pyind = ev.concordance_td()\n",
    "    time_grid = np.linspace(te_evt_time.min(), te_evt_time.max(), te_evt_time.max())\n",
    "    pybr= ev.integrated_brier_score(time_grid) \n",
    "    \n",
    "    print(\"hit index: {0}, brier: {1}\".format(hit_ind, pybr))\n",
    "    print(\"hit index: {0}, brier: {1}\".format(pyind , hit_br))\n",
    "    print(\"out index: {0}\".format(c_lifelines))\n",
    "    \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Deephit)",
   "language": "python",
   "name": "deephit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
