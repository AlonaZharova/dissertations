{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg_1lJMSpNd_"
   },
   "source": [
    "# Request & Save Data from The Guardian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl9p90VNpNeC"
   },
   "source": [
    "## Settings & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6R9sCR4pNeD"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import date, timedelta\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yg86hJy9pNeE"
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/M/Google_Drive/Thesis/Topic-Modeling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLCTG1IXpNeF"
   },
   "source": [
    "## 1. Request Data (The Guardian API)\n",
    "data was collected on 07/10/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx-CRSgfpNeF"
   },
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hb6F91QPpNeG"
   },
   "outputs": [],
   "source": [
    "path_json = 'Data/Technology-Data/raw/'\n",
    "makedirs(path_json, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgtN6tOcpNeH"
   },
   "outputs": [],
   "source": [
    "API_Key = open('Data/Infos/API_Key_Guardian.txt').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAI7mgMNpNeI",
    "outputId": "0f1fba70-16e8-4e71-a4ce-48eeda6618dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Data from 2000-01-01 to 2019-12-31, i.e. 7305 days\n"
     ]
    }
   ],
   "source": [
    "from_date = date(2000,1,1)\n",
    "to_date = date(2019,12,31) #date.today()-timedelta(days=1)\n",
    "dayrange = range((to_date - from_date).days + 1)\n",
    "print('Request Data from {} to {}, i.e. {} days'.format(from_date, to_date, len(dayrange)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65H321qgpNeJ"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'api-key': API_Key,\n",
    "    'use-date': 'published',\n",
    "    'from-date': '',\n",
    "    'to-date': '',\n",
    "    'order-by': \"newest\",\n",
    "    'show-fields': \"all\",\n",
    "    'show-tags': 'all',\n",
    "    'show-sections': 'true',\n",
    "    'section': 'technology',\n",
    "    'show-references': 'all',\n",
    "    'lang': 'en'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qh3vLCOYpNeK"
   },
   "source": [
    "### Request & Save Data (.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbenek7ZpNeN"
   },
   "outputs": [],
   "source": [
    "empty_response_dates = []\n",
    "already_existing_dates = []\n",
    "successful_download_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SMPTB9JpNeO"
   },
   "outputs": [],
   "source": [
    "# partly adapted from https://gist.github.com/dannguyen/c9cb220093ee4c12b840, 09/2020\n",
    "for day in dayrange:\n",
    "    try:\n",
    "        dt = from_date + timedelta(days=day)\n",
    "        date_str = dt.strftime('%Y-%m-%d')\n",
    "        file_name = join(path_json, date_str + '.json')\n",
    "\n",
    "        if not exists(file_name):\n",
    "            all_results = []\n",
    "            params['from-date'] = date_str\n",
    "            params['to-date'] = date_str\n",
    "            article_number = 1\n",
    "            amount_articles = 1\n",
    "\n",
    "            while article_number <= amount_articles:\n",
    "                params['page'] = article_number\n",
    "                resp = requests.get('http://content.guardianapis.com/search', params)\n",
    "                data = resp.json()\n",
    "                all_results.extend(data['response']['results'])\n",
    "                amount_articles = data['response']['pages']\n",
    "                article_number += 1\n",
    "\n",
    "            if len(all_results) > 0:\n",
    "                with open(file_name, 'w') as f:\n",
    "                    f.write(json.dumps(all_results, indent=2))\n",
    "                    successful_download_count = successful_download_count + 1\n",
    "            else:\n",
    "                empty_response_dates.append(date_str)\n",
    "        else:\n",
    "            already_existing_dates.append(date_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        if e == 'response':\n",
    "            print('Stop Iteration (API limitations): ', e)\n",
    "        else:\n",
    "            print('Exception: ', e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_Bhq57fpNeO",
    "outputId": "9dc8136b-941b-4568-a1d3-e5332f63e84f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...Number of dates for which articles exist in the json-directory: 6979.\n",
      "  ...Number of dates for which articles were successfully downloaded: 6979.\n",
      "  ...Number of dates for which articles were already saved: 0.\n",
      "  ...Number of dates for which no data could be received: 326.\n"
     ]
    }
   ],
   "source": [
    "print('  ...Number of dates for which articles exist in the json-directory: %d.' % (len(already_existing_dates)+successful_download_count))\n",
    "print('  ...Number of dates for which articles were successfully downloaded: %d.' % successful_download_count)\n",
    "print('  ...Number of dates for which articles were already saved: %d.' % len(already_existing_dates))\n",
    "print('  ...Number of dates for which no data could be received: %d.' % len(empty_response_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5agDgMipNeP"
   },
   "source": [
    "## 2. Create DataFrame from nested .json files (.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjBV4HGMpNeP"
   },
   "outputs": [],
   "source": [
    "path_csv = 'Data/Technology-Data/raw/'\n",
    "makedirs(path_csv, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXLz5XStpNeQ"
   },
   "outputs": [],
   "source": [
    "csv_filename =  os.path.join(path_csv, 'raw.csv')\n",
    "meta_columns = ['id', 'type', 'sectionId', 'sectionName', 'webPublicationDate', 'webTitle', 'webUrl', 'apiUrl',\n",
    "                'fields', 'isHosted', 'pillarId', 'pillarName']\n",
    "csv_columns =['filename','id','webUrl',\n",
    "              'author',\n",
    "              'headline','trailText','bodyText',\n",
    "              'publication',\n",
    "              'webPublicationDate','newspaperEditionDate','firstPublicationDate',\n",
    "              'charCount','wordcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc16VZrtpNeQ"
   },
   "outputs": [],
   "source": [
    "news = pd.DataFrame()\n",
    "error_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEOwBX6cpNeQ",
    "outputId": "c06c22f1-e6e8-49b3-9969-13ead1a57579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...A (new) DataFrame called \"news\" has been constructed and saved as  Data/Technology-Data/raw/raw.csv\n",
      "  ...Shape of that DataFrame: (51819, 13)\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in sorted(os.walk(path_json)):\n",
    "    for file in files:\n",
    "        try:\n",
    "            if file.endswith(\".json\") and 'checkpoint' not in file:\n",
    "                json_file = os.path.join(root, file)\n",
    "                with open(json_file) as f:\n",
    "                    json_data = json.load(f)\n",
    "                # normalise data from nested .json:\n",
    "                df1 = pd.json_normalize(json_data, 'tags', meta=meta_columns, record_prefix='tag_')\n",
    "                df2 = pd.json_normalize(df1['fields']).copy()\n",
    "                df1.drop(columns=['fields'], inplace=True)\n",
    "                df_all = pd.concat([df1, df2], axis=1)\n",
    "                df_all['filename'] = json_file\n",
    "                # extract authors (some articles have multiple authors!):\n",
    "                authors = df_all.loc[df_all['tag_type'] == 'contributor', ['id','tag_webTitle']].copy()\n",
    "                authors.drop_duplicates(inplace=True)\n",
    "                authors.rename({'tag_webTitle':'author'}, axis=1, inplace=True)\n",
    "                authors = authors.groupby('id')['author'].apply(list)\n",
    "                authors = authors.astype('str')\n",
    "                # add data to dataframe for selected columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='tag_')))]\n",
    "                df_all.drop_duplicates(inplace=True)\n",
    "                df_all = df_all.merge(authors, on = 'id', how = 'left')\n",
    "                df = pd.DataFrame(columns=csv_columns)\n",
    "                df = df.append(df_all[df.columns.intersection(df_all.columns)])\n",
    "                df = df.drop_duplicates().copy()\n",
    "                news = news.append(df, ignore_index=True)\n",
    "                del df1, df2, authors, df_all, df\n",
    "        except Exception as E:\n",
    "            print(E, 'in line: ', sys.exc_info()[2].tb_lineno)\n",
    "try:\n",
    "    news.to_csv(csv_filename, sep=';')\n",
    "    print('  ...A (new) DataFrame called \"news\" has been constructed and saved as ', csv_filename)\n",
    "    print('  ...Shape of that DataFrame:', news.shape)\n",
    "\n",
    "except Exception as e:\n",
    "    print('  ...Saving of .csv-file failed: ', e)\n",
    "\n",
    "if len(error_files)>0:\n",
    "    print('  ...Files that could not be added to the DataFrame: ', error_files)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_Data-Collection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
