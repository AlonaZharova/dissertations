{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nKhi3euqG8k",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Final pre-processing \n",
    "- Includes the final pre-processing steps as required by D-ETM\n",
    "- Ensures that the data for the word clouds in 07_Visualisation.ipynb has gone through exactly the same pre-processing steps (min_df,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HT7pBVYqG8n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx9YFfLbqG8o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import html\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import re\n",
    "from scipy import sparse\n",
    "from scipy.io import savemat, loadmat\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import time\n",
    "import unidecode\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCA9XvfQqG8p",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VppQyzIqG8q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.close()\n",
    "plt.interactive(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSH6MJbPqG8r"
   },
   "outputs": [],
   "source": [
    "group_years = True\n",
    "use_subset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzPCMGgwqG8s",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N40gMZjbqG8s",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/M/Google_Drive/Thesis/Topic-Modeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEXsYnTGqG8t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'Data/Technology-Data/processed/preprocessed/'\n",
    "data_dir_final = 'Data/Technology-Data/processed/final/'\n",
    "results_dir = 'Results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8Sh62gCqG8u"
   },
   "outputs": [],
   "source": [
    "if use_subset:\n",
    "    csv_file = data_dir + 'news_sub.csv'\n",
    "    txt_file = data_dir + 'texts_sub.txt'\n",
    "else:\n",
    "    csv_file = data_dir + 'news.csv'\n",
    "    txt_file = data_dir + 'texts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRLQv8VmqG8v",
    "outputId": "5d9b67e3-18a7-45bd-a47e-bafdb6bc231d"
   },
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpiyxMzPqG8z"
   },
   "source": [
    "## Build the Vocabulary & Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL8xXyQgqG8z"
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krBX1aKxqG8z"
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv(csv_file, sep=\";\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytqN_qqtqG80"
   },
   "outputs": [],
   "source": [
    "if group_years:\n",
    "        news.loc[news['year']<2002, 'year_gr'] = '2000-2001'\n",
    "        news.loc[(news['year']>=2002) & (news['year']<2004), 'year_gr'] = '2002-2003'\n",
    "        news.loc[(news['year']>=2004) & (news['year']<2006), 'year_gr'] = '2004-2005'\n",
    "        news.loc[(news['year']>=2006) & (news['year']<2008), 'year_gr'] = '2006-2007'\n",
    "        news.loc[(news['year']>=2008) & (news['year']<2010), 'year_gr'] = '2008-2009'\n",
    "        news.loc[(news['year']>=2010) & (news['year']<2012), 'year_gr'] = '2010-2011'\n",
    "        news.loc[(news['year']>=2012) & (news['year']<2014), 'year_gr'] = '2012-2013'\n",
    "        news.loc[(news['year']>=2014) & (news['year']<2016), 'year_gr'] = '2014-2015'\n",
    "        news.loc[(news['year']>=2016) & (news['year']<2018), 'year_gr'] = '2016-2017'\n",
    "        news.loc[(news['year']>=2018) & (news['year']<2020), 'year_gr'] = '2018-2019'\n",
    "else:\n",
    "    print(\"Don't group years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzdM25TnqG80",
    "outputId": "68e6a258-0964-458c-9a38-30d9eeb85d52"
   },
   "outputs": [],
   "source": [
    "if group_years:\n",
    "    print(Counter(news['year_gr']))\n",
    "else:\n",
    "    print(Counter(news['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jeUsCVkqG9D"
   },
   "outputs": [],
   "source": [
    "if group_years:\n",
    "    timestamps = news['year_gr'].tolist()\n",
    "else:\n",
    "    timestamps = news['year'].tolist()\n",
    "articles = news['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMlIeMQBqG9F"
   },
   "source": [
    "## Create Input Data for D-ETM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM63_ciDqqQn"
   },
   "source": [
    "This section of the notebook follows the pre-processing steps by Adji Dieng\n",
    "(https://github.com/adjidieng/DETM/blob/master/scripts/data_undebates.py) as required for D-ETM. However, a few adjustments were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lVbdcKWqG9F"
   },
   "source": [
    "### Create mapping dictionaries for timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQUE05DvqG9F"
   },
   "outputs": [],
   "source": [
    "all_times = sorted(set(timestamps))\n",
    "time2id = dict([(t, i) for i, t in enumerate(all_times)])\n",
    "id2time = dict([(i, t) for i, t in enumerate(all_times)])\n",
    "time_list = [id2time[i] for i in range(len(all_times))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xbE6I0nqG9F",
    "outputId": "83e0546b-9962-4b72-8c6f-d33f263986a7"
   },
   "outputs": [],
   "source": [
    "time2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhTq5W7wqG9N"
   },
   "source": [
    "### Split into train, test, valid and create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "iVfGaYUFqG9N",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f0d876dc-5055-4203-9801-64481445e4ed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_articles = len(articles)\n",
    "print('Number of articles: ', num_articles)\n",
    "\n",
    "trSize = int(np.floor(0.85 * num_articles))\n",
    "tsSize = int(np.floor(0.10 * num_articles))\n",
    "vaSize = int(num_articles - trSize - tsSize)\n",
    "\n",
    "print('Defined training set size: %d, test set size: %d, validation set size: %d' % (trSize, tsSize, vaSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFjsFk_TqG9O"
   },
   "outputs": [],
   "source": [
    "idx_permute = np.random.permutation(num_articles).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOIYKIVkqG9P"
   },
   "source": [
    "### Construct Vocabulary for Modeling Using Context-Insensitive Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RKfTUJsqG9P"
   },
   "source": [
    "#### Maximum / minimum article frequency:\n",
    "- proportion of articles: 0.7 (e.g., ignore words occurring in > 70 % of the articles)\n",
    "- absolute count of articles: 50 (e.g., only include words occurring in at least 50 documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hBdKpDNqG9P",
    "outputId": "d18e5f54-a493-4ccd-ad66-9a5760c2d6ca"
   },
   "outputs": [],
   "source": [
    "0.001*len([a for a in articles if len(a)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJRdtX-8qG9P",
    "outputId": "26b0cb8e-0640-4745-c23d-2b4104d1bfd5"
   },
   "outputs": [],
   "source": [
    "max_df = 0.7\n",
    "min_df = 50\n",
    "print(' max df: %d%%, \\n min_df: %d'%(max_df*100, min_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xVtGPZ6qG9Q"
   },
   "source": [
    "#### Apply CountVectorizer\n",
    "- Count article frequency of words\n",
    "- Convert collection of articles to a matrix of token counts\n",
    "- Stopwords will not be removed as that has already been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "yIh0M3FWqG9R",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4acc90da-5af7-4d34-c121-d588a9100c62",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=None, lowercase=False, tokenizer=lambda x: x.split(' '))\n",
    "cvz = cvectorizer.fit_transform(articles).sign()\n",
    "print('Shape of CVZ matrix (articles, words): ', cvz.shape)\n",
    "num_articles = cvz.shape[0]\n",
    "vocab_full_size = cvz.shape[1]\n",
    "cvz_array = cvz.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79FExIpOqG9S"
   },
   "source": [
    "##### Get the number of occurrences across all articles for each token (vocab_full_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yqDVUamqG9S"
   },
   "outputs": [],
   "source": [
    "sum_counts_matrix = cvz.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx49iipnqG9S"
   },
   "source": [
    "##### The size of the vocabulary is the number of tokens left after correcting for max_df, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uigvNU6qG9S",
    "outputId": "08c1f26d-1024-4b10-d6c9-148df53e21c8"
   },
   "outputs": [],
   "source": [
    "print('Initial vocabulary size: {}'.format(vocab_full_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQfLrXP2qG9T"
   },
   "source": [
    "##### Save the counts for each word in a numpy array instead of a matrix of shape (1, vocab_full_size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "wVKu2hhiqG9V",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum_counts = np.zeros(vocab_full_size, dtype=int)\n",
    "for v in range(vocab_full_size):\n",
    "    sum_counts[v] = sum_counts_matrix[0, v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otvTn6qJqG9V"
   },
   "source": [
    "#### Set up word2id and id2word dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "OJwGXZ9UqG9V",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])\n",
    "id2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMfOQwYEqG9V"
   },
   "source": [
    "#### Create Vocabulary & Sort elements in vocabulary\n",
    "\n",
    "- Sort by number of occurrences in ascending order and return array of indices\n",
    "- Set up vocabulary of tokens in ascending order of word occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "PALZHylIqG9V",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_sort = np.argsort(sum_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "x1MzGBnTqG9W",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = [id2word[idx_sort[idx_token]] for idx_token in range(vocab_full_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG2YakCDqG9W",
    "outputId": "c340d7c0-68ab-4b7c-c72b-cf2991cd16a7"
   },
   "outputs": [],
   "source": [
    "print('The 5 most common words in our vocabulary are: ', vocab[-5:])\n",
    "print('They occur the following number of times:')\n",
    "print(sorted(sum_counts)[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K121_yxyqG9X"
   },
   "outputs": [],
   "source": [
    "used_trigrams = []\n",
    "used_bigrams = []\n",
    "for w in vocab:\n",
    "    if w.count('_') == 2:\n",
    "        used_trigrams.append(w)\n",
    "    if w.count('_') == 1:\n",
    "        used_bigrams.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh0f1wdWqG9X",
    "outputId": "1724f36b-5a4e-4cfe-defa-9b6ea605369c"
   },
   "outputs": [],
   "source": [
    "len(used_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8efKiRQqG9Y",
    "outputId": "1047752b-11ba-40eb-d621-05d19bc9c8a5"
   },
   "outputs": [],
   "source": [
    "len(used_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvdxRpEFqG9b"
   },
   "source": [
    "#### Create dictionary and inverse dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "tLEo_XbCqG9b",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6BwbpDMqG9b",
    "outputId": "812f3d09-9e5c-44e2-f3bc-1b4c5351edc3"
   },
   "outputs": [],
   "source": [
    "id2word[word2id[vocab[-1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgTOZHcqqG9c"
   },
   "source": [
    "##### for every article that is randomly selected to be part of the training set, split the article and add the word to the vocabulary if it is in word2id (means that words not in train but in the other sets will not be included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzQ0DZL_qG9c"
   },
   "outputs": [],
   "source": [
    "idx_permute = np.random.permutation(num_articles).astype(int)\n",
    "vocab = list(set([w for idx_article in range(trSize) for w in articles[idx_permute[idx_article]].split() if w in word2id]))\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "vocab_train_size = len(vocab)\n",
    "if (vocab_full_size - vocab_train_size) > 0:\n",
    "    print(vocab_full_size - vocab_train_size, ' words have not been included in the vocabulary as they do not occur in the training data.')\n",
    "    print('Vocabulary after removing words not in train: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "canP9M0bqG9d",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6cde6c5b-e5d1-4cd7-9ed5-284677abbe22",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "articles_full = [[word2id[w] for w in articles[idx_d].split() if w in word2id] for idx_d in range(len(articles))]\n",
    "timestamps_full = [time2id[timestamps[idx_d]] for idx_d in range(len(articles))]\n",
    "articles_tr = [[word2id[w] for w in articles[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\n",
    "timestamps_tr = [time2id[timestamps[idx_permute[idx_d]]] for idx_d in range(trSize)]\n",
    "articles_ts = [[word2id[w] for w in articles[idx_permute[idx_d + trSize]].split() if w in word2id] for idx_d in range(tsSize)]\n",
    "timestamps_ts = [time2id[timestamps[idx_permute[idx_d + trSize]]] for idx_d in range(tsSize)]\n",
    "articles_va = [[word2id[w] for w in articles[idx_permute[idx_d + trSize + tsSize]].split() if w in word2id] for idx_d in range(vaSize)]\n",
    "timestamps_va = [time2id[timestamps[idx_permute[idx_d + trSize + tsSize]]] for idx_d in range(vaSize)]\n",
    "\n",
    "print('number of articles (full dataset): {} \\n... equal to len(articles) and len(timestamps_full)? - {}'.format(\n",
    "    len(articles_full), len(articles_full) == len(articles) == len(timestamps_full)))\n",
    "print('number of articles (train): {} \\n... equal to len(articles_tr) and len(timestamps_tr)? - {}'.format(\n",
    "    len(articles_tr), len(articles_tr) == len(articles_tr) == len(timestamps_tr)))\n",
    "print('number of articles (test): {} \\n... equal to len(articles_ts) and len(timestamps_ts)? - {}'.format(\n",
    "    len(articles_ts), len(articles_ts) == len(articles_ts) == len(timestamps_ts)))\n",
    "print('number of articles (valid): {} \\n... equal to len(articles_va) and len(timestamps_va)? - {}'.format(\n",
    "    len(articles_va), len(articles_va) == len(articles_va) == len(timestamps_va)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEWOaSc3qG9e"
   },
   "source": [
    "### Split test set in 2 halves\n",
    "This is done to perform the document completion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "SlOSgArBqG9e",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "articles_ts_h1 = [[w for i, w in enumerate(article) if i <= len(article) / 2.0 - 1] for article in articles_ts]\n",
    "articles_ts_h2 = [[w for i, w in enumerate(article) if i > len(article) / 2.0 - 1] for article in articles_ts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJST-ozyqG9e"
   },
   "source": [
    "### Getting lists of words and article_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "D0viGjiKqG9f",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "12450fa6-4ba4-4a6e-8439-dfdcba084312",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Creating lists of words...')\n",
    "def create_list_words(in_articles):\n",
    "    return [word for article in in_articles for word in article]\n",
    "\n",
    "words_full = create_list_words(articles_full)\n",
    "words_tr = create_list_words(articles_tr)\n",
    "words_ts = create_list_words(articles_ts)\n",
    "words_ts_h1 = create_list_words(articles_ts_h1)\n",
    "words_ts_h2 = create_list_words(articles_ts_h2)\n",
    "words_va = create_list_words(articles_va)\n",
    "\n",
    "print('len(words_full): ', len(words_full))\n",
    "print('len(words_tr): ', len(words_tr))\n",
    "print('len(words_ts): ', len(words_ts))\n",
    "print('len(words_ts_h1): ', len(words_ts_h1))\n",
    "print('len(words_ts_h2): ', len(words_ts_h2))\n",
    "print('len(words_va): ', len(words_va))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM24UCqbqG9g"
   },
   "source": [
    "### Get article indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "jvZNZIY-qG9g",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "558fd492-edb4-4e6c-c335-44301e72e1c8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_article_indices(in_articles):\n",
    "    # for every word in the article add the index of the article in the corresponding set\n",
    "    aux = [[j for i in range(len(article))] for j, article in enumerate(in_articles)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "article_indices_full = create_article_indices(articles_full)\n",
    "article_indices_tr = create_article_indices(articles_tr)\n",
    "article_indices_ts = create_article_indices(articles_ts)\n",
    "article_indices_ts_h1 = create_article_indices(articles_ts_h1)\n",
    "article_indices_ts_h2 = create_article_indices(articles_ts_h2)\n",
    "article_indices_va = create_article_indices(articles_va)\n",
    "\n",
    "print('len(article_indices_full): {} \\n...should be len(words_full): {}'.format(len(article_indices_full), len(words_full)))\n",
    "print('len(article_indices_tr): {} \\n...should be len(words_tr): {}'.format(len(article_indices_tr), len(words_tr)))\n",
    "print('\\n')\n",
    "print('len(np.unique(article_indices_full)): {} \\n...should be {}'.format(len(np.unique(article_indices_full)), len(articles_full)))\n",
    "print('len(np.unique(article_indices_tr)): {} \\n...should be {}'.format(len(np.unique(article_indices_tr)), len(articles_tr)))\n",
    "print('len(np.unique(article_indices_ts)): {} \\n...should be {}'.format(len(np.unique(article_indices_ts)), len(articles_ts)))\n",
    "print('len(np.unique(article_indices_ts_h1)): {} \\n...should be {}'.format(len(np.unique(article_indices_ts_h1)), len(articles_ts_h1)))\n",
    "print('len(np.unique(article_indices_ts_h2)): {} \\n...should be {}'.format(len(np.unique(article_indices_ts_h2)), len(articles_ts_h2)))\n",
    "print('len(np.unique(article_indices_va)): {} \\n...should be {}'.format(len(np.unique(article_indices_va)), len(articles_va)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhgpIbPIqG9h"
   },
   "source": [
    "### Number of articles in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "X9r_rS6oqG9h",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_articles_full = len(articles_full)\n",
    "n_articles_tr = len(articles_tr)\n",
    "n_articles_ts = len(articles_ts)\n",
    "n_articles_ts_h1 = len(articles_ts_h1)\n",
    "n_articles_ts_h2 = len(articles_ts_h2)\n",
    "n_articles_va = len(articles_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1XVHi6hqG9h"
   },
   "source": [
    "### Create BoW representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "7H3YJDaOqG9i",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_bow(article_indices, words, n_articles, vocab_size):\n",
    "    return sparse.coo_matrix(([1] * len(article_indices), (article_indices, words)), shape=(n_articles, vocab_size)).tocsr()\n",
    "\n",
    "bow_full = create_bow(article_indices_full, words_full, n_articles_full, len(vocab))\n",
    "bow_tr = create_bow(article_indices_tr, words_tr, n_articles_tr, len(vocab))\n",
    "bow_ts = create_bow(article_indices_ts, words_ts, n_articles_ts, len(vocab))\n",
    "bow_ts_h1 = create_bow(article_indices_ts_h1, words_ts_h1, n_articles_ts_h1, len(vocab))\n",
    "bow_ts_h2 = create_bow(article_indices_ts_h2, words_ts_h2, n_articles_ts_h2, len(vocab))\n",
    "bow_va = create_bow(article_indices_va, words_va, n_articles_va, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "zsmm2WqlqG9i",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "cdc8c208-d62a-46e2-a510-c49dbcd49d88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(bow_full.shape, bow_tr.shape,bow_ts.shape,bow_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqCNxS4MqG9i",
    "outputId": "5f6afbb4-26b1-4ed0-ef4f-fe93c4723af7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if use_subset:\n",
    "    data_dir_final = data_dir_final + 'subset/'\n",
    "\n",
    "if group_years:\n",
    "    data_dir_final = data_dir_final + 'grouped_years/'\n",
    "\n",
    "data_dir_final = os.path.join(data_dir_final, 'min_df_{}'.format(min_df))\n",
    "print('Final data directory:', data_dir_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "8egABSqBqG9j",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(data_dir_final):\n",
    "    os.system('mkdir -p ' + data_dir_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRKJhXB7qG9j"
   },
   "source": [
    "### Save the vocabulary and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "0Yi-4VIcqG9j",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir_final,'vocab.txt'), \"w\") as f:\n",
    "    for v in vocab:\n",
    "        f.write(v + '\\n')\n",
    "\n",
    "with open(os.path.join(data_dir_final, 'vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "with open(os.path.join(data_dir_final,'timestamps.txt'), \"w\") as f:\n",
    "    for t in time_list:\n",
    "        f.write(str(t) + '\\n')\n",
    "\n",
    "with open(os.path.join(data_dir_final,'timestamps.pkl'), 'wb') as f:\n",
    "    pickle.dump(time_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzarEUwxqG9j"
   },
   "source": [
    "### Save timestamps corresponding to BoW document representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "bdMilp_lqG9j",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "savemat(os.path.join(data_dir_final, 'bow_full_timestamps.mat'), {'timestamps': timestamps_full}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_tr_timestamps.mat'), {'timestamps': timestamps_tr}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_ts_timestamps.mat'), {'timestamps': timestamps_ts}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_va_timestamps.mat'), {'timestamps': timestamps_va}, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrK6DaQiqG9k"
   },
   "source": [
    "### Split bow into token/value pairs & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "kaaPo4ejqG9k",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_bow(bow_in, n_articles):\n",
    "    indices = [[w for w in bow_in[doc, :].indices] for doc in range(n_articles)]\n",
    "    counts = [[c for c in bow_in[doc, :].data] for doc in range(n_articles)]\n",
    "    return indices, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "vcj89_SwqG9k",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b1b2f781-f829-4eb2-df9a-11107b5a246d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bow_full_tokens, bow_full_counts = split_bow(bow_full, n_articles_full)\n",
    "savemat(os.path.join(data_dir_final, 'bow_full_tokens.mat'), {'tokens': bow_full_tokens}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_full_counts.mat'), {'counts': bow_full_counts}, do_compression=True)\n",
    "\n",
    "bow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_articles_tr)\n",
    "savemat(os.path.join(data_dir_final, 'bow_tr_tokens.mat'), {'tokens': bow_tr_tokens}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_tr_counts.mat'), {'counts': bow_tr_counts}, do_compression=True)\n",
    "\n",
    "bow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_articles_ts)\n",
    "savemat(os.path.join(data_dir_final, 'bow_ts_tokens.mat'), {'tokens': bow_ts_tokens}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_ts_counts.mat'), {'counts': bow_ts_counts}, do_compression=True)\n",
    "\n",
    "bow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_articles_ts_h1)\n",
    "savemat(os.path.join(data_dir_final, 'bow_ts_h1_tokens.mat'), {'tokens': bow_ts_h1_tokens}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_ts_h1_counts.mat'), {'counts': bow_ts_h1_counts}, do_compression=True)\n",
    "\n",
    "bow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_articles_ts_h2)\n",
    "savemat(os.path.join(data_dir_final, 'bow_ts_h2_tokens.mat'), {'tokens': bow_ts_h2_tokens}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_ts_h2_counts.mat'), {'counts': bow_ts_h2_counts}, do_compression=True)\n",
    "\n",
    "bow_va_tokens, bow_va_counts = split_bow(bow_va, n_articles_va)\n",
    "savemat(os.path.join(data_dir_final, 'bow_va_tokens.mat'), {'tokens': bow_va_tokens}, do_compression=True)\n",
    "savemat(os.path.join(data_dir_final, 'bow_va_counts.mat'), {'counts': bow_va_counts}, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYyCc1yUqG9k"
   },
   "source": [
    "## Create Data for Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvNwf_P_qG9l"
   },
   "outputs": [],
   "source": [
    "concat_texts = news.groupby('year_gr')['text'].apply(','.join).reset_index()\n",
    "concat_texts.rename(columns={'year_gr':'time','text':'text_orig'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kpyh306eqG9l"
   },
   "outputs": [],
   "source": [
    "def remove_non_vocab(text):\n",
    "    text = \" \".join([id2word[word2id[w]] for w in text.split() if w in word2id.keys()])\n",
    "    return text\n",
    "\n",
    "concat_texts['words'] = concat_texts['text_orig'].apply(remove_non_vocab)\n",
    "concat_texts['number_words'] = concat_texts['words'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcG3i9X5qG9m",
    "outputId": "90ec12f7-cb61-4d0c-a5bb-c0153b09c8c2"
   },
   "outputs": [],
   "source": [
    "concat_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9y1GhYNlqG9m"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir + 'pseudotext_wordClouds'):\n",
    "    os.makedirs(data_dir + 'pseudotext_wordClouds')\n",
    "    \n",
    "concat_texts[['time','words']].to_csv(data_dir + 'pseudotext_wordClouds.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9HT7pBVYqG8n",
    "iCA9XvfQqG8p",
    "VzPCMGgwqG8s",
    "IpiyxMzPqG8z",
    "BL8xXyQgqG8z",
    "nMlIeMQBqG9F",
    "-lVbdcKWqG9F",
    "fhTq5W7wqG9N",
    "lOIYKIVkqG9P",
    "5RKfTUJsqG9P",
    "2xVtGPZ6qG9Q",
    "otvTn6qJqG9V",
    "OMfOQwYEqG9V",
    "VvdxRpEFqG9b",
    "XEWOaSc3qG9e",
    "RJST-ozyqG9e",
    "zM24UCqbqG9g",
    "OhgpIbPIqG9h",
    "p1XVHi6hqG9h",
    "kRKJhXB7qG9j",
    "VzarEUwxqG9j",
    "zrK6DaQiqG9k",
    "dYyCc1yUqG9k"
   ],
   "name": "04_Final-Prep.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
